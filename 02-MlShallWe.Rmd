# (PART) Machine learning: Shall we? {-} 

```{r setupMlShall, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(DT)
library(plotly)

```


Before we answer the question **Machine learning, shall we?** lets look into what machine learning is

# What is machine learning? {#whatML}

Machine learning is a sub domain of artificial intelligence and has several definitions:



```{r echo=FALSE, out.width='50%'}
knitr::include_graphics('images/MachineLearning.png')
```


>"Field of study that gives computers the ability to learn without being explicitly programmed"  
>
>--- Arthur Samuel ^[American pioneer in the field of computer gaming and artificial intelligence https://en.wikipedia.org/wiki/Arthur_Samuel]:  



> "Learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E
>
>--- Tom Mitchell ^[Professor of Computer Science and Machine Learning at Carnegie Mellon http://www.cs.cmu.edu/~tom/] 



## What is intelligence?

To discuss the question of what is artificial intelligence, the first step is to define what intelligence is.

A group of 52 psychology researchers published in [@gottfredson1997mainstream] the following definition:   

>A very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. It is not merely book learning, a narrow academic skill, or test-taking smarts. Rather, it reflects a broader and deeper capability for comprehending our surroundings—"catching on," "making sense" of things, or "figuring out" what to do.

Alfred Binet, a french psychologist who invented first practical IQ test defined in 1905 [@binet1916new]:

>Judgment, otherwise called "good sense", "practical sense", "initiative", the faculty of adapting one's self to circumstances

And Albert Einstein said:  

>The measure of intelligence is the ability to change.

Tegmark's summarizes the situation in [@tegmark2017life]  

> There’s no agreement on what intelligence is even among intelligent intelligence researchers! So there’s clearly no undisputed “correct” definition of intelligence.

### Definition of artificial intelligence sub domains

Even though there is no undisputed definition of **intelligence** there is a undisputed definition of how machine learning is related to artificial intelligence

Agreement: **Machine learning is a sub domain of artificial intelligence (AI)**

---

<img src="images/AiMlDeep.png"  align="middle" style="width:100%;">

---


The history of those fields goes back to the 1950's

---

<img src="images/AiMlDeepTimeLine.png"  align="middle" style="width:100%;">

---

## Is AI smarter than humans?

Which of the following questions can a computer answer better?

```{block2 echo=TRUE, type='rmdtip'}

- 17*35  ^[17*35=595]
- Wie viele Tiere von jeder Art nahm Moses mit auf die Arche?     ^[Knowledge neglect https://en.wikipedia.org/wiki/Knowledge_neglect]
- Wie heißen die drei letzten Bundespräsidenten?           ^[Die drei letzten Bundespräsidenten: Steinmeier, Gauck, Wulf]

```



```{r echo=FALSE, out.width='40%'}
knitr::include_graphics('images/ship.png')
```

### Thinking, fast and slow [@kahneman2011thinking]

In Daniel Kahneman's **Thinking, fast and slow**  [@kahneman2011thinking] there are plenty of surprising social psychology experiments, on page 166 the following question is posed:


<blockquote>

A cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data:  
- 85% of the cabs in the city are Green and 15% are Blue.   
- A witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. 


**What is the probability that the cab involved in the accident was Blue rather than Green?** ^[https://www.anesi.com/bayes.htm]

</blockquote>

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('images/CabsBlueGreen.png')
```

Please cast your vote at https://pingo.coactum.de/157678 ^[Pingo question: What is the probability that the cab involved in the accident was Blue rather than Green? https://pingo.coactum.de/questions] or access Pingo webpage scanning QR code below:

```{r echo=FALSE, out.width='40%'}
knitr::include_graphics('images/PingoQrCode.png')
```



## Comparisons between AI and humans

### Breast cancer detection
<p>In a Google Health project ^[Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7% and 1.2% (USA and UK) in false positives and 9.4% and 2.7% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening. https://www.nature.com/articles/s41586-019-1799-6] the following results were achieved: <img src="images/Stetoscope.svg" alt="Smiley face" align="right" style="width:10%;"></p>

```{block2 echo=TRUE, type='rmdtip'}
- Absolute reduction of 5.7% and 1.2% (USA and UK) in false positives 
- Absolute reduction 9.4% and 2.7% (USA and UK)in false negatives. 
```

In an independent study of six radiologists, the AI system outperformed all of the human readers.
More on the study at https://www.nature.com/articles/s41586-019-1799-6

### Working together: Lung cancer detection
With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States

```{r echo=FALSE, out.width='20%'}
knitr::include_graphics('images/lung.svg')
```

A study published in Nature medicine ^[Abstract With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States1. Lung cancer screening using low-dose computed tomography has been shown to reduce mortality by 20–43% and is now included in US screening guidelines. Existing challenges include inter-grader variability and high false-positive and false-negative rates. We propose a deep learning algorithm that uses a patient’s current and prior computed tomography volumes to predict the risk of lung cancer. Our model achieves a state-of-the-art performance (94.4% area under the curve) on 6,716 National Lung Cancer Screening Trial cases, and performs similarly on an independent clinical validation set of 1,139 cases. We conducted two reader studies. When prior computed tomography imaging was not available, our model outperformed all six radiologists with absolute reductions of 11% in false positives and 5% in false negatives. Where prior computed tomography imaging was available, the model performance was on-par with the same radiologists. This creates an opportunity to optimize the screening process via computer assistance and automation. While the vast majority of patients remain unscreened, we show the potential for deep learning models to increase the accuracy, consistency and adoption of lung cancer screening worldwide. Website: https://www.nature.com/articles/s41591-019-0447-x] a team of members of Google AI and several hospitals reported

When prior computed tomography imaging was not available

```{block2 echo=TRUE, type='rmdtip'}

- Model outperformed all six radiologists
- Absolute reductions of 11% in false positives 
- Absolute reductions 5% in false negatives
```


### ImageNet Large Scale Visual Recognition Challenge (ILSVRC)

The ImageNet Large Scale Visual Recognition Challenge ^[Homepage of ILSVRC http://www.image-net.org/challenges/LSVRC/] (ILSVRC) [@ILSVRC15] evaluates algorithms for object recognition and image classification on a large scale.

Facts of ImageNet ^[ Homepage of ImageNet http://www.image-net.org]:

```{block2 echo=TRUE, type='rmdtip'}

- 14 million images
- 20,000 image categories
- 1000 image categories used for ILSVRC

```

The development of the results is shown in the graph below. The number of layers is a indication of model complexity

--- 

```{r booklistTable, out.height= '80%',out.width= '80%'}
ILSVRC <- tribble(
  ~ Year, ~ ErrorRate, ~ NumberOfLayers,
  2010, 28.2, 1,
  2011, 25.8, 1,
  2012, 16.4, 8,
  2013, 11.7, 8,
  2014, 7.3, 19,
  2015, 3.6,152)
  

p <- ggplot(ILSVRC, aes(Year, ErrorRate)) + geom_col(aes(fill=NumberOfLayers)) + labs(title = "Error rate of ILSVRC", x = "Year", y = "Error Rate [%]", caption ="Human level = 5%") +geom_hline(yintercept =5, color ="red") + geom_label(color = "red",aes(2012.5, 4,label = "Human Level", vjust = -1))
p <- ggplot(ILSVRC, aes(Year, ErrorRate)) + geom_col(aes(fill=NumberOfLayers)) + labs(title = "Error rate of ILSVRC, human error rate = 5% (red line)", x = "Year", y = "Error Rate [%]", caption ="Human level = 5%") +geom_hline(yintercept =5, color ="red") 
p <- ggplotly(p)
p
```

--- 

In 2017 the problem set to status "solved"  

```{block2 echo=TRUE, type='rmdtip'}

- 29 of 38 competing teams had an accuracy of more than 95%  
- ImageNet stopped competition

```

### AlphaGo Zero

Go is a strategy game **invented 2500 years ago** and has an estimated number of **possible board configuration of 10¹⁷⁴** compared to chess which has is 10¹²º. A detailed description is given by DeepMind's blog post "AlphaGo Zero: Starting from scratch" ^[https://deepmind.com/blog/article/alphago-zero-starting-scratch]

**AlphaGo Zero** is a version of DeepMind's^[Homepage of DeepMind https://deepmind.com] Go software AlphaGo

```{block2 echo=TRUE, type='rmdtip'}

- <p>No human intervention<img src="images/chess.svg" alt="Smiley face" align="right" style="width:30%;"> </p>
- No usage of historical data
- After 3 days of training as good as AlphaGo which beat world champion in 4 out of 5
- After 40 days of training becomes best Go player in the world

```

 AlphaZero learned three games,
 
 ```{r echo=FALSE, out.width='70%'}
knitr::include_graphics('images/AlphaZeroTrainingTime.png')
```

The capability progress of Alpha Zero during training is shown below

 ![Figure from https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go](images/AlphaZeroTraining.gif){width=100% .external}
 
 
  NOTE: EACH TRAINING STEP REPRESENTS 4,096 BOARD POSITIONS.
 

At the end of the training Alpha Zero achieved the following performance:

 ![Figure from https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go](images/AlphaZeroPerformance.png){width=100% .external}
 
 
 Implications are wider than just playing a game, as Garry Kasparov, a former world chess champion puts it:
 
 >The implications go far beyond my beloved chessboard... Not only do these self-taught expert machines perform incredibly well, but we can actually learn from the new knowledge they produce."
 
    
## ML models with bias {#ModelsWithBias}

Models might end up biased, why is that?

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/tlOIHko8ySg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

[source: https://www.youtube.com/watch?time_continue=1&v=tlOIHko8ySg&feature=emb_logo]

With a **unsuitable reward function** an **undesired result** can occur

```{block2 echo=TRUE, type='rmdtip'}

- <p>Framing the problem <img src="images/goal.svg" alt="Smiley face" align="right" style="width:10%;"> </p>
    - Goal is business reason, not fairness or avoidance of discrimination
    - Goal might lead to unwanted side effects https://openai.com/blog/faulty-reward-functions/ 
    
- <p>Collecting data<img src="images/zebra.svg" alt="Smiley face" align="right" style="width:10%;"></p>
     - Unrepresentative of reality
          - Collecting images of zebras only when sun shines => model might look for shadow for classifying a zebra 
     - Reflects existing prejudices
          - Historical data might lead recruiting tools to dismiss female candidates

- <p>Preparing the data<img src="images/Gender.svg" alt="Smiley face" align="right" style="width:10%;"></p>
    - Selecting attributes to be considered might lead to bias
        - Attribute gender might lead to bias

```

####  How to avoid bias

Avoiding bias is harder than you might think

```{block2 echo=TRUE, type='rmdtip'}

- <p>Unknown unknowns<img src="images/Unknown.svg" alt="Smiley face" align="right" style="width:10%;"></p>
    - Gender might be deducted by recruiting tool from use of language

- Imperfect processes
    - Test data has same bias as training data
    - Bias not easy to discover

```

#### Human bias

Machine learning model can be biased for several reasons as shown above, how about humans?

```{block2 echo=TRUE, type='rmdtip'}

- <p>Study in Germany<img src="images/HumanBias.png" alt="Smiley face" align="right" style="width:40%;"></p>
- Judges read description of shoplifter
- Rolled a pair of loaded dice
- Dice = **3** => Average **5** months prison
- Dice = **9** => Average **8** months prison

```


## Attacks on ML models

Especially image classification models have shown to be susceptible to attacks which leads to wrong classifications. This could lead to

```{block2 echo=TRUE, type='rmdwarning'}

- Traffic sign misclassification
    - targeted attack $\implies$ force a certain prediction
    - untargeted attack $\implies$ force misclassification, but not a particular predcition
- Avoiding face detection
```

How a attack can be performed is described by Goodfellow et al. in [@goodfellow2014explaining]

### Adding noise to image leads to misclassification


 ![Figure from Image Credit: Goodfellow et al. [@goodfellow2014explaining])](images/GanAttack.png){width=100% .external}


--- 


### But what about attacks on human perception?

Which statement is correct?

- <p>Top line longer<img src="images/LineLengthIllusion.png" alt="Smiley face" align="right" style="width:40%;"></p>
- Bottom line longer
- Both are same length

---


<div class="HeadingNoNumber">
Is this a picture of a real person? 

</div>

Look at the picture below, is it a real person or an animation?

--- 

 ![Figure from https://commons.wikimedia.org/wiki/File:Woman_1.jpg (Image Credit: Owlsmcgee [Public domain] )](images/GanWomen.png){width=60% .external}
 

The image is create using a generative adversarial network (GAN), see below for the principle, for detailed description see https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f 

---

```{r, layout="l-screen-inset"}
knitr::include_graphics('images/GanPrincipleAndSteps.png')
```

---


### Model Hacking ADAS to Pave Safer Roads for Autonomous Vehicles

A McAfee Advanced Threat Research (ATR) hack which caused cars to drive 50 miles faster than the speed limit by tricking the camera using a piece of tape on the traffic sign. This simple attack tricked the Advanced Driver Assist Systems (ADAS) to drive the car at 85mph instead of 35mph. A detailed description of the hack can be found at https://www.mcafee.com/blogs/other-blogs/mcafee-labs/model-hacking-adas-to-pave-safer-roads-for-autonomous-vehicles/



<div class="rmdtip">
<p>Facts about ATR hack:<img src="images/blackTakeAttack.png" alt="Smiley face" align="right" style="width:30%;"></p>

- McAfee Advanced Threat Research (ATR)
- MobilEye camera system
     - utilized by over 40Mio vehicles (incl. Tesla Harware Pack 1)
- MobileEye reads a 35mph sign as 85mph sign    

</div>

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/4uGV_fRj0UA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

---

As a look forward Steve Povolny, Head of McAfee Advanced Threat Research wrote:

<blockquote>
In order to drive success in this key industry and shift the perception that machine learning systems are secure, we need to accelerate discussions and awareness of the problems and steer the direction and development of next-generation technologies. Puns intended.  

Steve Povolny, Head of McAfee Advanced Threat Research
</blockquote>

#### Press coverage

Interesting was to see the coverage of the media


<blockquote>
TESLA CARS TRICKED INTO AUTONOMOUSLY ACCELERATING 50 MPH USING SPEED SIGN ALTERED WITH SMALL PIECE OF TAPE
[Newsweek](https://www.newsweek.com/tesla-model-x-model-s-mcafee-research-tricks-acceleration-speed-limit-signs-1487956)
</blockquote>

<blockquote>

Tesla in Autopilot mode tricked into speeding using terrifying hack
[The Telegraph](https://www.telegraph.co.uk/technology/2020/02/19/self-driving-tesla-tricked-speeding-using-terrifying-hack/)
</blockquote>




## Measuring the Algorithmic Efficiency of Neural Networks TBD

[@hernandez2020measuring]

Measuring the Algorithmic Efficiency of Neural Networks

