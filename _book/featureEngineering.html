<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.4 Feature engineering | Machine learning orientation</title>
  <meta name="description" content="A guide to machine learning" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="6.4 Feature engineering | Machine learning orientation" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A guide to machine learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.4 Feature engineering | Machine learning orientation" />
  
  <meta name="twitter:description" content="A guide to machine learning" />
  

<meta name="author" content="Uwe Sterr" />


<meta name="date" content="2020-02-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="eda.html"/>
<link rel="next" href="MlProjectProcessModelFit.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.49.4/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.49.4/plotly-latest.min.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.11/datatables.js"></script>
<link href="libs/dt-core-1.10.19/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.19/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.19/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/uweBookdown.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning orientation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="part"><span><b>I Machine learning: Shall we?</b></span></li>
<li class="chapter" data-level="2" data-path="whatML.html"><a href="whatML.html"><i class="fa fa-check"></i><b>2</b> What is machine learning?</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-intelligence.html"><a href="what-is-intelligence.html"><i class="fa fa-check"></i><b>2.1</b> What is intelligence?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="what-is-intelligence.html"><a href="what-is-intelligence.html#definition-of-artificial-intelligence-sub-domains"><i class="fa fa-check"></i><b>2.1.1</b> Definition of artificial intelligence sub domains</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="is-ai-smarter-than-humans.html"><a href="is-ai-smarter-than-humans.html"><i class="fa fa-check"></i><b>2.2</b> Is AI smarter than humans?</a><ul>
<li class="chapter" data-level="2.2.1" data-path="is-ai-smarter-than-humans.html"><a href="is-ai-smarter-than-humans.html#thinking-fast-and-slow-kahneman2011thinking"><i class="fa fa-check"></i><b>2.2.1</b> Thinking, fast and slow <span class="citation">(Kahneman <span>2011</span>)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="comparisons-between-ai-and-humans.html"><a href="comparisons-between-ai-and-humans.html"><i class="fa fa-check"></i><b>2.3</b> Comparisons between AI and humans</a><ul>
<li class="chapter" data-level="2.3.1" data-path="comparisons-between-ai-and-humans.html"><a href="comparisons-between-ai-and-humans.html#breast-cancer-detection"><i class="fa fa-check"></i><b>2.3.1</b> Breast cancer detection</a></li>
<li class="chapter" data-level="2.3.2" data-path="comparisons-between-ai-and-humans.html"><a href="comparisons-between-ai-and-humans.html#working-together-lung-cancer-detection"><i class="fa fa-check"></i><b>2.3.2</b> Working together: Lung cancer detection</a></li>
<li class="chapter" data-level="2.3.3" data-path="comparisons-between-ai-and-humans.html"><a href="comparisons-between-ai-and-humans.html#imagenet-large-scale-visual-recognition-challenge-ilsvrc"><i class="fa fa-check"></i><b>2.3.3</b> ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</a></li>
<li class="chapter" data-level="2.3.4" data-path="comparisons-between-ai-and-humans.html"><a href="comparisons-between-ai-and-humans.html#alphago-zero"><i class="fa fa-check"></i><b>2.3.4</b> AlphaGo Zero</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ml-models-with-bias.html"><a href="ml-models-with-bias.html"><i class="fa fa-check"></i><b>2.4</b> ML models with bias</a></li>
<li class="chapter" data-level="2.5" data-path="attacks-on-ml-models.html"><a href="attacks-on-ml-models.html"><i class="fa fa-check"></i><b>2.5</b> Attacks on ML models</a><ul>
<li class="chapter" data-level="2.5.1" data-path="attacks-on-ml-models.html"><a href="attacks-on-ml-models.html#adding-noise-to-image-leads-to-misclassification"><i class="fa fa-check"></i><b>2.5.1</b> Adding noise to image leads to misclassification</a></li>
<li class="chapter" data-level="2.5.2" data-path="attacks-on-ml-models.html"><a href="attacks-on-ml-models.html#but-what-about-attacks-on-human-perception"><i class="fa fa-check"></i><b>2.5.2</b> But what about attacks on human perception?</a></li>
<li class="chapter" data-level="2.5.3" data-path="attacks-on-ml-models.html"><a href="attacks-on-ml-models.html#model-hacking-adas-to-pave-safer-roads-for-autonomous-vehicles"><i class="fa fa-check"></i><b>2.5.3</b> Model Hacking ADAS to Pave Safer Roads for Autonomous Vehicles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="outlook.html"><a href="outlook.html"><i class="fa fa-check"></i><b>3</b> Outlook</a><ul>
<li class="chapter" data-level="3.1" data-path="development-of-life.html"><a href="development-of-life.html"><i class="fa fa-check"></i><b>3.1</b> Development of life</a><ul>
<li class="chapter" data-level="3.1.1" data-path="development-of-life.html"><a href="development-of-life.html#when-will-superhuman-ai-come-and-will-it-be-good"><i class="fa fa-check"></i><b>3.1.1</b> When will superhuman AI come, and will it be good?</a></li>
<li class="chapter" data-level="3.1.2" data-path="development-of-life.html"><a href="development-of-life.html#ai-aftermath-scenario"><i class="fa fa-check"></i><b>3.1.2</b> AI aftermath scenario</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-religion-dataism.html"><a href="data-religion-dataism.html"><i class="fa fa-check"></i><b>3.2</b> Data religion: Dataism</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discussion-points.html"><a href="discussion-points.html"><i class="fa fa-check"></i><b>4</b> Discussion points</a><ul>
<li class="chapter" data-level="4.1" data-path="researcher-stops-his-work-due-to-ethical-concerns.html"><a href="researcher-stops-his-work-due-to-ethical-concerns.html"><i class="fa fa-check"></i><b>4.1</b> Researcher stops his work due to ethical concerns</a></li>
<li class="chapter" data-level="4.2" data-path="career-oxford-seeks-ai-ethics-professor.html"><a href="career-oxford-seeks-ai-ethics-professor.html"><i class="fa fa-check"></i><b>4.2</b> Career: Oxford seeks AI ethics professor</a></li>
<li class="chapter" data-level="4.3" data-path="shaping-europes-digital-future-commission-presents-strategies-for-data-and-artificial-intelligence.html"><a href="shaping-europes-digital-future-commission-presents-strategies-for-data-and-artificial-intelligence.html"><i class="fa fa-check"></i><b>4.3</b> Shaping Europe’s digital future: Commission presents strategies for data and Artificial Intelligence</a></li>
</ul></li>
<li class="part"><span><b>II Machine learning fundamentals</b></span></li>
<li class="chapter" data-level="5" data-path="MachineLearningFundamentals.html"><a href="MachineLearningFundamentals.html"><i class="fa fa-check"></i><b>5</b> Machine learning fundamentals</a></li>
<li class="chapter" data-level="6" data-path="ml-project-process.html"><a href="ml-project-process.html"><i class="fa fa-check"></i><b>6</b> ML project process</a><ul>
<li class="chapter" data-level="6.1" data-path="identify-if-ml-is-suited-to-fulfill-need.html"><a href="identify-if-ml-is-suited-to-fulfill-need.html"><i class="fa fa-check"></i><b>6.1</b> Identify if ML is suited to fulfill need</a></li>
<li class="chapter" data-level="6.2" data-path="gather-data.html"><a href="gather-data.html"><i class="fa fa-check"></i><b>6.2</b> Gather data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="gather-data.html"><a href="gather-data.html#how-much-data-is-necessary"><i class="fa fa-check"></i><b>6.2.1</b> How much data is necessary?</a></li>
<li class="chapter" data-level="6.2.2" data-path="gather-data.html"><a href="gather-data.html#which-data-is-useful"><i class="fa fa-check"></i><b>6.2.2</b> Which data is useful?</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>6.3</b> Exploratory and quantitative data analysis</a><ul>
<li class="chapter" data-level="6.3.1" data-path="eda.html"><a href="eda.html#example-for-exploratory-and-quantitative-data-analysis"><i class="fa fa-check"></i><b>6.3.1</b> Example for exploratory and quantitative data analysis</a></li>
<li class="chapter" data-level="6.3.2" data-path="eda.html"><a href="eda.html#visualizations-for-categorical-data-exploring-the-okcupid-data"><i class="fa fa-check"></i><b>6.3.2</b> Visualizations for Categorical Data: Exploring the OkCupid Data</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="featureEngineering.html"><a href="featureEngineering.html"><i class="fa fa-check"></i><b>6.4</b> Feature engineering</a><ul>
<li class="chapter" data-level="6.4.1" data-path="featureEngineering.html"><a href="featureEngineering.html#encoding-categorical-predictors"><i class="fa fa-check"></i><b>6.4.1</b> Encoding Categorical Predictors</a></li>
<li class="chapter" data-level="6.4.2" data-path="featureEngineering.html"><a href="featureEngineering.html#engineering-numeric-features"><i class="fa fa-check"></i><b>6.4.2</b> Engineering numeric features</a></li>
<li class="chapter" data-level="6.4.3" data-path="featureEngineering.html"><a href="featureEngineering.html#featureImportance"><i class="fa fa-check"></i><b>6.4.3</b> Feature importance</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="MlProjectProcessModelFit.html"><a href="MlProjectProcessModelFit.html"><i class="fa fa-check"></i><b>6.5</b> Model fit</a></li>
<li class="chapter" data-level="6.6" data-path="tuneHyperparameter.html"><a href="tuneHyperparameter.html"><i class="fa fa-check"></i><b>6.6</b> Model tuning</a><ul>
<li class="chapter" data-level="6.6.1" data-path="tuneHyperparameter.html"><a href="tuneHyperparameter.html#metrics"><i class="fa fa-check"></i><b>6.6.1</b> Metrics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="machineLearningClasses.html"><a href="machineLearningClasses.html"><i class="fa fa-check"></i><b>7</b> Machine learning types</a><ul>
<li class="chapter" data-level="7.1" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>7.1</b> Supervised learning</a><ul>
<li class="chapter" data-level="7.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#MlSelfSupvervised"><i class="fa fa-check"></i><b>7.1.1</b> Self supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>7.2</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="7.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#discovering-clusters"><i class="fa fa-check"></i><b>7.2.1</b> Discovering clusters</a></li>
<li class="chapter" data-level="7.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#discovering-latent-factors"><i class="fa fa-check"></i><b>7.2.2</b> Discovering latent factors</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html"><i class="fa fa-check"></i><b>7.3</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="7.3.1" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#elements-of-reinforcement-learning"><i class="fa fa-check"></i><b>7.3.1</b> Elements of reinforcement learning</a></li>
<li class="chapter" data-level="7.3.2" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#rl-algorithms"><i class="fa fa-check"></i><b>7.3.2</b> RL algorithms</a></li>
<li class="chapter" data-level="7.3.3" data-path="reinforcement-learning.html"><a href="reinforcement-learning.html#example-self-driving-car-mit"><i class="fa fa-check"></i><b>7.3.3</b> Example self driving car MIT</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="MlAlgorithm.html"><a href="MlAlgorithm.html"><i class="fa fa-check"></i><b>8</b> ML algorithms</a><ul>
<li class="chapter" data-level="8.1" data-path="MlAlgoLinReg.html"><a href="MlAlgoLinReg.html"><i class="fa fa-check"></i><b>8.1</b> Linear regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="MlAlgoLinReg.html"><a href="MlAlgoLinReg.html#example-for-linear-regression"><i class="fa fa-check"></i><b>8.1.1</b> Example for linear regression</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="MlAlgoLogReg.html"><a href="MlAlgoLogReg.html"><i class="fa fa-check"></i><b>8.2</b> Logistic regression</a><ul>
<li class="chapter" data-level="8.2.1" data-path="MlAlgoLogReg.html"><a href="MlAlgoLogReg.html#python-example-logistic-regression"><i class="fa fa-check"></i><b>8.2.1</b> Python example logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="MlAlgoTrees.html"><a href="MlAlgoTrees.html"><i class="fa fa-check"></i><b>8.3</b> Tree based methods</a><ul>
<li class="chapter" data-level="8.3.1" data-path="MlAlgoTrees.html"><a href="MlAlgoTrees.html#splitting-metrics"><i class="fa fa-check"></i><b>8.3.1</b> Splitting metrics</a></li>
<li class="chapter" data-level="8.3.2" data-path="MlAlgoTrees.html"><a href="MlAlgoTrees.html#ensembles"><i class="fa fa-check"></i><b>8.3.2</b> Ensembles</a></li>
<li class="chapter" data-level="8.3.3" data-path="MlAlgoTrees.html"><a href="MlAlgoTrees.html#MlAlgoTreesRandomForest"><i class="fa fa-check"></i><b>8.3.3</b> Random forest</a></li>
<li class="chapter" data-level="8.3.4" data-path="MlAlgoTrees.html"><a href="MlAlgoTrees.html#MlAlgoTreesGBM"><i class="fa fa-check"></i><b>8.3.4</b> Boosted trees</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="MlAlgoSvm.html"><a href="MlAlgoSvm.html"><i class="fa fa-check"></i><b>8.4</b> Support Vector Machine (SVM) TBD</a><ul>
<li class="chapter" data-level="8.4.1" data-path="MlAlgoSvm.html"><a href="MlAlgoSvm.html#kernels"><i class="fa fa-check"></i><b>8.4.1</b> Kernels</a></li>
<li class="chapter" data-level="8.4.2" data-path="MlAlgoSvm.html"><a href="MlAlgoSvm.html#python-example-for-svm"><i class="fa fa-check"></i><b>8.4.2</b> Python example for SVM</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="MlAlgoNN.html"><a href="MlAlgoNN.html"><i class="fa fa-check"></i><b>8.5</b> Neural networks</a><ul>
<li class="chapter" data-level="8.5.1" data-path="MlAlgoNN.html"><a href="MlAlgoNN.html#convolutional-neural-network-cnn-tbd"><i class="fa fa-check"></i><b>8.5.1</b> Convolutional Neural Network (CNN) TBD</a></li>
<li class="chapter" data-level="8.5.2" data-path="MlAlgoNN.html"><a href="MlAlgoNN.html#rnn-tbd"><i class="fa fa-check"></i><b>8.5.2</b> RNN TBD</a></li>
<li class="chapter" data-level="8.5.3" data-path="MlAlgoNN.html"><a href="MlAlgoNN.html#gans"><i class="fa fa-check"></i><b>8.5.3</b> GANs</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="a-gentle-introduction-to-cyclegan-for-image-translation.html"><a href="a-gentle-introduction-to-cyclegan-for-image-translation.html"><i class="fa fa-check"></i><b>8.6</b> A Gentle Introduction to CycleGAN for Image Translation</a><ul>
<li class="chapter" data-level="8.6.1" data-path="a-gentle-introduction-to-cyclegan-for-image-translation.html"><a href="a-gentle-introduction-to-cyclegan-for-image-translation.html#examples-for-gans"><i class="fa fa-check"></i><b>8.6.1</b> Examples for GANs</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="software-that-can-generate-photos-from-paintings-turn-horses-into-zebras-perform-style-transfer-and-more-.html"><a href="software-that-can-generate-photos-from-paintings-turn-horses-into-zebras-perform-style-transfer-and-more-.html"><i class="fa fa-check"></i><b>8.7</b> Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.</a></li>
<li class="chapter" data-level="8.8" data-path="transformers-tbd.html"><a href="transformers-tbd.html"><i class="fa fa-check"></i><b>8.8</b> Transformers TBD</a><ul>
<li class="chapter" data-level="8.8.1" data-path="transformers-tbd.html"><a href="transformers-tbd.html#example-transformer-for-time-series-forecasting"><i class="fa fa-check"></i><b>8.8.1</b> Example transformer for time series forecasting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="food-for-the-algorithms-data.html"><a href="food-for-the-algorithms-data.html"><i class="fa fa-check"></i><b>9</b> Food for the algorithms: Data</a><ul>
<li class="chapter" data-level="9.1" data-path="nlp.html"><a href="nlp.html"><i class="fa fa-check"></i><b>9.1</b> NLP</a></li>
<li class="chapter" data-level="9.2" data-path="dataset-search-from-google.html"><a href="dataset-search-from-google.html"><i class="fa fa-check"></i><b>9.2</b> Dataset search from Google</a></li>
</ul></li>
<li class="part"><span><b>III Explainable ML</b></span></li>
<li class="chapter" data-level="10" data-path="ExplainableMl.html"><a href="ExplainableMl.html"><i class="fa fa-check"></i><b>10</b> Explainable ML tbd</a><ul>
<li class="chapter" data-level="10.1" data-path="lime-tbd.html"><a href="lime-tbd.html"><i class="fa fa-check"></i><b>10.1</b> Lime tbd</a></li>
<li class="chapter" data-level="10.2" data-path="alibi-tbd.html"><a href="alibi-tbd.html"><i class="fa fa-check"></i><b>10.2</b> alibi tbd</a></li>
<li class="chapter" data-level="10.3" data-path="tf-explain-tbd.html"><a href="tf-explain-tbd.html"><i class="fa fa-check"></i><b>10.3</b> tf-explain tbd</a></li>
<li class="chapter" data-level="10.4" data-path="keras-salient-object-visualization.html"><a href="keras-salient-object-visualization.html"><i class="fa fa-check"></i><b>10.4</b> keras-salient-object-visualization</a><ul>
<li class="chapter" data-level="10.4.1" data-path="keras-salient-object-visualization.html"><a href="keras-salient-object-visualization.html#explaining-how-a-deep-neural-network-trained-with-end-to-end-learning-steers-a-car"><i class="fa fa-check"></i><b>10.4.1</b> Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car</a></li>
<li class="chapter" data-level="10.4.2" data-path="keras-salient-object-visualization.html"><a href="keras-salient-object-visualization.html#visualbackprop-efficient-visualization-of-cnns"><i class="fa fa-check"></i><b>10.4.2</b> VisualBackProp: efficient visualization of CNNs</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV ML online resources</b></span></li>
<li class="chapter" data-level="11" data-path="MlMooc.html"><a href="MlMooc.html"><i class="fa fa-check"></i><b>11</b> ML online courses</a><ul>
<li class="chapter" data-level="11.1" data-path="coursera.html"><a href="coursera.html"><i class="fa fa-check"></i><b>11.1</b> Coursera</a></li>
<li class="chapter" data-level="11.2" data-path="udemy.html"><a href="udemy.html"><i class="fa fa-check"></i><b>11.2</b> Udemy</a></li>
<li class="chapter" data-level="11.3" data-path="datacamp.html"><a href="datacamp.html"><i class="fa fa-check"></i><b>11.3</b> DataCamp</a></li>
<li class="chapter" data-level="11.4" data-path="udacity.html"><a href="udacity.html"><i class="fa fa-check"></i><b>11.4</b> Udacity</a><ul>
<li class="chapter" data-level="11.4.1" data-path="udacity.html"><a href="udacity.html#example-for-self-driving-car-course-project"><i class="fa fa-check"></i><b>11.4.1</b> Example for self-driving car course project</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="fast-ai.html"><a href="fast-ai.html"><i class="fa fa-check"></i><b>11.5</b> Fast.ai</a></li>
<li class="chapter" data-level="11.6" data-path="fastai.html"><a href="fastai.html"><i class="fa fa-check"></i><b>11.6</b> FastAI</a></li>
<li class="chapter" data-level="11.7" data-path="kaggle-courses.html"><a href="kaggle-courses.html"><i class="fa fa-check"></i><b>11.7</b> Kaggle Courses</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="MlResources.html"><a href="MlResources.html"><i class="fa fa-check"></i><b>12</b> ML online resources</a><ul>
<li class="chapter" data-level="12.1" data-path="in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos.html"><a href="in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos.html"><i class="fa fa-check"></i><b>12.1</b> In-depth introduction to machine learning in 15 hours of expert videos</a><ul>
<li class="chapter" data-level="12.1.1" data-path="in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos.html"><a href="in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos.html#an-introduction-to-statistical-learning"><i class="fa fa-check"></i><b>12.1.1</b> An Introduction to Statistical Learning</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="the-learning-machine.html"><a href="the-learning-machine.html"><i class="fa fa-check"></i><b>12.2</b> The learning machine</a></li>
<li class="chapter" data-level="12.3" data-path="deepai-the-front-page-of-a-i-.html"><a href="deepai-the-front-page-of-a-i-.html"><i class="fa fa-check"></i><b>12.3</b> DeepAI: The front page of A.I.</a></li>
<li class="chapter" data-level="12.4" data-path="tensorflow-tutorials.html"><a href="tensorflow-tutorials.html"><i class="fa fa-check"></i><b>12.4</b> TensorFlow tutorials</a><ul>
<li class="chapter" data-level="12.4.1" data-path="tensorflow-tutorials.html"><a href="tensorflow-tutorials.html#mit-6.s191-introduction-to-deep-learning"><i class="fa fa-check"></i><b>12.4.1</b> MIT 6.S191 Introduction to Deep Learning</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="embedding-projector.html"><a href="embedding-projector.html"><i class="fa fa-check"></i><b>12.5</b> Embedding Projector</a></li>
<li class="chapter" data-level="12.6" data-path="tensorboard-playground.html"><a href="tensorboard-playground.html"><i class="fa fa-check"></i><b>12.6</b> Tensorboard playground</a></li>
<li class="chapter" data-level="12.7" data-path="empowering-companies-to-jumpstart-ai-and-generate-real-world-value.html"><a href="empowering-companies-to-jumpstart-ai-and-generate-real-world-value.html"><i class="fa fa-check"></i><b>12.7</b> Empowering companies to jumpstart AI and generate real-world value</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ml-online-books.html"><a href="ml-online-books.html"><i class="fa fa-check"></i><b>13</b> ML online books</a><ul>
<li class="chapter" data-level="13.1" data-path="neural-networks-and-deep-learning.html"><a href="neural-networks-and-deep-learning.html"><i class="fa fa-check"></i><b>13.1</b> Neural Networks and Deep Learning</a></li>
<li class="chapter" data-level="13.2" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>13.2</b> Deep Learning</a></li>
</ul></li>
<li class="part"><span><b>V Examples from Kaggle</b></span></li>
<li class="chapter" data-level="14" data-path="KaggleExamples.html"><a href="KaggleExamples.html"><i class="fa fa-check"></i><b>14</b> Examples in Kaggle</a></li>
<li class="chapter" data-level="15" data-path="melbourne-university-aesmathworksnih-seizure-prediction.html"><a href="melbourne-university-aesmathworksnih-seizure-prediction.html"><i class="fa fa-check"></i><b>15</b> Melbourne University AES/MathWorks/NIH Seizure Prediction</a><ul>
<li class="chapter" data-level="15.1" data-path="winning-solution-1st.html"><a href="winning-solution-1st.html"><i class="fa fa-check"></i><b>15.1</b> Winning solution (1st)</a><ul>
<li class="chapter" data-level="15.1.1" data-path="winning-solution-1st.html"><a href="winning-solution-1st.html#alex-gilberto-models"><i class="fa fa-check"></i><b>15.1.1</b> Alex / Gilberto models</a></li>
<li class="chapter" data-level="15.1.2" data-path="winning-solution-1st.html"><a href="winning-solution-1st.html#feng-models"><i class="fa fa-check"></i><b>15.1.2</b> Feng models</a></li>
<li class="chapter" data-level="15.1.3" data-path="winning-solution-1st.html"><a href="winning-solution-1st.html#andriy-models"><i class="fa fa-check"></i><b>15.1.3</b> Andriy models</a></li>
<li class="chapter" data-level="15.1.4" data-path="winning-solution-1st.html"><a href="winning-solution-1st.html#code-on-github"><i class="fa fa-check"></i><b>15.1.4</b> Code on GitHub</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="solution4th-place.html"><a href="solution4th-place.html"><i class="fa fa-check"></i><b>15.2</b> Solution(4th place)</a><ul>
<li class="chapter" data-level="15.2.1" data-path="solution4th-place.html"><a href="solution4th-place.html#pre-processing-6"><i class="fa fa-check"></i><b>15.2.1</b> Pre-processing</a></li>
<li class="chapter" data-level="15.2.2" data-path="solution4th-place.html"><a href="solution4th-place.html#features-3"><i class="fa fa-check"></i><b>15.2.2</b> Features</a></li>
<li class="chapter" data-level="15.2.3" data-path="solution4th-place.html"><a href="solution4th-place.html#model"><i class="fa fa-check"></i><b>15.2.3</b> Model</a></li>
<li class="chapter" data-level="15.2.4" data-path="solution4th-place.html"><a href="solution4th-place.html#github-code"><i class="fa fa-check"></i><b>15.2.4</b> GitHub code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="bosch-production-line-performance.html"><a href="bosch-production-line-performance.html"><i class="fa fa-check"></i><b>16</b> Bosch Production Line Performance</a><ul>
<li class="chapter" data-level="16.1" data-path="st-place-solution.html"><a href="st-place-solution.html"><i class="fa fa-check"></i><b>16.1</b> 1st place solution</a><ul>
<li class="chapter" data-level="16.1.1" data-path="st-place-solution.html"><a href="st-place-solution.html#data-exploration"><i class="fa fa-check"></i><b>16.1.1</b> Data exploration</a></li>
<li class="chapter" data-level="16.1.2" data-path="st-place-solution.html"><a href="st-place-solution.html#hand-crafted-features"><i class="fa fa-check"></i><b>16.1.2</b> Hand crafted features</a></li>
<li class="chapter" data-level="16.1.3" data-path="st-place-solution.html"><a href="st-place-solution.html#hardware"><i class="fa fa-check"></i><b>16.1.3</b> Hardware</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="rd-place-solution-tbd.html"><a href="rd-place-solution-tbd.html"><i class="fa fa-check"></i><b>16.2</b> 3rd place solution TBD</a></li>
<li class="chapter" data-level="16.3" data-path="th-place-solution-with-github.html"><a href="th-place-solution-with-github.html"><i class="fa fa-check"></i><b>16.3</b> 8th place solution with GitHub</a><ul>
<li class="chapter" data-level="16.3.1" data-path="th-place-solution-with-github.html"><a href="th-place-solution-with-github.html#overall-architecture"><i class="fa fa-check"></i><b>16.3.1</b> Overall architecture</a></li>
<li class="chapter" data-level="16.3.2" data-path="th-place-solution-with-github.html"><a href="th-place-solution-with-github.html#input-data-sets"><i class="fa fa-check"></i><b>16.3.2</b> Input data sets</a></li>
<li class="chapter" data-level="16.3.3" data-path="th-place-solution-with-github.html"><a href="th-place-solution-with-github.html#ensembling"><i class="fa fa-check"></i><b>16.3.3</b> Ensembling</a></li>
<li class="chapter" data-level="16.3.4" data-path="th-place-solution-with-github.html"><a href="th-place-solution-with-github.html#features-4"><i class="fa fa-check"></i><b>16.3.4</b> Features</a></li>
<li class="chapter" data-level="16.3.5" data-path="th-place-solution-with-github.html"><a href="th-place-solution-with-github.html#validation-method"><i class="fa fa-check"></i><b>16.3.5</b> Validation method</a></li>
<li class="chapter" data-level="16.3.6" data-path="th-place-solution-with-github.html"><a href="th-place-solution-with-github.html#software-1"><i class="fa fa-check"></i><b>16.3.6</b> Software</a></li>
<li class="chapter" data-level="16.3.7" data-path="th-place-solution-with-github.html"><a href="th-place-solution-with-github.html#code-on-github-1"><i class="fa fa-check"></i><b>16.3.7</b> Code on GitHub</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="corporación-favorita-grocery-sales-forecasting.html"><a href="corporación-favorita-grocery-sales-forecasting.html"><i class="fa fa-check"></i><b>17</b> Corporación Favorita Grocery Sales Forecasting</a><ul>
<li class="chapter" data-level="17.1" data-path="st-place-solution-1.html"><a href="st-place-solution-1.html"><i class="fa fa-check"></i><b>17.1</b> 1st place solution</a></li>
<li class="chapter" data-level="17.2" data-path="th-place-solution-overview.html"><a href="th-place-solution-overview.html"><i class="fa fa-check"></i><b>17.2</b> 4th-Place Solution Overview</a></li>
<li class="chapter" data-level="17.3" data-path="th-place-solution.html"><a href="th-place-solution.html"><i class="fa fa-check"></i><b>17.3</b> 5th Place Solution</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="severstal-steel-defect-detection.html"><a href="severstal-steel-defect-detection.html"><i class="fa fa-check"></i><b>18</b> Severstal: Steel Defect Detection</a></li>
<li class="chapter" data-level="19" data-path="lyft-3d-object-detection-for-autonomous-vehicles.html"><a href="lyft-3d-object-detection-for-autonomous-vehicles.html"><i class="fa fa-check"></i><b>19</b> Lyft 3D Object Detection for Autonomous Vehicles</a><ul>
<li class="chapter" data-level="19.1" data-path="rd-place-solution.html"><a href="rd-place-solution.html"><i class="fa fa-check"></i><b>19.1</b> 3rd place solution</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="aptos-2019-blindness-detection.html"><a href="aptos-2019-blindness-detection.html"><i class="fa fa-check"></i><b>20</b> APTOS 2019 Blindness Detection</a><ul>
<li class="chapter" data-level="20.1" data-path="st-place-solution-summary.html"><a href="st-place-solution-summary.html"><i class="fa fa-check"></i><b>20.1</b> 1st place solution summary</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="predicting-molecular-properties.html"><a href="predicting-molecular-properties.html"><i class="fa fa-check"></i><b>21</b> Predicting Molecular Properties</a><ul>
<li class="chapter" data-level="21.1" data-path="solution-hybrid.html"><a href="solution-hybrid.html"><i class="fa fa-check"></i><b>21.1</b> #1 Solution - hybrid</a><ul>
<li class="chapter" data-level="21.1.1" data-path="solution-hybrid.html"><a href="solution-hybrid.html#overall-architecture-1"><i class="fa fa-check"></i><b>21.1.1</b> Overall architecture</a></li>
<li class="chapter" data-level="21.1.2" data-path="solution-hybrid.html"><a href="solution-hybrid.html#ensembling-1"><i class="fa fa-check"></i><b>21.1.2</b> Ensembling</a></li>
<li class="chapter" data-level="21.1.3" data-path="solution-hybrid.html"><a href="solution-hybrid.html#hardware-1"><i class="fa fa-check"></i><b>21.1.3</b> Hardware</a></li>
<li class="chapter" data-level="21.1.4" data-path="solution-hybrid.html"><a href="solution-hybrid.html#software-2"><i class="fa fa-check"></i><b>21.1.4</b> Software</a></li>
<li class="chapter" data-level="21.1.5" data-path="solution-hybrid.html"><a href="solution-hybrid.html#code-on-github-2"><i class="fa fa-check"></i><b>21.1.5</b> Code on GitHub</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="solution-quantum-uncertainty.html"><a href="solution-quantum-uncertainty.html"><i class="fa fa-check"></i><b>21.2</b> #2 solution 🤖 Quantum Uncertainty 🤖</a><ul>
<li class="chapter" data-level="21.2.1" data-path="solution-quantum-uncertainty.html"><a href="solution-quantum-uncertainty.html#overall-architecture-2"><i class="fa fa-check"></i><b>21.2.1</b> Overall architecture</a></li>
<li class="chapter" data-level="21.2.2" data-path="solution-quantum-uncertainty.html"><a href="solution-quantum-uncertainty.html#input-features-and-embeddings-1"><i class="fa fa-check"></i><b>21.2.2</b> Input features and embeddings</a></li>
<li class="chapter" data-level="21.2.3" data-path="solution-quantum-uncertainty.html"><a href="solution-quantum-uncertainty.html#data-augmentation"><i class="fa fa-check"></i><b>21.2.3</b> Data augmentation</a></li>
<li class="chapter" data-level="21.2.4" data-path="solution-quantum-uncertainty.html"><a href="solution-quantum-uncertainty.html#ensembling-2"><i class="fa fa-check"></i><b>21.2.4</b> Ensembling</a></li>
<li class="chapter" data-level="21.2.5" data-path="solution-quantum-uncertainty.html"><a href="solution-quantum-uncertainty.html#hardware-2"><i class="fa fa-check"></i><b>21.2.5</b> Hardware</a></li>
<li class="chapter" data-level="21.2.6" data-path="solution-quantum-uncertainty.html"><a href="solution-quantum-uncertainty.html#software-3"><i class="fa fa-check"></i><b>21.2.6</b> Software</a></li>
<li class="chapter" data-level="21.2.7" data-path="solution-quantum-uncertainty.html"><a href="solution-quantum-uncertainty.html#code-on-github-3"><i class="fa fa-check"></i><b>21.2.7</b> Code on GitHub</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="local-examples.html"><a href="local-examples.html"><i class="fa fa-check"></i><b>22</b> Local examples</a><ul>
<li class="chapter" data-level="22.1" data-path="master-autonomous-driving.html"><a href="master-autonomous-driving.html"><i class="fa fa-check"></i><b>22.1</b> Master Autonomous Driving</a></li>
<li class="chapter" data-level="22.2" data-path="university-suttgart-indoor-ortung-mit-mobilfunk.html"><a href="university-suttgart-indoor-ortung-mit-mobilfunk.html"><i class="fa fa-check"></i><b>22.2</b> University Suttgart: Indoor-Ortung mit Mobilfunk</a></li>
<li class="chapter" data-level="22.3" data-path="bionic-learning-network.html"><a href="bionic-learning-network.html"><i class="fa fa-check"></i><b>22.3</b> Bionic Learning Network</a></li>
</ul></li>
<li class="part"><span><b>VI Real world example</b></span></li>
<li class="chapter" data-level="23" data-path="RealWorld.html"><a href="RealWorld.html"><i class="fa fa-check"></i><b>23</b> Real world example</a><ul>
<li class="chapter" data-level="23.1" data-path="subject-of-the-project.html"><a href="subject-of-the-project.html"><i class="fa fa-check"></i><b>23.1</b> Subject of the project</a><ul>
<li class="chapter" data-level="" data-path="subject-of-the-project.html"><a href="subject-of-the-project.html#depending-from-where-you-were-looking"><i class="fa fa-check"></i>Depending from where you were looking:</a></li>
<li class="chapter" data-level="" data-path="subject-of-the-project.html"><a href="subject-of-the-project.html#looking-from-the-perspective-of-machine-learning-expert"><i class="fa fa-check"></i>Looking from the perspective of machine learning expert</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="project-phases.html"><a href="project-phases.html"><i class="fa fa-check"></i><b>23.2</b> Project phases</a><ul>
<li class="chapter" data-level="" data-path="project-phases.html"><a href="project-phases.html#the-main-project-phases-are"><i class="fa fa-check"></i>The main project phases are:</a></li>
<li class="chapter" data-level="" data-path="project-phases.html"><a href="project-phases.html#after-data-gathering-iteration-is-trump"><i class="fa fa-check"></i>After data gathering iteration is trump</a></li>
<li class="chapter" data-level="23.2.1" data-path="project-phases.html"><a href="project-phases.html#feature-engineering"><i class="fa fa-check"></i><b>23.2.1</b> Feature engineering</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="algorithm-selection.html"><a href="algorithm-selection.html"><i class="fa fa-check"></i><b>23.3</b> Algorithm selection</a><ul>
<li class="chapter" data-level="23.3.1" data-path="algorithm-selection.html"><a href="algorithm-selection.html#logistic-regression"><i class="fa fa-check"></i><b>23.3.1</b> Logistic regression</a></li>
<li class="chapter" data-level="23.3.2" data-path="algorithm-selection.html"><a href="algorithm-selection.html#tree-based"><i class="fa fa-check"></i><b>23.3.2</b> Tree based</a></li>
<li class="chapter" data-level="23.3.3" data-path="algorithm-selection.html"><a href="algorithm-selection.html#support-vector-machine-svm-tbd"><i class="fa fa-check"></i><b>23.3.3</b> Support Vector Machine (SVM) TBD</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="ConfusionMatrixRocAuc.html"><a href="ConfusionMatrixRocAuc.html"><i class="fa fa-check"></i><b>23.4</b> Performance measurement</a><ul>
<li class="chapter" data-level="23.4.1" data-path="ConfusionMatrixRocAuc.html"><a href="ConfusionMatrixRocAuc.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>23.4.1</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="23.4.2" data-path="ConfusionMatrixRocAuc.html"><a href="ConfusionMatrixRocAuc.html#RocExplanation"><i class="fa fa-check"></i><b>23.4.2</b> Receiver operating characteristic (ROC)</a></li>
</ul></li>
<li class="chapter" data-level="23.5" data-path="confusion-matrix-and-roc-for-pulse.html"><a href="confusion-matrix-and-roc-for-pulse.html"><i class="fa fa-check"></i><b>23.5</b> Confusion matrix and ROC for pulse</a><ul>
<li class="chapter" data-level="23.5.1" data-path="confusion-matrix-and-roc-for-pulse.html"><a href="confusion-matrix-and-roc-for-pulse.html#r-plots"><i class="fa fa-check"></i><b>23.5.1</b> R Plots</a></li>
</ul></li>
<li class="chapter" data-level="23.6" data-path="create-augmented-labeled-data.html"><a href="create-augmented-labeled-data.html"><i class="fa fa-check"></i><b>23.6</b> Create augmented labeled data</a><ul>
<li class="chapter" data-level="23.6.1" data-path="create-augmented-labeled-data.html"><a href="create-augmented-labeled-data.html#features-of-time-signals"><i class="fa fa-check"></i><b>23.6.1</b> Features of time signals</a></li>
</ul></li>
<li class="chapter" data-level="23.7" data-path="features-generated.html"><a href="features-generated.html"><i class="fa fa-check"></i><b>23.7</b> Features generated</a><ul>
<li class="chapter" data-level="23.7.1" data-path="features-generated.html"><a href="features-generated.html#analysis-of-generated-features"><i class="fa fa-check"></i><b>23.7.1</b> Analysis of generated features</a></li>
<li class="chapter" data-level="23.7.2" data-path="features-generated.html"><a href="features-generated.html#dynamic-time-warp-dtw-for-signal"><i class="fa fa-check"></i><b>23.7.2</b> Dynamic time warp (DTW) for signal</a></li>
</ul></li>
<li class="chapter" data-level="23.8" data-path="algorithm.html"><a href="algorithm.html"><i class="fa fa-check"></i><b>23.8</b> Algorithm</a></li>
<li class="chapter" data-level="23.9" data-path="confusion-matrix-results-logistic-regression-for-measured-data.html"><a href="confusion-matrix-results-logistic-regression-for-measured-data.html"><i class="fa fa-check"></i><b>23.9</b> Confusion matrix results logistic regression for measured data</a><ul>
<li class="chapter" data-level="23.9.1" data-path="confusion-matrix-results-logistic-regression-for-measured-data.html"><a href="confusion-matrix-results-logistic-regression-for-measured-data.html#roc-results-for-measured-data"><i class="fa fa-check"></i><b>23.9.1</b> ROC results for measured data</a></li>
</ul></li>
<li class="chapter" data-level="23.10" data-path="several-algorithms-results-for-snr-18db.html"><a href="several-algorithms-results-for-snr-18db.html"><i class="fa fa-check"></i><b>23.10</b> Several algorithms results for SNR = 18dB</a><ul>
<li class="chapter" data-level="23.10.1" data-path="several-algorithms-results-for-snr-18db.html"><a href="several-algorithms-results-for-snr-18db.html#roc-results-for-snr-18db"><i class="fa fa-check"></i><b>23.10.1</b> ROC results for SNR 18dB</a></li>
</ul></li>
<li class="chapter" data-level="23.11" data-path="compare-models-for-snr-18db.html"><a href="compare-models-for-snr-18db.html"><i class="fa fa-check"></i><b>23.11</b> Compare models for SNR = 18dB</a></li>
<li class="chapter" data-level="23.12" data-path="optimize-ml-hyper-parameter.html"><a href="optimize-ml-hyper-parameter.html"><i class="fa fa-check"></i><b>23.12</b> Optimize ML hyper parameter</a></li>
</ul></li>
<li class="part"><span><b>VII Cloud-based machine learning</b></span></li>
<li class="chapter" data-level="24" data-path="CloudBasedMl.html"><a href="CloudBasedMl.html"><i class="fa fa-check"></i><b>24</b> Cloud-based machine learning</a></li>
<li class="part"><span><b>VIII Kaggle Survey</b></span></li>
<li class="chapter" data-level="25" data-path="KaggleSurvey.html"><a href="KaggleSurvey.html"><i class="fa fa-check"></i><b>25</b> Kaggle survey introduction</a><ul>
<li class="chapter" data-level="25.1" data-path="kaggle-survey-details.html"><a href="kaggle-survey-details.html"><i class="fa fa-check"></i><b>25.1</b> Kaggle survey details</a></li>
<li class="chapter" data-level="25.2" data-path="purpose.html"><a href="purpose.html"><i class="fa fa-check"></i><b>25.2</b> Purpose</a></li>
<li class="chapter" data-level="25.3" data-path="navigation-and-handling.html"><a href="navigation-and-handling.html"><i class="fa fa-check"></i><b>25.3</b> Navigation and handling</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>26</b> Results</a><ul>
<li class="chapter" data-level="26.1" data-path="survey-participants-education-level.html"><a href="survey-participants-education-level.html"><i class="fa fa-check"></i><b>26.1</b> Survey participants education level</a></li>
<li class="chapter" data-level="26.2" data-path="who-uses-which-algorithm.html"><a href="who-uses-which-algorithm.html"><i class="fa fa-check"></i><b>26.2</b> Who uses which algorithm</a></li>
<li class="chapter" data-level="26.3" data-path="machine-learning-experience-and-algorithms.html"><a href="machine-learning-experience-and-algorithms.html"><i class="fa fa-check"></i><b>26.3</b> Machine learning experience and algorithms</a></li>
<li class="chapter" data-level="26.4" data-path="experience-and-new-algorithms.html"><a href="experience-and-new-algorithms.html"><i class="fa fa-check"></i><b>26.4</b> Experience and new algorithms</a></li>
<li class="chapter" data-level="26.5" data-path="role-of-participants.html"><a href="role-of-participants.html"><i class="fa fa-check"></i><b>26.5</b> Role of participants</a></li>
<li class="chapter" data-level="26.6" data-path="company-size.html"><a href="company-size.html"><i class="fa fa-check"></i><b>26.6</b> Company size</a></li>
<li class="chapter" data-level="26.7" data-path="company-incorporation-of-machine-learning.html"><a href="company-incorporation-of-machine-learning.html"><i class="fa fa-check"></i><b>26.7</b> Company incorporation of machine learning</a></li>
<li class="chapter" data-level="26.8" data-path="favourite-media-sources-on-data-science-topics.html"><a href="favourite-media-sources-on-data-science-topics.html"><i class="fa fa-check"></i><b>26.8</b> Favourite media sources on data science topics</a></li>
<li class="chapter" data-level="26.9" data-path="favourite-online-course-platform.html"><a href="favourite-online-course-platform.html"><i class="fa fa-check"></i><b>26.9</b> Favourite online course platform</a></li>
<li class="chapter" data-level="26.10" data-path="favourite-data-analyzing-tool.html"><a href="favourite-data-analyzing-tool.html"><i class="fa fa-check"></i><b>26.10</b> Favourite data analyzing tool</a></li>
<li class="chapter" data-level="26.11" data-path="experience-in-data-analysis-coding.html"><a href="experience-in-data-analysis-coding.html"><i class="fa fa-check"></i><b>26.11</b> Experience in data analysis coding</a></li>
<li class="chapter" data-level="26.12" data-path="favourite-integrated-development-environments-ides.html"><a href="favourite-integrated-development-environments-ides.html"><i class="fa fa-check"></i><b>26.12</b> Favourite integrated development environments (IDE’s)</a></li>
<li class="chapter" data-level="26.13" data-path="favourite-hosted-notebook-products.html"><a href="favourite-hosted-notebook-products.html"><i class="fa fa-check"></i><b>26.13</b> Favourite hosted notebook products</a></li>
<li class="chapter" data-level="26.14" data-path="favourite-programming-languages.html"><a href="favourite-programming-languages.html"><i class="fa fa-check"></i><b>26.14</b> Favourite programming languages</a></li>
<li class="chapter" data-level="26.15" data-path="recommended-entry-programming-language.html"><a href="recommended-entry-programming-language.html"><i class="fa fa-check"></i><b>26.15</b> Recommended entry programming language</a></li>
<li class="chapter" data-level="26.16" data-path="favourite-data-visualization-libraries-or-tools.html"><a href="favourite-data-visualization-libraries-or-tools.html"><i class="fa fa-check"></i><b>26.16</b> Favourite data visualization libraries or tools</a></li>
<li class="chapter" data-level="26.17" data-path="favourite-specialized-hardware.html"><a href="favourite-specialized-hardware.html"><i class="fa fa-check"></i><b>26.17</b> Favourite specialized hardware</a></li>
<li class="chapter" data-level="26.18" data-path="favourite-machine-learning-frameworks.html"><a href="favourite-machine-learning-frameworks.html"><i class="fa fa-check"></i><b>26.18</b> Favourite machine learning frameworks</a></li>
<li class="chapter" data-level="26.19" data-path="favourite-cloud-computing-platforms.html"><a href="favourite-cloud-computing-platforms.html"><i class="fa fa-check"></i><b>26.19</b> Favourite cloud computing platforms</a></li>
<li class="chapter" data-level="26.20" data-path="favourite-big-data-analytics-products.html"><a href="favourite-big-data-analytics-products.html"><i class="fa fa-check"></i><b>26.20</b> Favourite big data / analytics products</a></li>
<li class="chapter" data-level="26.21" data-path="favourite-automated-machine-learning-tools-or-partial-automl-tools.html"><a href="favourite-automated-machine-learning-tools-or-partial-automl-tools.html"><i class="fa fa-check"></i><b>26.21</b> Favourite automated machine learning tools (or partial AutoML tools)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning orientation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="featureEngineering" class="section level2">
<h2><span class="header-section-number">6.4</span> Feature engineering</h2>
<p>Sometimes even the best models have less than useful predictive performance. This could be because key relationships are not directly available.</p>
<div class="rmdtip">
<p>
Key relationships that are not directly available: <img src="images/search.svg" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Transformation of a predictor</li>
<li>Interaction of two or more predictors such as a product or ratio</li>
<li>Functional relationship among predictors</li>
<li>Equivalent re-representation of a predictor</li>
</ul>
</div>
<p>Feature engineering can be defined as:</p>
<blockquote>
<p>Adjusting and reworking the predictors to enable models to better uncover predictor-response relationships has been termed feature engineering.</p>
<span class="citation">(Kuhn and Johnson <a href="#ref-FeatureEngineeringKuhnWebsite" role="doc-biblioref">2018</a>)</span>
</blockquote>
<p>Since there are different faculties which work on machine learning (staticians, computer scientists, neurologists) there are different terms for the same thing, so variables that go into model are called:</p>

<div class="rmdtip">
<ul>
<li><p>
Predictors<img src="images/In.png" alt="Smiley face" align="right" style="width:10%;">
</p></li>
<li>Features</li>
<li>Independent variables</li>
</ul>
</div>

<p>Quantity being modeled called:</p>

<div class="rmdtip">
<ul>
<li><p>
Prediction<img src="images/Out.png" alt="Smiley face" align="right" style="width:10%;">
</p></li>
<li>Outcome</li>
<li>Response</li>
<li>Dependent variable</li>
</ul>
</div>

<p>So machine learning is nothing more spectacular than finding a function that maps the input, features, to the output, response as close as possible.</p>
<div class="rmdtip">
<p><span class="math display">\[outcome = f(features) = f(X_1, X_2, \dots, Xp) = f(X)\]</span></p>
<p><span class="math display">\[\hat{Y} = \hat{f}(X)\]</span></p>
</div>
<div id="encoding-categorical-predictors" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Encoding Categorical Predictors</h3>
<p>Categorical features are not quantitative nature but <strong>qualitative data</strong>, i.e. discrete values in an ordered relationship
For numerical features it is straight forward to feed them into a model, but how can categorical data be handled?</p>
<div class="rmdtip">
<p>
Categorical features: <img src="images/mail.svg" alt="Smiley face" align="right" style="width:15%;">
</p>
<ul>
<li>Gender</li>
<li>Breeds of dogs</li>
<li>Postal code</li>
</ul>
</div>
<p>Ordered categorical features have a clear progression of values</p>
<div class="rmdtip">
<p>
Ordered categorical features:<img src="images/badGood.png" alt="Smiley face" align="right" style="width:30%;">
</p>
<ul>
<li>Bad</li>
<li>Good</li>
<li>Better</li>
</ul>
</div>
<p>Ordered and unordered features might be handled differently to include the embedded information into a model.</p>
<p>Not all models need a special treatment for categorical data</p>
<div class="rmdtip">
<p>
Models that can deal with categorical features:<img src="images/tree.svg" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Tree based models</li>
<li>Naive Bayes models</li>
</ul>
</div>
<p>one hot vs dummy variable</p>
<p>TBC</p>
<p>add
- bert
- word2vec
- one hot encoding
- dummy variable</p>
<p>This are ways how to modify qualitative features For continuous, real number values exists other tools for converting them into features that can be better utilized by a model.</p>
</div>
<div id="engineering-numeric-features" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Engineering numeric features</h3>
<p>Features with continuous, real number values might need some pre-processing before being useful for a model. The pre-processing can be model dependent</p>
<div class="rmdtip">
<p>
Model dependent pre-processing:<img src="images/SkewedDistribution.png" alt="Smiley face" align="right" style="width:30%;">
</p>
<ul>
<li>Skewed distribution
<ul>
<li>No problem for:
<ul>
<li>Trees (based on rank rather than values)</li>
</ul></li>
<li>Problem for:
<ul>
<li>K-nearest neighbors</li>
<li>SVM</li>
<li>Neural networks</li>
</ul></li>
</ul></li>
<li>Correlated features
<ul>
<li>No problem for:
<ul>
<li>Partial least square</li>
</ul></li>
<li>Problem for:
<ul>
<li>Neural networks</li>
<li>Multiple linear regression</li>
</ul></li>
</ul></li>
</ul>
</div>
<p>Commonly occurring issues for numerical features and the counter measures are treated in this chapter</p>
<div class="rmdtip">
<p>
Commonly occurring issues:<img src="images/issues.svg" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Vastly different scales</li>
<li>Skewed distribution</li>
<li>Small number of extreme values</li>
<li>Values clipped at on the low/or high end of range</li>
<li>Complex relationship with response</li>
<li>Redundant information</li>
</ul>
</div>
<p>Those problems are addressed by various methods in the next chapters</p>
<div class="rmdtip">
<p>
Remedies to commonly occurring issues:<img src="images/remedies.png" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Transform individual feature’s distribution in chapter <a href="featureEngineering.html#Transformations">6.4.2.1</a>
<ul>
<li>avoid skewed distribution</li>
</ul></li>
<li>Expanding individual features into many features in chapter <a href="featureEngineering.html#OneManyTransformations">6.4.2.2</a>
<ul>
<li>helps with complex relationships with response</li>
</ul></li>
<li>Mapping higher to lower dimensional space in chapter <a href="featureEngineering.html#ManytoManyTransformations">6.4.2.3</a>
<ul>
<li>helps with redundant information</li>
</ul></li>
</ul>
</div>
<div id="Transformations" class="section level4">
<h4><span class="header-section-number">6.4.2.1</span> 1:1 Transformations</h4>
<p>There are plenty of methods to transform a feature, first focus is on distribution shaping</p>
<div class="HeadingNoNumber">
<p>Box-Cox transformation</p>
</div>
<p>Apply the Box-Cox transformation the <strong>skewed distribution below</strong></p>
<div class="figure">
<img src="images/SkewedDistribution.png" class="external" style="width:80.0%" alt="" />
<p class="caption">Figure from <span class="citation">(Kuhn and Johnson <a href="#ref-FeatureEngineeringKuhnWebsite" role="doc-biblioref">2018</a>)</span></p>
</div>
<p>can be shaped by Box-Cox transformation to be almost normally distributed as shown below.</p>
<div class="figure">
<img src="images/BoxCoxShapedDistribution.png" class="external" style="width:80.0%" alt="" />
<p class="caption">Figure from <span class="citation">(Kuhn and Johnson <a href="#ref-FeatureEngineeringKuhnWebsite" role="doc-biblioref">2018</a>)</span></p>
</div>
<p>Box-Cox transformation maps the original data by the following equation</p>
<div class="rmdmath">
<p>Box-Cox transformation equation:</p>
<p><span class="math display">\[x^{*}=\left\{\begin{array}{ll}{\frac{x^{\lambda}-1}{\lambda \bar{x}^{\lambda-1}},} &amp; {\lambda \neq 0} \\ {\tilde{x} \log x,} &amp; {\lambda=0}\end{array}\right.\]</span></p>
<hr />
<p>where</p>
<p><span class="math inline">\(\tilde{x}\)</span> is geometric mean <span class="math inline">\(\bar{x}=\sqrt[n]{\prod_{i=1}^{n} x_{i}}=\sqrt[n]{x_{1} \cdot x_{2} \cdots x_{n}}\)</span></p>
</div>
<p><span class="math inline">\(\lambda\)</span> is estimated from the data, there are a few special cases</p>
<div class="rmdtip">
<p>Special cases for <span class="math inline">\(\lambda\)</span>:</p>
<ul>
<li><span class="math inline">\(\lambda\)</span> = 1 <span class="math inline">\(\implies\)</span> no transformation</li>
<li><span class="math inline">\(\lambda\)</span> = 0.5 <span class="math inline">\(\implies\)</span> square root transformation</li>
<li><span class="math inline">\(\lambda\)</span> = -1 <span class="math inline">\(\implies\)</span> inverse transformation</li>
</ul>
</div>
</div>
<div id="OneManyTransformations" class="section level4">
<h4><span class="header-section-number">6.4.2.2</span> 1:Many Transformations</h4>
<p>A single numeric feature can be expanded to many features to improve model performance.</p>
<div class="rmdmath">
<p>The cubic expansion can be written as:</p>
<p><span class="math display">\[f(x)=\sum_{i=1}^{3} \beta_{i} f_{i}(x)=\beta_{1} x+\beta_{2} x^{2}+\beta_{3} x^{3}\]</span></p>
</div>
<p>The <span class="math inline">\(\beta\)</span> values can be calculated using linear regression. If the true trend were linear the second and third regression parameter would be close to zero.</p>
<p>For example the <strong>AMES sale price vs lot area </strong> as given below is clearly <strong>not linear.</strong></p>
<div class="figure">
<img src="images/numeric-ames-lot-size-1.svg" class="external" style="width:80.0%" alt="" />
<p class="caption">Figure from <span class="citation">(Kuhn and Johnson <a href="#ref-FeatureEngineeringKuhnWebsite" role="doc-biblioref">2018</a>)</span></p>
</div>
<p>Considering a spline with 6 regions. Functions of x are shown in figure <em>“(a)”</em> below.
Figure <em>“(b)”</em> shows the spline</p>
<div class="figure">
<img src="images/numeric-ames-ns-1.svg" class="external" style="width:80.0%" alt="" />
<p class="caption">Figure from <span class="citation">(Kuhn and Johnson <a href="#ref-FeatureEngineeringKuhnWebsite" role="doc-biblioref">2018</a>)</span></p>
</div>
<div class="HeadingNoNumber">
<p>One feature was replaced by 6 hopefully more meaningful features for the model</p>
</div>
</div>
<div id="ManytoManyTransformations" class="section level4">
<h4><span class="header-section-number">6.4.2.3</span> Many:Many Transformations</h4>
<p>Many to many transformation can solve a variety of issues</p>
<div class="rmdtip">
<p>
Many to many transformations help to:<img src="images/outlier.svg" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Deal with
<ul>
<li>outliers</li>
<li>collinearity (high correlation)</li>
</ul></li>
<li>Reduce dimensionality</li>
</ul>
</div>
<p>Contrary to intuition more features are not always beneficial</p>
<div class="rmdtip">
<p>
Considering irrelevant features leads to:<img src="images/compEffort.png" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Increasing computational effort for model training</li>
<li>Decrease predictive performance</li>
<li>Complicating predictor importance calculation</li>
</ul>
</div>
<p>There are several dimension reduction algorithms</p>
<div class="rmdtip">
<p>
Dimension reduction algorithms:<img src="images/supervised.svg" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Unsupervised
<ul>
<li>principle component analysis (PCA)</li>
<li>t-SNE</li>
<li>independent component analysis (ICA)</li>
<li>non-negative matrix factorization (NNMF)</li>
<li>autoencoders</li>
</ul></li>
<li>Supervised
<ul>
<li>partial least squares (PLS)</li>
<li>reduced predictor space is optimally associated with the response</li>
</ul></li>
</ul>
</div>
<p>The <strong>embedding projector of TensorFlow</strong> at <a href="http://projector.tensorflow.org" class="uri">http://projector.tensorflow.org</a> gives great insight into PCA and t-SNE
Below a PCA of the Word2Vec 200 dimensional space onto three dimensions, the word <em>“neural”</em> and points close by are marked.</p>
<p><img src="images/projectorPlayground.png" class="external" style="width:100.0%" alt="Figure from http://projector.tensorflow.org" />
A few more details on some of the above mentioned algorithms</p>
<div class="HeadingNoNumber">
<p>Principle component analysis (PCA)</p>
</div>
<div class="rmdtip">
<p>
Principle component analysis (PCA):<img src="images/irsisPca.png" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Linear orthogonal transformation</li>
<li>Possibly correlated variables</li>
<li>Into set of uncorrelated variables
<ul>
<li>principle components</li>
</ul></li>
</ul>
</div>
<p>PCA maps a higher dimensional space to a lower dimensional space by linear orthogonal transformations. The orthogonality results from the condition that principle components are not correlated. Each principle component explains as much variance as possible. The principle components are weighted combinations of the independent variables. More on PCA can be found at Wikipedia <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" class="uri">https://en.wikipedia.org/wiki/Principal_component_analysis</a></p>
<p>A PCA of the irsis data set is given at <a href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py" class="uri">https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py</a>, it creates the graph below.</p>
<hr />
<p><img src="images/irsisPca.png" class="external" style="width:80.0%" /></p>
<hr />
<p>A minimal PCA Python example is given below, source <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="featureEngineering.html#cb1-1"></a></span>
<span id="cb1-2"><a href="featureEngineering.html#cb1-2"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="featureEngineering.html#cb1-3"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">from</span> sklearn.decomposition <span class="im">import</span> IncrementalPCA</span>
<span id="cb1-4"><a href="featureEngineering.html#cb1-4"></a><span class="op">&gt;&gt;&gt;</span> X <span class="op">=</span> np.array([[<span class="op">-</span><span class="dv">1</span>, <span class="dv">-1</span>], [<span class="op">-</span><span class="dv">2</span>, <span class="dv">-1</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">-2</span>], [<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">3</span>, <span class="dv">2</span>]])</span>
<span id="cb1-5"><a href="featureEngineering.html#cb1-5"></a><span class="op">&gt;&gt;&gt;</span> ipca <span class="op">=</span> IncrementalPCA(n_components<span class="op">=</span><span class="dv">2</span>, batch_size<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-6"><a href="featureEngineering.html#cb1-6"></a><span class="op">&gt;&gt;&gt;</span> ipca.fit(X)</span>
<span id="cb1-7"><a href="featureEngineering.html#cb1-7"></a>IncrementalPCA(batch_size<span class="op">=</span><span class="dv">3</span>, n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-8"><a href="featureEngineering.html#cb1-8"></a><span class="op">&gt;&gt;&gt;</span> ipca.transform(X) </span></code></pre></div>
<div class="HeadingNoNumber">
<p>t-distributed stochastic neighbor embedding (t-SNE)</p>
</div>
<p>The next algorithm is t-distributed stochastic neighbor embedding <strong>t-SNE</strong> which focuses more on the distance of samples than on variance. The main concept is that samples which are nearby in high dimensional space are also nearby in the lower dimensional space.</p>
<div class="rmdtip">
<p>
t-SNE:<img src="images/t-sneExample.png" alt="Smiley face" align="right" style="width:30%;">
</p>
<ul>
<li>t-distributed stochastic neighbor embedding</li>
<li>Nonlinear dimensionality reduction</li>
<li>Similar object modeled by nearby points</li>
</ul>
</div>
<p>The t-SNE algorithm has important hyperparameter which need to be set with some insight. More on those hyperparameter and their settings can be found at <a href="https://distill.pub/2016/misread-tsne/" class="uri">https://distill.pub/2016/misread-tsne/</a> and in Laurens van der Maaten’s original paper on t-SNE <span class="citation">(Maaten and Hinton <a href="#ref-maaten2008visualizing" role="doc-biblioref">2008</a>)</span></p>
<div class="HeadingNoNumber">
<p>Independent component analysis (ICA)</p>
</div>
<p>ICA is used in signal processing to differentiate different sources of signal, i.e. suppress in an audio signal the background noise which can be very helpful when using a phone in an loud environment.</p>
<div class="rmdtip">
<p>Independent component analysis (ICA):</p>
<ul>
<li>Origin in signal processing</li>
<li>Separating mulivariate signal into additive subcomponents</li>
</ul>
</div>
<p>The ability to separate signal sources can be seen in the following example. Whereas PCA picks up the variance ICA assigns signals to the sources.</p>
<hr />
<p><img src="images/icaPca.png" class="external" style="width:80.0%" /></p>
<hr />
<p>The code is from at <a href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py" class="uri">https://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py</a></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="featureEngineering.html#cb2-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-2"><a href="featureEngineering.html#cb2-2"></a><span class="co">==========================</span></span>
<span id="cb2-3"><a href="featureEngineering.html#cb2-3"></a><span class="co">FastICA on 2D point clouds</span></span>
<span id="cb2-4"><a href="featureEngineering.html#cb2-4"></a><span class="co">==========================</span></span>
<span id="cb2-5"><a href="featureEngineering.html#cb2-5"></a></span>
<span id="cb2-6"><a href="featureEngineering.html#cb2-6"></a><span class="co">This example illustrates visually in the feature space a comparison by</span></span>
<span id="cb2-7"><a href="featureEngineering.html#cb2-7"></a><span class="co">results using two different component analysis techniques.</span></span>
<span id="cb2-8"><a href="featureEngineering.html#cb2-8"></a></span>
<span id="cb2-9"><a href="featureEngineering.html#cb2-9"></a><span class="co">:ref:`ICA` vs :ref:`PCA`.</span></span>
<span id="cb2-10"><a href="featureEngineering.html#cb2-10"></a></span>
<span id="cb2-11"><a href="featureEngineering.html#cb2-11"></a><span class="co">Representing ICA in the feature space gives the view of &#39;geometric ICA&#39;:</span></span>
<span id="cb2-12"><a href="featureEngineering.html#cb2-12"></a><span class="co">ICA is an algorithm that finds directions in the feature space</span></span>
<span id="cb2-13"><a href="featureEngineering.html#cb2-13"></a><span class="co">corresponding to projections with high non-Gaussianity. These directions</span></span>
<span id="cb2-14"><a href="featureEngineering.html#cb2-14"></a><span class="co">need not be orthogonal in the original feature space, but they are</span></span>
<span id="cb2-15"><a href="featureEngineering.html#cb2-15"></a><span class="co">orthogonal in the whitened feature space, in which all directions</span></span>
<span id="cb2-16"><a href="featureEngineering.html#cb2-16"></a><span class="co">correspond to the same variance.</span></span>
<span id="cb2-17"><a href="featureEngineering.html#cb2-17"></a></span>
<span id="cb2-18"><a href="featureEngineering.html#cb2-18"></a><span class="co">PCA, on the other hand, finds orthogonal directions in the raw feature</span></span>
<span id="cb2-19"><a href="featureEngineering.html#cb2-19"></a><span class="co">space that correspond to directions accounting for maximum variance.</span></span>
<span id="cb2-20"><a href="featureEngineering.html#cb2-20"></a></span>
<span id="cb2-21"><a href="featureEngineering.html#cb2-21"></a><span class="co">Here we simulate independent sources using a highly non-Gaussian</span></span>
<span id="cb2-22"><a href="featureEngineering.html#cb2-22"></a><span class="co">process, 2 student T with a low number of degrees of freedom (top left</span></span>
<span id="cb2-23"><a href="featureEngineering.html#cb2-23"></a><span class="co">figure). We mix them to create observations (top right figure).</span></span>
<span id="cb2-24"><a href="featureEngineering.html#cb2-24"></a><span class="co">In this raw observation space, directions identified by PCA are</span></span>
<span id="cb2-25"><a href="featureEngineering.html#cb2-25"></a><span class="co">represented by orange vectors. We represent the signal in the PCA space,</span></span>
<span id="cb2-26"><a href="featureEngineering.html#cb2-26"></a><span class="co">after whitening by the variance corresponding to the PCA vectors (lower</span></span>
<span id="cb2-27"><a href="featureEngineering.html#cb2-27"></a><span class="co">left). Running ICA corresponds to finding a rotation in this space to</span></span>
<span id="cb2-28"><a href="featureEngineering.html#cb2-28"></a><span class="co">identify the directions of largest non-Gaussianity (lower right).</span></span>
<span id="cb2-29"><a href="featureEngineering.html#cb2-29"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-30"><a href="featureEngineering.html#cb2-30"></a><span class="bu">print</span>(__doc__)</span>
<span id="cb2-31"><a href="featureEngineering.html#cb2-31"></a></span>
<span id="cb2-32"><a href="featureEngineering.html#cb2-32"></a><span class="co"># Authors: Alexandre Gramfort, Gael Varoquaux</span></span>
<span id="cb2-33"><a href="featureEngineering.html#cb2-33"></a><span class="co"># License: BSD 3 clause</span></span>
<span id="cb2-34"><a href="featureEngineering.html#cb2-34"></a></span>
<span id="cb2-35"><a href="featureEngineering.html#cb2-35"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-36"><a href="featureEngineering.html#cb2-36"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-37"><a href="featureEngineering.html#cb2-37"></a></span>
<span id="cb2-38"><a href="featureEngineering.html#cb2-38"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA, FastICA</span>
<span id="cb2-39"><a href="featureEngineering.html#cb2-39"></a></span>
<span id="cb2-40"><a href="featureEngineering.html#cb2-40"></a><span class="co"># #############################################################################</span></span>
<span id="cb2-41"><a href="featureEngineering.html#cb2-41"></a><span class="co"># Generate sample data</span></span>
<span id="cb2-42"><a href="featureEngineering.html#cb2-42"></a>rng <span class="op">=</span> np.random.RandomState(<span class="dv">42</span>)</span>
<span id="cb2-43"><a href="featureEngineering.html#cb2-43"></a>S <span class="op">=</span> rng.standard_t(<span class="fl">1.5</span>, size<span class="op">=</span>(<span class="dv">20000</span>, <span class="dv">2</span>))</span>
<span id="cb2-44"><a href="featureEngineering.html#cb2-44"></a>S[:, <span class="dv">0</span>] <span class="op">*=</span> <span class="fl">2.</span></span>
<span id="cb2-45"><a href="featureEngineering.html#cb2-45"></a></span>
<span id="cb2-46"><a href="featureEngineering.html#cb2-46"></a><span class="co"># Mix data</span></span>
<span id="cb2-47"><a href="featureEngineering.html#cb2-47"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">2</span>]])  <span class="co"># Mixing matrix</span></span>
<span id="cb2-48"><a href="featureEngineering.html#cb2-48"></a></span>
<span id="cb2-49"><a href="featureEngineering.html#cb2-49"></a>X <span class="op">=</span> np.dot(S, A.T)  <span class="co"># Generate observations</span></span>
<span id="cb2-50"><a href="featureEngineering.html#cb2-50"></a></span>
<span id="cb2-51"><a href="featureEngineering.html#cb2-51"></a>pca <span class="op">=</span> PCA()</span>
<span id="cb2-52"><a href="featureEngineering.html#cb2-52"></a>S_pca_ <span class="op">=</span> pca.fit(X).transform(X)</span>
<span id="cb2-53"><a href="featureEngineering.html#cb2-53"></a></span>
<span id="cb2-54"><a href="featureEngineering.html#cb2-54"></a>ica <span class="op">=</span> FastICA(random_state<span class="op">=</span>rng)</span>
<span id="cb2-55"><a href="featureEngineering.html#cb2-55"></a>S_ica_ <span class="op">=</span> ica.fit(X).transform(X)  <span class="co"># Estimate the sources</span></span>
<span id="cb2-56"><a href="featureEngineering.html#cb2-56"></a></span>
<span id="cb2-57"><a href="featureEngineering.html#cb2-57"></a>S_ica_ <span class="op">/=</span> S_ica_.std(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-58"><a href="featureEngineering.html#cb2-58"></a></span>
<span id="cb2-59"><a href="featureEngineering.html#cb2-59"></a></span>
<span id="cb2-60"><a href="featureEngineering.html#cb2-60"></a><span class="co"># #############################################################################</span></span>
<span id="cb2-61"><a href="featureEngineering.html#cb2-61"></a><span class="co"># Plot results</span></span>
<span id="cb2-62"><a href="featureEngineering.html#cb2-62"></a></span>
<span id="cb2-63"><a href="featureEngineering.html#cb2-63"></a><span class="kw">def</span> plot_samples(S, axis_list<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-64"><a href="featureEngineering.html#cb2-64"></a>    plt.scatter(S[:, <span class="dv">0</span>], S[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">2</span>, marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, zorder<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb2-65"><a href="featureEngineering.html#cb2-65"></a>                color<span class="op">=</span><span class="st">&#39;steelblue&#39;</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-66"><a href="featureEngineering.html#cb2-66"></a>    <span class="cf">if</span> axis_list <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-67"><a href="featureEngineering.html#cb2-67"></a>        colors <span class="op">=</span> [<span class="st">&#39;orange&#39;</span>, <span class="st">&#39;red&#39;</span>]</span>
<span id="cb2-68"><a href="featureEngineering.html#cb2-68"></a>        <span class="cf">for</span> color, axis <span class="kw">in</span> <span class="bu">zip</span>(colors, axis_list):</span>
<span id="cb2-69"><a href="featureEngineering.html#cb2-69"></a>            axis <span class="op">/=</span> axis.std()</span>
<span id="cb2-70"><a href="featureEngineering.html#cb2-70"></a>            x_axis, y_axis <span class="op">=</span> axis</span>
<span id="cb2-71"><a href="featureEngineering.html#cb2-71"></a>            <span class="co"># Trick to get legend to work</span></span>
<span id="cb2-72"><a href="featureEngineering.html#cb2-72"></a>            plt.plot(<span class="fl">0.1</span> <span class="op">*</span> x_axis, <span class="fl">0.1</span> <span class="op">*</span> y_axis, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>color)</span>
<span id="cb2-73"><a href="featureEngineering.html#cb2-73"></a>            plt.quiver(<span class="dv">0</span>, <span class="dv">0</span>, x_axis, y_axis, zorder<span class="op">=</span><span class="dv">11</span>, width<span class="op">=</span><span class="fl">0.01</span>, scale<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb2-74"><a href="featureEngineering.html#cb2-74"></a>                       color<span class="op">=</span>color)</span>
<span id="cb2-75"><a href="featureEngineering.html#cb2-75"></a></span>
<span id="cb2-76"><a href="featureEngineering.html#cb2-76"></a>    plt.hlines(<span class="dv">0</span>, <span class="dv">-3</span>, <span class="dv">3</span>)</span>
<span id="cb2-77"><a href="featureEngineering.html#cb2-77"></a>    plt.vlines(<span class="dv">0</span>, <span class="dv">-3</span>, <span class="dv">3</span>)</span>
<span id="cb2-78"><a href="featureEngineering.html#cb2-78"></a>    plt.xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb2-79"><a href="featureEngineering.html#cb2-79"></a>    plt.ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb2-80"><a href="featureEngineering.html#cb2-80"></a>    plt.xlabel(<span class="st">&#39;x&#39;</span>)</span>
<span id="cb2-81"><a href="featureEngineering.html#cb2-81"></a>    plt.ylabel(<span class="st">&#39;y&#39;</span>)</span>
<span id="cb2-82"><a href="featureEngineering.html#cb2-82"></a></span>
<span id="cb2-83"><a href="featureEngineering.html#cb2-83"></a>plt.figure()</span>
<span id="cb2-84"><a href="featureEngineering.html#cb2-84"></a>plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb2-85"><a href="featureEngineering.html#cb2-85"></a>plot_samples(S <span class="op">/</span> S.std())</span>
<span id="cb2-86"><a href="featureEngineering.html#cb2-86"></a>plt.title(<span class="st">&#39;True Independent Sources&#39;</span>)</span>
<span id="cb2-87"><a href="featureEngineering.html#cb2-87"></a></span>
<span id="cb2-88"><a href="featureEngineering.html#cb2-88"></a>axis_list <span class="op">=</span> [pca.components_.T, ica.mixing_]</span>
<span id="cb2-89"><a href="featureEngineering.html#cb2-89"></a>plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-90"><a href="featureEngineering.html#cb2-90"></a>plot_samples(X <span class="op">/</span> np.std(X), axis_list<span class="op">=</span>axis_list)</span>
<span id="cb2-91"><a href="featureEngineering.html#cb2-91"></a>legend <span class="op">=</span> plt.legend([<span class="st">&#39;PCA&#39;</span>, <span class="st">&#39;ICA&#39;</span>], loc<span class="op">=</span><span class="st">&#39;upper right&#39;</span>)</span>
<span id="cb2-92"><a href="featureEngineering.html#cb2-92"></a>legend.set_zorder(<span class="dv">100</span>)</span>
<span id="cb2-93"><a href="featureEngineering.html#cb2-93"></a></span>
<span id="cb2-94"><a href="featureEngineering.html#cb2-94"></a>plt.title(<span class="st">&#39;Observations&#39;</span>)</span>
<span id="cb2-95"><a href="featureEngineering.html#cb2-95"></a></span>
<span id="cb2-96"><a href="featureEngineering.html#cb2-96"></a>plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb2-97"><a href="featureEngineering.html#cb2-97"></a>plot_samples(S_pca_ <span class="op">/</span> np.std(S_pca_, axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb2-98"><a href="featureEngineering.html#cb2-98"></a>plt.title(<span class="st">&#39;PCA recovered signals&#39;</span>)</span>
<span id="cb2-99"><a href="featureEngineering.html#cb2-99"></a></span>
<span id="cb2-100"><a href="featureEngineering.html#cb2-100"></a>plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb2-101"><a href="featureEngineering.html#cb2-101"></a>plot_samples(S_ica_ <span class="op">/</span> np.std(S_ica_))</span>
<span id="cb2-102"><a href="featureEngineering.html#cb2-102"></a>plt.title(<span class="st">&#39;ICA recovered signals&#39;</span>)</span>
<span id="cb2-103"><a href="featureEngineering.html#cb2-103"></a></span>
<span id="cb2-104"><a href="featureEngineering.html#cb2-104"></a>plt.subplots_adjust(<span class="fl">0.09</span>, <span class="fl">0.04</span>, <span class="fl">0.94</span>, <span class="fl">0.94</span>, <span class="fl">0.26</span>, <span class="fl">0.36</span>)</span>
<span id="cb2-105"><a href="featureEngineering.html#cb2-105"></a>plt.show()</span>
<span id="cb2-106"><a href="featureEngineering.html#cb2-106"></a></span></code></pre></div>
<div class="HeadingNoNumber">
<p>Partial least squares</p>
</div>
<p>Since PCA does not take into consideration any of the response information. PLS is a supervised version of PCA which reduced dimensions are optimally related to the response <span class="citation">(Stone and Brooks <a href="#ref-stone1990continuum" role="doc-biblioref">1990</a>)</span>.</p>
<div class="rmdtip">
<p>Partial least square (PLS) facts:</p>
<ul>
<li>Supervised technique</li>
<li>Objective
<ul>
<li>find linear functions (latent factors)</li>
<li>with optimal covariance with response</li>
<li>each new dimension is uncorrelated</li>
</ul></li>
<li>Less PLS than PCA dimensions are needed for same performance</li>
</ul>
</div>
<hr />
<p>The following example is from <a href="https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html#sphx-glr-auto-examples-cross-decomposition-plot-compare-cross-decomposition-py" class="uri">https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html#sphx-glr-auto-examples-cross-decomposition-plot-compare-cross-decomposition-py</a> and shows the <strong>PLS components of two multivariate covarying two-dimensional datasets, X, and Y.</strong></p>
<p>The scatterplot shows:</p>
<ul>
<li>components 1 in dataset X and dataset Y are maximally correlated</li>
<li>components 2 in dataset X and dataset Y are maximally correlated</li>
<li>correlation across datasets for different components is weak</li>
</ul>
<p><img src="images/plsScatterplot.png" class="external" style="width:100.0%" /></p>
<hr />
<p>The code of the example is given below.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="featureEngineering.html#cb3-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-2"><a href="featureEngineering.html#cb3-2"></a><span class="co">===================================</span></span>
<span id="cb3-3"><a href="featureEngineering.html#cb3-3"></a><span class="co">Compare cross decomposition methods</span></span>
<span id="cb3-4"><a href="featureEngineering.html#cb3-4"></a><span class="co">===================================</span></span>
<span id="cb3-5"><a href="featureEngineering.html#cb3-5"></a></span>
<span id="cb3-6"><a href="featureEngineering.html#cb3-6"></a><span class="co">Simple usage of various cross decomposition algorithms:</span></span>
<span id="cb3-7"><a href="featureEngineering.html#cb3-7"></a><span class="co">- PLSCanonical</span></span>
<span id="cb3-8"><a href="featureEngineering.html#cb3-8"></a><span class="co">- PLSRegression, with multivariate response, a.k.a. PLS2</span></span>
<span id="cb3-9"><a href="featureEngineering.html#cb3-9"></a><span class="co">- PLSRegression, with univariate response, a.k.a. PLS1</span></span>
<span id="cb3-10"><a href="featureEngineering.html#cb3-10"></a><span class="co">- CCA</span></span>
<span id="cb3-11"><a href="featureEngineering.html#cb3-11"></a></span>
<span id="cb3-12"><a href="featureEngineering.html#cb3-12"></a><span class="co">Given 2 multivariate covarying two-dimensional datasets, X, and Y,</span></span>
<span id="cb3-13"><a href="featureEngineering.html#cb3-13"></a><span class="co">PLS extracts the &#39;directions of covariance&#39;, i.e. the components of each</span></span>
<span id="cb3-14"><a href="featureEngineering.html#cb3-14"></a><span class="co">datasets that explain the most shared variance between both datasets.</span></span>
<span id="cb3-15"><a href="featureEngineering.html#cb3-15"></a><span class="co">This is apparent on the **scatterplot matrix** display: components 1 in</span></span>
<span id="cb3-16"><a href="featureEngineering.html#cb3-16"></a><span class="co">dataset X and dataset Y are maximally correlated (points lie around the</span></span>
<span id="cb3-17"><a href="featureEngineering.html#cb3-17"></a><span class="co">first diagonal). This is also true for components 2 in both dataset,</span></span>
<span id="cb3-18"><a href="featureEngineering.html#cb3-18"></a><span class="co">however, the correlation across datasets for different components is</span></span>
<span id="cb3-19"><a href="featureEngineering.html#cb3-19"></a><span class="co">weak: the point cloud is very spherical.</span></span>
<span id="cb3-20"><a href="featureEngineering.html#cb3-20"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-21"><a href="featureEngineering.html#cb3-21"></a><span class="bu">print</span>(__doc__)</span>
<span id="cb3-22"><a href="featureEngineering.html#cb3-22"></a></span>
<span id="cb3-23"><a href="featureEngineering.html#cb3-23"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-24"><a href="featureEngineering.html#cb3-24"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-25"><a href="featureEngineering.html#cb3-25"></a><span class="im">from</span> sklearn.cross_decomposition <span class="im">import</span> PLSCanonical, PLSRegression, CCA</span>
<span id="cb3-26"><a href="featureEngineering.html#cb3-26"></a></span>
<span id="cb3-27"><a href="featureEngineering.html#cb3-27"></a><span class="co"># #############################################################################</span></span>
<span id="cb3-28"><a href="featureEngineering.html#cb3-28"></a><span class="co"># Dataset based latent variables model</span></span>
<span id="cb3-29"><a href="featureEngineering.html#cb3-29"></a></span>
<span id="cb3-30"><a href="featureEngineering.html#cb3-30"></a>n <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb3-31"><a href="featureEngineering.html#cb3-31"></a><span class="co"># 2 latents vars:</span></span>
<span id="cb3-32"><a href="featureEngineering.html#cb3-32"></a>l1 <span class="op">=</span> np.random.normal(size<span class="op">=</span>n)</span>
<span id="cb3-33"><a href="featureEngineering.html#cb3-33"></a>l2 <span class="op">=</span> np.random.normal(size<span class="op">=</span>n)</span>
<span id="cb3-34"><a href="featureEngineering.html#cb3-34"></a></span>
<span id="cb3-35"><a href="featureEngineering.html#cb3-35"></a>latents <span class="op">=</span> np.array([l1, l1, l2, l2]).T</span>
<span id="cb3-36"><a href="featureEngineering.html#cb3-36"></a>X <span class="op">=</span> latents <span class="op">+</span> np.random.normal(size<span class="op">=</span><span class="dv">4</span> <span class="op">*</span> n).reshape((n, <span class="dv">4</span>))</span>
<span id="cb3-37"><a href="featureEngineering.html#cb3-37"></a>Y <span class="op">=</span> latents <span class="op">+</span> np.random.normal(size<span class="op">=</span><span class="dv">4</span> <span class="op">*</span> n).reshape((n, <span class="dv">4</span>))</span>
<span id="cb3-38"><a href="featureEngineering.html#cb3-38"></a></span>
<span id="cb3-39"><a href="featureEngineering.html#cb3-39"></a>X_train <span class="op">=</span> X[:n <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb3-40"><a href="featureEngineering.html#cb3-40"></a>Y_train <span class="op">=</span> Y[:n <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb3-41"><a href="featureEngineering.html#cb3-41"></a>X_test <span class="op">=</span> X[n <span class="op">//</span> <span class="dv">2</span>:]</span>
<span id="cb3-42"><a href="featureEngineering.html#cb3-42"></a>Y_test <span class="op">=</span> Y[n <span class="op">//</span> <span class="dv">2</span>:]</span>
<span id="cb3-43"><a href="featureEngineering.html#cb3-43"></a></span>
<span id="cb3-44"><a href="featureEngineering.html#cb3-44"></a><span class="bu">print</span>(<span class="st">&quot;Corr(X)&quot;</span>)</span>
<span id="cb3-45"><a href="featureEngineering.html#cb3-45"></a><span class="bu">print</span>(np.<span class="bu">round</span>(np.corrcoef(X.T), <span class="dv">2</span>))</span>
<span id="cb3-46"><a href="featureEngineering.html#cb3-46"></a><span class="bu">print</span>(<span class="st">&quot;Corr(Y)&quot;</span>)</span>
<span id="cb3-47"><a href="featureEngineering.html#cb3-47"></a><span class="bu">print</span>(np.<span class="bu">round</span>(np.corrcoef(Y.T), <span class="dv">2</span>))</span>
<span id="cb3-48"><a href="featureEngineering.html#cb3-48"></a></span>
<span id="cb3-49"><a href="featureEngineering.html#cb3-49"></a><span class="co"># #############################################################################</span></span>
<span id="cb3-50"><a href="featureEngineering.html#cb3-50"></a><span class="co"># Canonical (symmetric) PLS</span></span>
<span id="cb3-51"><a href="featureEngineering.html#cb3-51"></a></span>
<span id="cb3-52"><a href="featureEngineering.html#cb3-52"></a><span class="co"># Transform data</span></span>
<span id="cb3-53"><a href="featureEngineering.html#cb3-53"></a><span class="co"># ~~~~~~~~~~~~~~</span></span>
<span id="cb3-54"><a href="featureEngineering.html#cb3-54"></a>plsca <span class="op">=</span> PLSCanonical(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-55"><a href="featureEngineering.html#cb3-55"></a>plsca.fit(X_train, Y_train)</span>
<span id="cb3-56"><a href="featureEngineering.html#cb3-56"></a>X_train_r, Y_train_r <span class="op">=</span> plsca.transform(X_train, Y_train)</span>
<span id="cb3-57"><a href="featureEngineering.html#cb3-57"></a>X_test_r, Y_test_r <span class="op">=</span> plsca.transform(X_test, Y_test)</span>
<span id="cb3-58"><a href="featureEngineering.html#cb3-58"></a></span>
<span id="cb3-59"><a href="featureEngineering.html#cb3-59"></a><span class="co"># Scatter plot of scores</span></span>
<span id="cb3-60"><a href="featureEngineering.html#cb3-60"></a><span class="co"># ~~~~~~~~~~~~~~~~~~~~~~</span></span>
<span id="cb3-61"><a href="featureEngineering.html#cb3-61"></a><span class="co"># 1) On diagonal plot X vs Y scores on each components</span></span>
<span id="cb3-62"><a href="featureEngineering.html#cb3-62"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb3-63"><a href="featureEngineering.html#cb3-63"></a>plt.subplot(<span class="dv">221</span>)</span>
<span id="cb3-64"><a href="featureEngineering.html#cb3-64"></a>plt.scatter(X_train_r[:, <span class="dv">0</span>], Y_train_r[:, <span class="dv">0</span>], label<span class="op">=</span><span class="st">&quot;train&quot;</span>,</span>
<span id="cb3-65"><a href="featureEngineering.html#cb3-65"></a>            marker<span class="op">=</span><span class="st">&quot;o&quot;</span>, c<span class="op">=</span><span class="st">&quot;b&quot;</span>, s<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb3-66"><a href="featureEngineering.html#cb3-66"></a>plt.scatter(X_test_r[:, <span class="dv">0</span>], Y_test_r[:, <span class="dv">0</span>], label<span class="op">=</span><span class="st">&quot;test&quot;</span>,</span>
<span id="cb3-67"><a href="featureEngineering.html#cb3-67"></a>            marker<span class="op">=</span><span class="st">&quot;o&quot;</span>, c<span class="op">=</span><span class="st">&quot;r&quot;</span>, s<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb3-68"><a href="featureEngineering.html#cb3-68"></a>plt.xlabel(<span class="st">&quot;x scores&quot;</span>)</span>
<span id="cb3-69"><a href="featureEngineering.html#cb3-69"></a>plt.ylabel(<span class="st">&quot;y scores&quot;</span>)</span>
<span id="cb3-70"><a href="featureEngineering.html#cb3-70"></a>plt.title(<span class="st">&#39;Comp. 1: X vs Y (test corr = </span><span class="sc">%.2f</span><span class="st">)&#39;</span> <span class="op">%</span></span>
<span id="cb3-71"><a href="featureEngineering.html#cb3-71"></a>          np.corrcoef(X_test_r[:, <span class="dv">0</span>], Y_test_r[:, <span class="dv">0</span>])[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb3-72"><a href="featureEngineering.html#cb3-72"></a>plt.xticks(())</span>
<span id="cb3-73"><a href="featureEngineering.html#cb3-73"></a>plt.yticks(())</span>
<span id="cb3-74"><a href="featureEngineering.html#cb3-74"></a>plt.legend(loc<span class="op">=</span><span class="st">&quot;best&quot;</span>)</span>
<span id="cb3-75"><a href="featureEngineering.html#cb3-75"></a></span>
<span id="cb3-76"><a href="featureEngineering.html#cb3-76"></a>plt.subplot(<span class="dv">224</span>)</span>
<span id="cb3-77"><a href="featureEngineering.html#cb3-77"></a>plt.scatter(X_train_r[:, <span class="dv">1</span>], Y_train_r[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">&quot;train&quot;</span>,</span>
<span id="cb3-78"><a href="featureEngineering.html#cb3-78"></a>            marker<span class="op">=</span><span class="st">&quot;o&quot;</span>, c<span class="op">=</span><span class="st">&quot;b&quot;</span>, s<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb3-79"><a href="featureEngineering.html#cb3-79"></a>plt.scatter(X_test_r[:, <span class="dv">1</span>], Y_test_r[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">&quot;test&quot;</span>,</span>
<span id="cb3-80"><a href="featureEngineering.html#cb3-80"></a>            marker<span class="op">=</span><span class="st">&quot;o&quot;</span>, c<span class="op">=</span><span class="st">&quot;r&quot;</span>, s<span class="op">=</span><span class="dv">25</span>)</span>
<span id="cb3-81"><a href="featureEngineering.html#cb3-81"></a>plt.xlabel(<span class="st">&quot;x scores&quot;</span>)</span>
<span id="cb3-82"><a href="featureEngineering.html#cb3-82"></a>plt.ylabel(<span class="st">&quot;y scores&quot;</span>)</span>
<span id="cb3-83"><a href="featureEngineering.html#cb3-83"></a>plt.title(<span class="st">&#39;Comp. 2: X vs Y (test corr = </span><span class="sc">%.2f</span><span class="st">)&#39;</span> <span class="op">%</span></span>
<span id="cb3-84"><a href="featureEngineering.html#cb3-84"></a>          np.corrcoef(X_test_r[:, <span class="dv">1</span>], Y_test_r[:, <span class="dv">1</span>])[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb3-85"><a href="featureEngineering.html#cb3-85"></a>plt.xticks(())</span>
<span id="cb3-86"><a href="featureEngineering.html#cb3-86"></a>plt.yticks(())</span>
<span id="cb3-87"><a href="featureEngineering.html#cb3-87"></a>plt.legend(loc<span class="op">=</span><span class="st">&quot;best&quot;</span>)</span>
<span id="cb3-88"><a href="featureEngineering.html#cb3-88"></a></span>
<span id="cb3-89"><a href="featureEngineering.html#cb3-89"></a><span class="co"># 2) Off diagonal plot components 1 vs 2 for X and Y</span></span>
<span id="cb3-90"><a href="featureEngineering.html#cb3-90"></a>plt.subplot(<span class="dv">222</span>)</span>
<span id="cb3-91"><a href="featureEngineering.html#cb3-91"></a>plt.scatter(X_train_r[:, <span class="dv">0</span>], X_train_r[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">&quot;train&quot;</span>,</span>
<span id="cb3-92"><a href="featureEngineering.html#cb3-92"></a>            marker<span class="op">=</span><span class="st">&quot;*&quot;</span>, c<span class="op">=</span><span class="st">&quot;b&quot;</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb3-93"><a href="featureEngineering.html#cb3-93"></a>plt.scatter(X_test_r[:, <span class="dv">0</span>], X_test_r[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">&quot;test&quot;</span>,</span>
<span id="cb3-94"><a href="featureEngineering.html#cb3-94"></a>            marker<span class="op">=</span><span class="st">&quot;*&quot;</span>, c<span class="op">=</span><span class="st">&quot;r&quot;</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb3-95"><a href="featureEngineering.html#cb3-95"></a>plt.xlabel(<span class="st">&quot;X comp. 1&quot;</span>)</span>
<span id="cb3-96"><a href="featureEngineering.html#cb3-96"></a>plt.ylabel(<span class="st">&quot;X comp. 2&quot;</span>)</span>
<span id="cb3-97"><a href="featureEngineering.html#cb3-97"></a>plt.title(<span class="st">&#39;X comp. 1 vs X comp. 2 (test corr = </span><span class="sc">%.2f</span><span class="st">)&#39;</span></span>
<span id="cb3-98"><a href="featureEngineering.html#cb3-98"></a>          <span class="op">%</span> np.corrcoef(X_test_r[:, <span class="dv">0</span>], X_test_r[:, <span class="dv">1</span>])[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb3-99"><a href="featureEngineering.html#cb3-99"></a>plt.legend(loc<span class="op">=</span><span class="st">&quot;best&quot;</span>)</span>
<span id="cb3-100"><a href="featureEngineering.html#cb3-100"></a>plt.xticks(())</span>
<span id="cb3-101"><a href="featureEngineering.html#cb3-101"></a>plt.yticks(())</span>
<span id="cb3-102"><a href="featureEngineering.html#cb3-102"></a></span>
<span id="cb3-103"><a href="featureEngineering.html#cb3-103"></a>plt.subplot(<span class="dv">223</span>)</span>
<span id="cb3-104"><a href="featureEngineering.html#cb3-104"></a>plt.scatter(Y_train_r[:, <span class="dv">0</span>], Y_train_r[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">&quot;train&quot;</span>,</span>
<span id="cb3-105"><a href="featureEngineering.html#cb3-105"></a>            marker<span class="op">=</span><span class="st">&quot;*&quot;</span>, c<span class="op">=</span><span class="st">&quot;b&quot;</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb3-106"><a href="featureEngineering.html#cb3-106"></a>plt.scatter(Y_test_r[:, <span class="dv">0</span>], Y_test_r[:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">&quot;test&quot;</span>,</span>
<span id="cb3-107"><a href="featureEngineering.html#cb3-107"></a>            marker<span class="op">=</span><span class="st">&quot;*&quot;</span>, c<span class="op">=</span><span class="st">&quot;r&quot;</span>, s<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb3-108"><a href="featureEngineering.html#cb3-108"></a>plt.xlabel(<span class="st">&quot;Y comp. 1&quot;</span>)</span>
<span id="cb3-109"><a href="featureEngineering.html#cb3-109"></a>plt.ylabel(<span class="st">&quot;Y comp. 2&quot;</span>)</span>
<span id="cb3-110"><a href="featureEngineering.html#cb3-110"></a>plt.title(<span class="st">&#39;Y comp. 1 vs Y comp. 2 , (test corr = </span><span class="sc">%.2f</span><span class="st">)&#39;</span></span>
<span id="cb3-111"><a href="featureEngineering.html#cb3-111"></a>          <span class="op">%</span> np.corrcoef(Y_test_r[:, <span class="dv">0</span>], Y_test_r[:, <span class="dv">1</span>])[<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb3-112"><a href="featureEngineering.html#cb3-112"></a>plt.legend(loc<span class="op">=</span><span class="st">&quot;best&quot;</span>)</span>
<span id="cb3-113"><a href="featureEngineering.html#cb3-113"></a>plt.xticks(())</span>
<span id="cb3-114"><a href="featureEngineering.html#cb3-114"></a>plt.yticks(())</span>
<span id="cb3-115"><a href="featureEngineering.html#cb3-115"></a>plt.show()</span>
<span id="cb3-116"><a href="featureEngineering.html#cb3-116"></a></span>
<span id="cb3-117"><a href="featureEngineering.html#cb3-117"></a><span class="co"># #############################################################################</span></span>
<span id="cb3-118"><a href="featureEngineering.html#cb3-118"></a><span class="co"># PLS regression, with multivariate response, a.k.a. PLS2</span></span>
<span id="cb3-119"><a href="featureEngineering.html#cb3-119"></a></span>
<span id="cb3-120"><a href="featureEngineering.html#cb3-120"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-121"><a href="featureEngineering.html#cb3-121"></a>q <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb3-122"><a href="featureEngineering.html#cb3-122"></a>p <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-123"><a href="featureEngineering.html#cb3-123"></a>X <span class="op">=</span> np.random.normal(size<span class="op">=</span>n <span class="op">*</span> p).reshape((n, p))</span>
<span id="cb3-124"><a href="featureEngineering.html#cb3-124"></a>B <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>] <span class="op">+</span> [<span class="dv">0</span>] <span class="op">*</span> (p <span class="op">-</span> <span class="dv">2</span>)] <span class="op">*</span> q).T</span>
<span id="cb3-125"><a href="featureEngineering.html#cb3-125"></a><span class="co"># each Yj = 1*X1 + 2*X2 + noize</span></span>
<span id="cb3-126"><a href="featureEngineering.html#cb3-126"></a>Y <span class="op">=</span> np.dot(X, B) <span class="op">+</span> np.random.normal(size<span class="op">=</span>n <span class="op">*</span> q).reshape((n, q)) <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb3-127"><a href="featureEngineering.html#cb3-127"></a></span>
<span id="cb3-128"><a href="featureEngineering.html#cb3-128"></a>pls2 <span class="op">=</span> PLSRegression(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-129"><a href="featureEngineering.html#cb3-129"></a>pls2.fit(X, Y)</span>
<span id="cb3-130"><a href="featureEngineering.html#cb3-130"></a><span class="bu">print</span>(<span class="st">&quot;True B (such that: Y = XB + Err)&quot;</span>)</span>
<span id="cb3-131"><a href="featureEngineering.html#cb3-131"></a><span class="bu">print</span>(B)</span>
<span id="cb3-132"><a href="featureEngineering.html#cb3-132"></a><span class="co"># compare pls2.coef_ with B</span></span>
<span id="cb3-133"><a href="featureEngineering.html#cb3-133"></a><span class="bu">print</span>(<span class="st">&quot;Estimated B&quot;</span>)</span>
<span id="cb3-134"><a href="featureEngineering.html#cb3-134"></a><span class="bu">print</span>(np.<span class="bu">round</span>(pls2.coef_, <span class="dv">1</span>))</span>
<span id="cb3-135"><a href="featureEngineering.html#cb3-135"></a>pls2.predict(X)</span>
<span id="cb3-136"><a href="featureEngineering.html#cb3-136"></a></span>
<span id="cb3-137"><a href="featureEngineering.html#cb3-137"></a><span class="co"># PLS regression, with univariate response, a.k.a. PLS1</span></span>
<span id="cb3-138"><a href="featureEngineering.html#cb3-138"></a></span>
<span id="cb3-139"><a href="featureEngineering.html#cb3-139"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-140"><a href="featureEngineering.html#cb3-140"></a>p <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-141"><a href="featureEngineering.html#cb3-141"></a>X <span class="op">=</span> np.random.normal(size<span class="op">=</span>n <span class="op">*</span> p).reshape((n, p))</span>
<span id="cb3-142"><a href="featureEngineering.html#cb3-142"></a>y <span class="op">=</span> X[:, <span class="dv">0</span>] <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> X[:, <span class="dv">1</span>] <span class="op">+</span> np.random.normal(size<span class="op">=</span>n <span class="op">*</span> <span class="dv">1</span>) <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb3-143"><a href="featureEngineering.html#cb3-143"></a>pls1 <span class="op">=</span> PLSRegression(n_components<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-144"><a href="featureEngineering.html#cb3-144"></a>pls1.fit(X, y)</span>
<span id="cb3-145"><a href="featureEngineering.html#cb3-145"></a><span class="co"># note that the number of components exceeds 1 (the dimension of y)</span></span>
<span id="cb3-146"><a href="featureEngineering.html#cb3-146"></a><span class="bu">print</span>(<span class="st">&quot;Estimated betas&quot;</span>)</span>
<span id="cb3-147"><a href="featureEngineering.html#cb3-147"></a><span class="bu">print</span>(np.<span class="bu">round</span>(pls1.coef_, <span class="dv">1</span>))</span>
<span id="cb3-148"><a href="featureEngineering.html#cb3-148"></a></span>
<span id="cb3-149"><a href="featureEngineering.html#cb3-149"></a><span class="co"># #############################################################################</span></span>
<span id="cb3-150"><a href="featureEngineering.html#cb3-150"></a><span class="co"># CCA (PLS mode B with symmetric deflation)</span></span>
<span id="cb3-151"><a href="featureEngineering.html#cb3-151"></a></span>
<span id="cb3-152"><a href="featureEngineering.html#cb3-152"></a>cca <span class="op">=</span> CCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-153"><a href="featureEngineering.html#cb3-153"></a>cca.fit(X_train, Y_train)</span>
<span id="cb3-154"><a href="featureEngineering.html#cb3-154"></a>X_train_r, Y_train_r <span class="op">=</span> cca.transform(X_train, Y_train)</span>
<span id="cb3-155"><a href="featureEngineering.html#cb3-155"></a>X_test_r, Y_test_r <span class="op">=</span> cca.transform(X_test, Y_test)</span></code></pre></div>
<hr />
<div class="HeadingNoNumber">
<p>Autoencoders</p>
</div>
<p>Autoencoders learn a latent representation of data by feeding them through a neural network with a bottleneck, i.e. a layer with less width than the input layer. Training an autoencoder is unsupervised since the training task is to create an output as similar to the input as possible.</p>
<p>Further explanation is given in an example implemented in Keras at <a href="https://towardsdatascience.com/autoencoders-made-simple-6f59e2ab37ef" class="uri">https://towardsdatascience.com/autoencoders-made-simple-6f59e2ab37ef</a></p>
<div class="rmdtip">
<p>Autoencoders:</p>
<ul>
<li>Type of neural network</li>
<li>Feeds information through bottleneck</li>
<li>Input and output shall be as similar as possible</li>
<li>Representation at bottleneck is dimensionality reduced</li>
</ul>
</div>
<div class="figure">
<img src="images/Autoencoder_schema.png" class="external" style="width:60.0%" alt="" />
<p class="caption">Figure from <a href="//commons.wikimedia.org/w/index.php?title=User:Michela_Massi&amp;action=edit&amp;redlink=1" class="new" title="User:Michela Massi (page does not exist)">Michela Massi</a> - <span class="int-own-work" lang="en">Own work</span>, <a href="https://creativecommons.org/licenses/by-sa/4.0" title="Creative Commons Attribution-Share Alike 4.0">CC BY-SA 4.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=80177333">Link</a> <span class="citation">(Kuhn and Johnson <a href="#ref-FeatureEngineeringKuhnWebsite" role="doc-biblioref">2018</a>)</span></p>
</div>
</div>
</div>
<div id="featureImportance" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Feature importance</h3>
<p>After creating features the next step is to find out which features are helpful for the model. First, two methods are compared</p>
<div class="rmdtip">
<p>
Feature importance analysis methods:<img src="images/permutation.svg" alt="Smiley face" align="right" style="width:20%;">
</p>
<ul>
<li>Permutation Importance
<ul>
<li>permute one feature</li>
<li>calculate the change in prediction performance</li>
<li>the bigger the drop in prediction performance <span class="math inline">\(\implies\)</span> more important feature</li>
</ul></li>
<li>Random Forest Feature Importance
<ul>
<li>during training compute decrease of impurity</li>
<li>average for decrease for all features</li>
<li>ranking according to decrease <span class="math inline">\(\implies\)</span> ranking of feature importance</li>
</ul></li>
</ul>
</div>
<p>An example shows strength and weakness of the algorithms. The example <em>“Permutation Importance vs Random Forest Feature Importance”</em> can be found at the Scikit-Learn <span class="citation">(Pedregosa et al. <a href="#ref-scikit-learn" role="doc-biblioref">2011</a>)</span> site <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html" class="uri">https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html</a></p>
<p>Comparing test and training accuracy give an indication that the <strong>model might have overfitted</strong></p>
<div class="rmdtip">
<p>Model might be overfitted:</p>
<p>RF train accuracy: 1.000<br />
RF test accuracy: 0.817</p>
</div>
<hr />
<p><img src="images/rfImportanceTitanic.png" class="external" style="width:80.0%" /></p>
<hr />
<p>The most important feature is <strong>random_num</strong> which is totally random numeric feature which the model has used to memorize the training data set but is useless to predict anything. The wrong importance measure results from two causes</p>
<div class="rmdtip">
<p>Causes of wrong feature importance:</p>
<ul>
<li>impurity-based importances are biased towards high cardinality features</li>
<li>impurity-based importances
<ul>
<li>computed on training set statistics</li>
<li>do not reflect the ability of feature to be useful to make predictions that generalize to the test set</li>
</ul></li>
</ul>
</div>
<div class="HeadingNoNumber">
<p>Permutation importance</p>
</div>
<p>Computing the permutation importance no a held out test data shows that the low cardinality categorical feature *<strong>sex</strong> has highest importance.</p>
<hr />
<p><img src="images/PermutationImportanceTitanicTest.png" class="external" style="width:80.0%" /></p>
<hr />
<div class="rmdtip">
<p>
Permutation importance:<img src="images/Gender.svg" alt="Smiley face" align="right" style="width:15%;">
</p>
<ul>
<li>Most important feature: sex</li>
<li>Random features low importance</li>
</ul>
</div>
<p>Computing the permutation importance on the training data shows that the random features get significantly higher importance ranking than when computed on the test set.</p>
<hr />
<p><img src="images/PermutationImportanceTitanicTrain.png" class="external" style="width:80.0%" /></p>
<hr />
<div class="HeadingNoNumber">
<p>Permutation Importance with Multicollinear or Correlated Features</p>
</div>
<p>Another example investigates the problem of multicollinear or correlated features. The example <em>“Permutation Importance with Multicollinear or Correlated Features”</em> can be found at the Scikit-Learn <span class="citation">(Pedregosa et al. <a href="#ref-scikit-learn" role="doc-biblioref">2011</a>)</span> site <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html" class="uri">https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html</a></p>
<div class="rmdtip">
<p>Facts of example:</p>
<ul>
<li>Wisconsin breast cancer datase</li>
<li>RandomForestClassifier 97% accuracy on a test data set</li>
<li>Contains multicollinear features</li>
</ul>
</div>
<p>The permutation importance plot shows that permuting a feature drops the accuracy by at most 0.012, which suggests taht none of the features are important.</p>
<hr />
<p><img src="images/rfImportance.png" style="width:100.0%" /></p>
<hr />
<div class="rmdtip">
<p>Permutation importance:</p>
<ul>
<li>highest drop = 0.012 <span class="math inline">\(\implies\)</span> no feature is important</li>
<li>when features collinear
<ul>
<li>permuting one feature will have little effect</li>
</ul></li>
<li>how to handle collinear features
<ul>
<li>performing hierarchical clustering on the Spearman rank-order correlations<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></li>
<li>pick threshold
<ul>
<li>in graph below = 2.0</li>
</ul></li>
<li>keep single feature from each cluster
<ul>
<li>in graph below 8 cluster <span class="math inline">\(\implies\)</span> 8 features</li>
</ul></li>
</ul></li>
</ul>
</div>
<hr />
<p>In the image below a dendrogram of the hierarchical cluster on the left hand side is derived of the correlation matrix on the right hand side.</p>
<hr />
<p><img src="images/clusterSpearman.png" style="width:100.0%" /></p>
<hr />
<p>If threshold 1.0 is chosen than accuracy stays at 0.97 with 14 features instead of 30</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-FeatureEngineeringKuhnWebsite">
<p>Kuhn, Max, and Kjell Johnson. 2018. “Feature Engineering and Selection: A Practical Approach for Predictive Models.” <a href="http://www.feat.engineering/index.html">http://www.feat.engineering/index.html</a>.</p>
</div>
<div id="ref-maaten2008visualizing">
<p>Maaten, L. J. P. van der, and G. E. Hinton. 2008. “Visualizing High-Dimensional Data Using T-Sne.”</p>
</div>
<div id="ref-scikit-learn">
<p>Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” <em>Journal of Machine Learning Research</em> 12: 2825–30.</p>
</div>
<div id="ref-stone1990continuum">
<p>Stone, Mervyn, and Rodney J Brooks. 1990. “Continuum Regression: Cross-Validated Sequentially Constructed Prediction Embracing Ordinary Least Squares, Partial Least Squares and Principal Components Regression.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 52 (2): 237–58.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p><a href="https://statistics.laerd.com/spss-tutorials/spearmans-rank-order-correlation-using-spss-statistics.php" class="uri">https://statistics.laerd.com/spss-tutorials/spearmans-rank-order-correlation-using-spss-statistics.php</a>: The Spearman rank-order correlation coefficient (Spearman’s correlation, for short) is a nonparametric measure of the strength and direction of association that exists between two variables measured on at least an ordinal scale.<a href="featureEngineering.html#fnref19" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="eda.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="MlProjectProcessModelFit.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MlOrienttionBookDown.pdf", "MlOrienttionBookDown.epub"],
"toc": {
"collapse": "subsection",
"toc_depth": 5
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
