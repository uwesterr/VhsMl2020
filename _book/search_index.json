[
["index.html", "Machine learning orientation Chapter 1 Introduction", " Machine learning orientation Uwe Sterr 2020-02-14 Chapter 1 Introduction This document gives on overview of Machine learing: Shall we? in chapter 2 Analysis of the 2019 Kaggle member survey in chapter 22 Machine learning fundamentals in chapter 4 Overview of online courses Overview online resources in chapter ??MlResources Real world example on signal detection in chapter 20 How to understand ML models: Explainable machine learing in chapter 9 Further examples from Kaggle in chapter 11 ML examples in the area vialytics https://vialytics.de enbw https://www.enbw.com/infrastruktur/sicherheitsinfrastruktur/geschaeftskunden/produkte/safeplaces How to get quickly started with cloud based machine learning platforms in chapter 21 Your browser does not support the video tag. "],
["want-to-meet-ml-people-from-academia-and-industrie.html", "1.1 Want to meet ML people from academia and industrie", " 1.1 Want to meet ML people from academia and industrie Machine Learning User Group Stuttgart MLUGS Technically oriented 419 members https://www.meetup.com/Machine-Learning-UserGroup-Stuttgart/ Build selfdrving RoboCar hands on, build a real world system 153 members https://www.meetup.com/Esslingen-Makerspace/ Stuttgart AI More concepts and industrie presenting themself 979 members https://www.meetup.com/StuttgartAI/ "],
["whatML.html", "Chapter 2 What is machine learning?", " Chapter 2 What is machine learning? Machine learning is a sub domain of artificial intelligence and has several definitions: “Field of study that gives computers the ability to learn without being explicitly programmed” — Arthur Samuel:1 \"Learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E — Tom Mitchell2 American pioneer in the field of computer gaming and artificial intelligence https://en.wikipedia.org/wiki/Arthur_Samuel↩︎ professor of Computer Science and Machine Learning at Carnegie Mellon http://www.cs.cmu.edu/~tom/↩︎ "],
["what-is-intelligence.html", "2.1 What is intelligence?", " 2.1 What is intelligence? To discuss the question of what is artificial intelligence, the first step is to define what intelligence is. A group of 52 psychology researchers published in (Gottfredson 1997) the following definition: A very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. It is not merely book learning, a narrow academic skill, or test-taking smarts. Rather, it reflects a broader and deeper capability for comprehending our surroundings—“catching on,” “making sense” of things, or “figuring out” what to do. Alfred Binet, a french psychologist who invented first practical IQ test defined in 1905 (Binet and Simon 1916): Judgment, otherwise called “good sense”, “practical sense”, “initiative”, the faculty of adapting one’s self to circumstances And Albert Einstein said The measure of intelligence is the ability to change. Tegmark’s summarizes the situation in (Tegmark 2017) There’s no agreement on what intelligence is even among intelligent intelligence researchers! So there’s clearly no undisputed “correct” definition of intelligence. 2.1.1 Definition of artificial intelligence sub domains Even though there is no undisputed definition of intelligence there is a undisputed definition of how machine learning is related to artificial intelligence Agreement: Machine learning is a sub domain of artificial intelligence (AI) The history of those fields goes back to the 1950’s References "],
["is-ai-smarter-than-humans.html", "2.2 Is AI smarter than humans?", " 2.2 Is AI smarter than humans? Which of the following questions can a computer answer better? 17*353 Wie viele Tiere von jeder Art nahm Moses mit auf die Arche?4 Wie heißen die drei letzten Bundespräsidenten?5 2.2.1 Thinking, fast and slow (Kahneman 2011) In Daniel Kahneman’s Thinking, fast and slow (Kahneman 2011) there are plenty of surprising social psychology experiments, on page 166 the following question is posed: A cab was involved in a hit-and-run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data: - 85% of the cabs in the city are Green and 15% are Blue. - A witness identified the cab as Blue. The court tested the reliability of the witness under the circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time. What is the probability that the cab involved in the accident was Blue rather than Green?6 Please cast your vote at https://pingo.coactum.de/1576787 or access Pingo webpage scanning QR code below: References "],
["comparisons-between-ai-and-humans.html", "2.3 Comparisons between AI and humans", " 2.3 Comparisons between AI and humans 2.3.1 Breast cancer detection In a Google Health project8 the following results were achieved: Absolute reduction of 5.7% and 1.2% (USA and UK) in false positives Absolute reduction 9.4% and 2.7% (USA and UK)in false negatives. In an independent study of six radiologists, the AI system outperformed all of the human readers. More on the study at https://www.nature.com/articles/s41586-019-1799-6 2.3.2 Working together: Lung cancer detection With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States A study published in Nature medicine9 a team of members of Google AI and several hospitals reported When prior computed tomography imaging was not available Model outperformed all six radiologists Absolute reductions of 11% in false positives Absolute reductions 5% in false negatives 2.3.3 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) The ImageNet Large Scale Visual Recognition Challenge10 (ILSVRC) (Russakovsky et al. 2015) evaluates algorithms for object recognition and image classification on a large scale. Facts of ImageNet:11 14 million images 20,000 image categories 1000 image categories used for ILSVRC The development of the results is shown in the graph below. The number of layers is a indication of model complexity In 2017 the problem set to status “solved” 29 of 38 competing teams had an accuracy of more than 95% ImageNet stopped competition 2.3.4 AlphaGo Zero Go is a strategy game invented 2500 years ago and has an estimated number of possible board configuration of 10¹⁷⁴ compared to chess which has is 10¹²º. A detailed description is given by DeepMind’s blog post “AlphaGo Zero: Starting from scratch”12 AlphaGo Zero is a version of DeepMind’s13 Go software AlphaGo No human intervention No usage of historical data After 3 days of training as good as AlphaGo which beat world champion in 4 out of 5 After 40 days of training becomes best Go player in the world AlphaZero learned three games, The capability progress of Alpha Zero during training is shown below Figure from https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go NOTE: EACH TRAINING STEP REPRESENTS 4,096 BOARD POSITIONS. At the end of the training Alpha Zero achieved the following performance: Figure from https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go Implications are wider than just playing a game, as Garry Kasparov, a former world chess champion puts it: The implications go far beyond my beloved chessboard… Not only do these self-taught expert machines perform incredibly well, but we can actually learn from the new knowledge they produce.\" References "],
["ml-models-with-bias.html", "2.4 ML models with bias", " 2.4 ML models with bias Models might end up biased, why is that? [source: https://www.youtube.com/watch?time_continue=1&amp;v=tlOIHko8ySg&amp;feature=emb_logo] With a unsuitable reward function an undesired result can occur Framing the problem Goal is business reason, not fairness or avoidance of discrimination Goal might lead to unwanted side effects https://openai.com/blog/faulty-reward-functions/ Collecting data Unrepresentative of reality Collecting images of zebras only when sun shines =&gt; model might look for shadow for classifying a zebra Reflects existing prejudices Historical data might lead recruiting tools to dismiss female candidates Preparing the data Selecting attributes to be considered might lead to bias Attribute gender might lead to bias 2.4.0.1 How to avoid bias Avoiding bias is harder than you might think Unknown unknowns Gender might be deducted by recruiting tool from use of language Imperfect processes Test data has same bias as training data Bias not easy to discover 2.4.0.2 Human bias Machine learning model can be biased for several reasons as shown above, how about humans? Study in Germany Judges read description of shoplifter Rolled a pair of loaded dice Dice = 3 =&gt; Average 5 months prison Dice = 9 =&gt; Average 8 months prison "],
["attacks-on-ml-models.html", "2.5 Attacks on ML models", " 2.5 Attacks on ML models Especially image classification models have shown to be susceptible to attacks which leads to wrong classifications. This could lead to Traffic sign misclassification Avoiding face detection How a attack can be performed is described by Goodfellow et al. in (Goodfellow, Shlens, and Szegedy 2014) 2.5.1 Adding noise to image leads to misclassification Figure from Image Credit: Goodfellow et al. (Goodfellow, Shlens, and Szegedy 2014)) 2.5.2 But what about attacks on human perception? Which statement is correct? Top line longer Bottom line longer Both are same length Is this a picture of a real person? Look at the picture below, is it a real person or an animation? Figure from https://commons.wikimedia.org/wiki/File:Woman_1.jpg (Image Credit: Owlsmcgee [Public domain] ) The image is create using a generative adversarial network (GAN), see below for the principle, for detailed description see https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f References "],
["outlook.html", "Chapter 3 Outlook", " Chapter 3 Outlook What will the future bring for society? The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom. Isaac Asimov "],
["development-of-life.html", "3.1 Development of life", " 3.1 Development of life Tegmark in “Life 3.0: Being human in the age of artificial intelligence” (Tegmark 2017) p. 23. classifies life into three stages and shows the two existing stages of life and the third stage which might be ahead. The three stages of life have overlapping skills, but only life 3.0 has all skills and is able to design its hardware and therefore might be able to have unlimited skills 3.1.1 When will superhuman AI come, and will it be good? Several opinions about when and if superhuman AI will appear and if it will be a good thing or not exists. Those opinions can be grouped as shown in the following graph. Luddite =&gt; A person opposed to new technology or ways of working Please cast your vote at https://pingo.coactum.de/15767814 3.1.2 AI aftermath scenario To be prepared we might want to ask yourselves: Do you want there to be superintelligence? Do you want humans to still exist, be replaced, cyborgized and/or uploaded/simulated? Do you want humans or machines in control? Do you want AIs to be conscious or not? Do you want to maximize positive experiences, minimize suffering or leave this to sort itself out? Do you want life spreading into the cosmos? Do you want a civilization striving toward a greater purpose that you sympathize with, or are you OK with future life forms that appear content? Depending on your answers this might lead to one of the following scenario A verbal description of the scenarios is given below, type the name of the scenario into the left field, if you want more scenarios to be shown increase the “Show entries” entry References "],
["data-religion-dataism.html", "3.2 Data religion: Dataism", " 3.2 Data religion: Dataism A data based religion called Dataism is a concept described by Harari in Homo Deus: A brief history of tomorrow (Harari 2016) and says: Universe consists of data flow Value of entity determined by contribution to data processing Collapses barrier between animals and machines15 electronic algorithms eventually outperform biochemical algorithms In data we trust Humans supposed to distill data =&gt; information information =&gt; knowledge knowledge =&gt; wisdom Dataists believe humans can not cope with immense flow of data put there trust in Big Data and computer algorithms Dataism: only wild fantasy? Dataism entrenched in computer science biology giraffes, tomatoes and human beings are just different methods for processing data that is current scientific dogma Economists interpret economy as data processing system Gathering data about desires and abilities Turning data into decisions Capitalism =&gt; distributed processing Communism =&gt; centralized processing Capitalists against high taxes capital accumulates at state more decisions by single processor, namely government References "],
["career-oxford-seeks-ai-ethics-professor.html", "3.3 Career: Oxford seeks AI ethics professor", " 3.3 Career: Oxford seeks AI ethics professor Associate Professorship or Professorship in Philosophy Apply for University of Oxford - Faculty of Philosophy (Ethics in AI) "],
["MachineLearningFundamentals.html", "Chapter 4 Machine learning fundamentals", " Chapter 4 Machine learning fundamentals “If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing, and reinforcement learning would be the carry.” – Yann LeCun "],
["ml-project-process.html", "Chapter 5 ML project process", " Chapter 5 ML project process Many ML projects get started the wrong way, trying a way to use data rather than using the data to fulfill a need, a need which has a benefit to the organization It is understandable that organizations want to learn from the data they have, but starting without a clear need in mind often leads to wasted efforts because sooner or later it will be discovered that the data available is not sufficient for a useful model. At the start of a ML project there should be a clear formulated need which should be answered by the model, because ML is only a tool to help to achieve the objectives of the organization At the beginning there is a need which ML is suitable to fulfill: Optimize fertilizer usage Improve user experience Reduce energy cost Increase milk production The main project phases Starting with the need the process can be split up in phases as shown below: The process is not sequential but highly iterative as is described in the next chapters "],
["identify-ml-suited-to-fulfill-need.html", "5.1 Identify ML suited to fulfill need", " 5.1 Identify ML suited to fulfill need There are plenty of needs within an organization and different entities within the organization will have different opinions about how to fulfill those needs. Often the people with the needs are not aware of the potential of ML to fulfill the need, on the other hand, often the people with ML knowledge don’t know of the needs. It is therefore necessary to enable that the right people get in contact. Enable contact people with: Needs ML knowlegde There are plenty of reasons why to choose a ML approach to fulfill the need, but there are also plenty of reasons why not to. Reasons why ML approach should be chosen: Suitable solution meets need low development effort no alternative technology Build up ML knowledge Reasons why ML approach should NOT be chosen: Less complex solution available Not enough experience to estimate effort Regulations might prohibit usage of ML due to testing requirements ML right now is very fashionable, but if there is no benefit from choosing ML over another solution other than it is more exciting than think twice before you make your choice. Make sure that the most suitable solution for the need is found, not the fanciest. "],
["gather-data-tbc.html", "5.2 Gather data TBC", " 5.2 Gather data TBC Gathering data is one of the key aspects of an ML project with two main questions: Two fundamental questions: How much data is necessary? Which data is useful? For the first question there are no clear answers, for the second the are plenty of methods to decide whether data is useful or not. 5.2.1 How much data is necessary? There are a number of rules of thumb out there like Rules of thumb: For regression analysis 10 times as many samples than parameters For image recognition 1000 samples per category can go down significantly using pre-trained models but those rules a just a rough guidance since there are plenty of factors influencing the data needed Factors influencing data requirement: model complexity similarity of data the higher the similarity the less new samples help noise on data more samples more computational effort for trees might be counterproductive Sometimes it is easy to create data. When Ayers was thinking about the title of his new book he targeted Google Ads, each with a different title. He got 250,000 samples related to which ad was clicked on most (Ayres 2007). During model training it might become obvious that we run into overfitting, that is the case when training error gets smaller and at the same time the validation error goes up or when the validation error is much higher than the training error. Overfitting as indicator for not enough data: Validation error is much higher than training error Validation error increase with training cycles Model memorizes dat but doesn’t generalise 5.2.1.1 Dealing with small data TBC source https://www.industryweek.com/technology-and-iiot/digital-tools/article/21122846/making-ai-work-with-small-data &gt; Synthetic data generation is used to synthesize novel images that are difficult to collect in real life. Recent advances in techniques such as GANs, variational autoencoders, domain randomization and data augmentation can be used to do this. Transfer learning is a technique that enables AI to learn from a related task where there is ample data available and then uses this knowledge to help solve the small data task. For example, an AI learns to find dents from 1,000 pictures of dents collected from a variety of products and data sources. It can then transfer this knowledge to detect dents in a specific novel product with only a few pictures of dents. Self-supervised learning: Similar to transfer-learning. but the obtained knowledge is acquired by solving a slightly different task and then adapted to small data problem. For example, you can take a lot of OK images and create a puzzle-like grid to be sorted by a base model. Solving this dummy problem will force the model to acquire domain knowledge that can be used as starting point in the small data task. In few-shot learning, the small-data problem is reformulated to help the AI system to learn an easier, less data hungry inspection task while achieving the same goal. In this scenario, AI is given thousands of easier inspection tasks, where each task has only 10 (or another similarly small number) examples. This forces the AI to learn to spot the most important patterns since it only has a small dataset. After that, when you expose this AI to the problem you care about, which has only a similar number of examples, its performance will benefit from it having seen thousands of similar small data tasks. One-shot learning is a special case of few-shot learning where the number of examples per class it has to learn from is one instead of a few (as in the example above). In anomaly detection, the AI sees zero examples of defect and only examples of OK images. The algorithm learns to flag anything that deviates significantly from the OK images as a potential problem. Hand-coded knowledge is an example in which an AI team interviews the inspection engineers and tries to encode as much of their institutional knowledge as possible into a system. Modern machine learning has been trending toward systems that rely on data rather than on human institutional knowledge, but when data isn’t available, skilled AI teams can engineer machine learning systems that leverage this knowledge. Human-in-the-loop describes situations where any of the techniques listed above can be used to build an initial, perhaps somewhat higher error system. But the AI is smart enough to know when it is confident in a label or not and knows to show it to a human expert and defer to their judgement in the latter case. Each time it does so, it also gets to learn from the human, so that it increases accuracy and confidence in its output over time. 5.2.2 Which data is useful? Ideally only data which explain the output are fed into a model. But there might be features which are not known to be of importance. On the other hand there might be features which are overrated as to the importance they have for the output. Anyhow, both can only be known after a model is build. Also, it might be that a feature is valuable for one model but not so much for another model. Data useful?: Could be detected during exploratory data analysis see chapter 5.3 Has to be tested with model Importance can be model dependent Not helpful features cause performance drop more complex models Finding the importance of a feature falls into the scope of feature engineering as described in chapter 5.4.1 References "],
["eda.html", "5.3 Exploratory and quantitative data analysis", " 5.3 Exploratory and quantitative data analysis Before starting to build any model it is good practice to analyze the data. Exploratory and quantitative data analysis are interlinked and therefore can be viewed together. Why data analysis? Understanding characteristic and distribution of response histogram box plot Uncover relationships between predictors and response scatter plots pairwise correlation plot among predictors projection of high-dimensional predictors into lower dimensional space heat maps across predictors The process of exploratory and quantitative data analysis is described in detail in the following example. 5.3.1 Example for exploratory and quantitative data analysis This example is from the online book “Feature Engineering and Selection: A Practical Approach for Predictive Models” (Kuhn and Johnson 2018) 5.3.1.1 Visualization for numeric data In this example the data set on ridership on the Chicago Transit Authority (CTA) “L” train system http://bit.ly/FES-Chicago is used to predict the ridership in order to optimize operation of the train system. Task: predict future ridership volume 14 days in advance Source: Wikimedia Commons, Creative Commons license Since for any prediction of future ridership volume only historical values are available lagging data are used. In this case a lag of 14 days are used, i.e. ridership at day D-14. Distribution of response The distribution of the response gives an indication what to expect from a model. The residuals of a model should have less variation than the variation of the response. If the distribution shows that the frequency of response decreases proportionally with larger values this might be an indication that the response follows a log-normal distribution. Log-transforming the response would induce a normal distribution and often will enable a model to have better prediction performance. Why look at distribution? Gives indication on what to be expected from model performance variance of residuals &lt; variance of response Distribution shaping might enable better prediction performance A box plot gives a quick idea of the distribution of a variable Figure from (Kuhn and Johnson 2018) Box plot legend: Vertical line median of data Blue area represents 50% of data Whiskers indicate upper and lower 25% of data Skewness of distribution In the following picture the relative position of the red line within its surrounding box shows the skewness of the data Figure from Ever.chae [CC BY-SA] What the skewness of data means for its distribution is shown in the picture below Figure from Diva Jain [CC BY-SA] mode: Value which appears most often in data set The box plot doesn’t show if there are multiple peaks or modes. Histograms and violin plots are better suited in that case Figure from (Kuhn and Johnson 2018) Box plot alternatives: Histogram data binned into equal regions height of bar proportional frequency of percentage of samples in region Violin plot compact visualization of distribution histogram-like characteristics could add lower quartile16 median upper quartile17 To compare multiple distributions box plots are still helpful as shown in the next image which shows the distribution of weekday ridership at all stations Figure from (Kuhn and Johnson 2018) Knowledge gained through box plot: Wider distribution than other stations Station is close to stadium of Chicago Clubs \\(\\implies\\) Clubs home game schedule would be important information for model Using faceting and colors to augment visualizations Facets create the same type of plots and splitting the plot into different panels based on some variable Faceting: Same type of plot Based on some variable Below faceting shows that ridership is different for parts of the week \\(\\implies\\) part of the week is important feature The plot below shows the ridership for Clark/Lake, and gives an explanation for the two modes seen in the histogram above, the ridership is vastly different on weekends than during the week. Figure from (Kuhn and Johnson 2018) Scatter plots Scatter plots can add a new dimesions to the analysis Scatter plot: One variable on x-axis, the other variable on y-axis Each sample plotted in this coordinate space Assess relationships between predictors between response and predictors Figure from (Kuhn and Johnson 2018) There are several conclusions which can be drawn from the scatter plot above Knowledge gained through scatter plot: Strong linear relationship between 14-day lag and current-day ridership Two distinct groups of points weekday weekend Plenty of outlier Uncovering explanation of outlier \\(\\implies\\) new useful feature Heatmaps Heatmaps are a versatile plots that displays one predictor on the x-axis and another predictor on the y-axis. Both predictors must be able to be categorized. The categorized predictors form a grid, this grid is filled by another variable. Heatmaps: Categorize predictor for x and y-axis Display another variable on grid categorical or continuous Color depends on either value or category The following heatmap investigates the all cases of weekday ridership less than 10,000. Those represent outlier needing explanation Figure from (Kuhn and Johnson 2018) blue: holiday green: extreme weather Heatmap concept Categorize predictor x-axis: represents year y-axis: represents month and day Red lines indicate weekdays ridership &lt; 10,000 Blue boxes mark holiday seasons Green boxes mark unusual data points both days hat extreme weather \\(\\implies\\) weather is important feature Correlation matrix plots An extension to scatter plot correlation matrix plots show the correlation between each pair of variable. Correlation matrix plots Extension to scatter plot Each variable is represented on the outer x-axis and outer y-axis Matrix colored based on correlation value Interactive figure based on (Kuhn and Johnson 2018) Knowledge gained through analysis of correlation matrix plot Ridership across station is positively correlated for nearly all pairs of stations Correlation for majority of stations is extremely high information present across stations is redundant Columns and rows are organized based on hierarchical cluster analysis stations that have similar correlation vectors will be nearby on the axis helps to identify groups \\(\\implies\\) may point to important features Principal Components Analysis (PCA) One way of condensing many dimensions into just two or three are dimension reductions techniques such as principle components analysis (PCA). An explanation of dimension reductions techniques is given in chapter 5.4.2 Principal components analysis finds combinations of the variables that best summarizes the variability in the original data (Dillon and Goldstein 1984) Figure from (Kuhn and Johnson 2018) Knowledge gained through analysis of PCA plot Component 1 focuses on part of the week Component 2 focuses on changes over time 5.3.2 Visualizations for Categorical Data: Exploring the OkCupid Data OkCupid is an online dating platform.A data set of 50,000 San Fransico user data is available at GitHub Data of OkCupid data set: open text essays related to an individual’s interests and personal descriptions, single-choice type fields such as profession, diet, and education, and multiple-choice fields such as languages spoken and fluency in programming languages. Task: Predict whether the profile’s author was worked in STEM (science, technology, engineering, and math) field 5.3.2.1 Visualizing Relationships between Outcomes and Predictors To find out whether religion would be a good predictor several kinds of plots are compared. Firstly use a bar plot. The figure is ordered from greatest ratio (left) to least ratio (right) of STEM members in that religion. Figure from (Kuhn and Johnson 2018) Bar plot shortcomings: Doesn’t easily show ratio see Hinduism Gives no sense on uncertainty number of profiles per religion the smaller the number the higher the uncertainty A better alternative is a stacked bar plot as shown in the next graph Figure from (Kuhn and Johnson 2018) Stacked plot shortcomings: No sense of frequency No information that there are very few Islamic profiles No sense of uncertainty A better alternative is a error bar plot as shown in the next graph Figure from (Kuhn and Johnson 2018) Error bar plot: Shows uncertainty 95% confidence level18 All plots combined are shown below Figure from (Kuhn and Johnson 2018) each graph should have a clearly defined hypothesis and that this hypothesis is shown concisely in a way that allows the reader to make quick and informative judgments based on the data Conclusion: Each graph should have clearly defined hypothesis Hypothesis shall be shown clearly Reader can make quick, informative judgments based on data \\(\\implies\\) Religion is a useful feature Relationship between a categorical outcome and a numeric predictor As an example the relationship between essay length and STEM and others are analyzed using a histogram as shown below Figure from (Kuhn and Johnson 2018) The histogram shows that the distribution is pretty similar between STEM and others. A way to check whether the predictor could be useful is to train a logistic regression an a basis expansion by building a regression spline smoother of the essay length. Conclusion: Distribution similar for STEM and others Train logisitc regression model on basis expanded essay length Values are close to base rate of 18.5% \\(\\implies\\) essay length is NOT a helpful feature References "],
["feature-engineering.html", "5.4 Feature engineering", " 5.4 Feature engineering Variables that go into model are called: Predictors Features Independent variables Quantity being modeled called: Prediction Outcome Response Dependent variable From input to output \\[outcome = f(features) = f(X_1, X_2, \\dots, Xp) = f(X)\\] \\[\\hat{Y} = \\hat{f}(X)\\] 5.4.1 Feature importance https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html ================================================================ Permutation Importance vs Random Forest Feature Importance (MDI) ================================================================ /Users/uwesterr/CloudProjectsUnderWork/ProjectsUnderWork/MlVhs2020/Python/permutationImportance.py ================================================================= Permutation Importance with Multicollinear or Correlated Features ================================================================= /Users/uwesterr/CloudProjectsUnderWork/ProjectsUnderWork/MlVhs2020/Python/plot_permutation_importance_multicollinear.py (Pedregosa et al. 2011) 5.4.2 Dimensionality reduction algorithms TBD check chapter at kuhn TBD http://www.feat.engineering/numeric-many-to-many.html Dimensionality reduction algorithms are employed in feature engineering to reduce the number of predictors in order to reduce complexity of the model and to reduce correlation between predictors The number of predictors should be reduced but no predictor can be omitted leads to less complex models To ensure that predictors are independent, i.e not correlated might improve prediction performance The predictors don’t have to be interpretable 5.4.2.1 Principle component analysis (PCA) TBD https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c Principle component 5.4.2.2 t-SNE TBD 5.4.2.3 Autoencoders References "],
["model-fit.html", "5.5 Model fit", " 5.5 Model fit Lastly, the no free lunch theorems say that there is no a-priori superiority for any classifier system over the others, so the best classifier for a particular task is itself task-dependent. However there is more compelling theory for the SVM that suggests it is likely to be better choice than many other approaches for many problems. "],
["model-tuning.html", "5.6 Model tuning", " 5.6 Model tuning "],
["after-data-gathering-iteration-is-trump.html", "After data gathering iteration is trump", " After data gathering iteration is trump Figure from (Kuhn and Johnson 2018) (Image Credit: Owlsmcgee [Public domain] ) EDA =&gt; exploratory data analysis source http://www.feat.engineering/intro-intro.html#the-model-versus-the-modeling-process] Exploratory data analysis Find correlations or mutial depence Quantiative analysis Check distribution Long tail =&gt; log of variable Feature engineering19 Create and select meaningful features Model fit Selecting a few suited models Model tuning Vary model hyperpparameters References "],
["machineLearningClasses.html", "Chapter 6 Machine learning classes", " Chapter 6 Machine learning classes There are three major classes of learning problems Supervised learning Unsupervised learning Reinforcement learning In the following chapters each class will be introduced and examples given "],
["supervised-learning.html", "6.1 Supervised learning", " 6.1 Supervised learning "],
["unsupervised-learning.html", "6.2 Unsupervised learning", " 6.2 Unsupervised learning "],
["reinforcement-learning.html", "6.3 Reinforcement learning", " 6.3 Reinforcement learning Reinforcement learning (RL) is the most complex concept of the learning classes. It helps to first look at a simple example where the goal is to find the best possible way in a grid world. How the best possible way is defined and ways to find it will be the topic of this chapter 6.3.1 Elements of reinforcement learning There are five elements of RL as depicted below: Figure from © MIT 6.S191: Introduction to Deep Learning Those elements together build a Markov decision process (MDP) which might be a more familiar term. In order to solve a taks using RL the first step would be if the real world problem can be described as a MDP in terms of the five RL elements. Elements of RL: Agent: takes actions. Environment: the world in which the agent exists and operates. Action \\(a_t\\): a move the agent can make in the environment. Observations: of the environment after taking actions. State \\(s_t\\): a situation which the agent perceives. Reward \\(r_t\\): feedback that measures the success or failure of the agent’s action. In order to understand the above defined terms better it is helpful to look at an example of self driving car in a simulator of MIT (https://selfdrivingcars.mit.edu/deeptraffic/). The agent is the car which can take any of five actions Actions of agent \\(a_t\\): No action Accelerate Break Change lanes to the left to the right The environment is the red marked space segments of the road The state is defined by the spaces in environment which are occupied by another car or empty. The reward is given by the speed of the car, the faster the better Reward A close look at the reward shows that the reward is summed up over time weighted with the so called discount factor \\(\\lambda\\) which is in the range of \\(0\\geq \\lambda \\leq 1\\). Therefore the reward is not only dependent on the immidient reward but also on the to be expected reward. \\[R_{t}=\\sum_{i=t}^{\\infty} \\gamma^{i} r_{i}=\\gamma^{t} r_{t}+\\gamma^{t+1} r_{t+1} \\ldots+\\gamma^{t+n} r_{t+n}+\\dots\\] Find good actions, i.e actions with high total reward A Q-function can be defined which gives the expected value of the total reward for an action \\(a\\) in a given state \\(s\\). \\[Q(s, a)=\\mathbb{E}\\left[R_{t}\\right]\\] where \\(\\mathbb{E}\\) is the expected value of the total reward. So the equation can be read as: Q value if the environment is in state \\(s\\) and the agent performing action \\(a\\) is the expected value of the total reward \\(\\mathbb{E}\\left[R_{t}\\right]\\) The Q-function captures the expected total future reward an agent in state \\(s\\) can receive by executing a certain action \\(a\\) . Find a good policy Ultimately, the agent needs a policy \\(\\pi(s)\\), to infer the best action to take at its state \\(s\\) The strategy is that the policy should choose an action that maximizes future reward \\[\\pi^{*}(s)=\\underset{a}{\\operatorname{argmax}} Q(s, a)\\] 6.3.2 RL algorithms There are two ways to learn the best action Value learning Policy learning Figure from © MIT 6.S191: Introduction to Deep Learning 6.3.2.1 Value learning The task in value learning is to find the Q-values for the states and acitons \\((s,a)\\) Find Q-values Depending on the complexity of the environment it might be difficult to find the Q values. Figure from © MIT 6.S191: Introduction to Deep Learning Q-values can be found using neural networks, the training however is than a two staged task. Traing deep q-learning Figure based on © MIT 6.S191: Introduction to Deep Learning Downsides of Q-learning Q-learning downsides: Complexity: Can model scenarios where the action space is discrete and small Cannot handle continuous action spaces Flexibility: Cannot learn stochastic policies since policy is deterministically computed from the Q function 6.3.2.2 Policy learning Figure based on © MIT 6.S191: Introduction to Deep Learning Traing policy learning Run a policy for a while Increase probability of actions that lead to high rewards Decrease probability of actions that lead to low/no rewards In a more mathematically way this could be written as pseudo code Pseudo code for training: function REINFORCE Initialize \\(\\theta\\) \\(\\mbox{for episode} \\sim \\pi_{\\theta}\\) \\(\\left\\{s_{i}, a_{i}, r_{i}\\right\\}_{i=1}^{T-1} \\leftarrow episode\\) for t = 1 to T-1 \\(\\nabla \\leftarrow \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} | s_{t}\\right) R_{t}\\) \\(\\theta \\leftarrow \\theta+\\alpha \\nabla\\) return \\(\\theta\\) where \\(\\log \\pi_{\\theta}\\left(a_{t} | s_{t}\\right)\\) is the log-likelihood of action \\(a_t\\) 6.3.3 Example self driving car MIT The DeepTraffic website of MIT is a great place to get a feeling for reinforcement learning. Parameters can be varied and the impact can be seen right away without the need to install any code on the computer. DeepTraffic: - Competition of MIT in the frame of their self-driving car course - Target: Create a neural network which drives a car fast through highway traffic - website https://selfdrivingcars.mit.edu/deeptraffic/ - documentation: https://selfdrivingcars.mit.edu/deeptraffic-documentation/ The following variables control the size of the input the net gets – a larger input area provides more information about the traffic situation, but it also makes it harder to learn the relevant parts, and may require longer learning times. Environment: For each car the grid cells below it are filled with the car’s speed, empty cells are filled with a high value to symbolize the potential for speed. Your car gets a car-centric cutout of that map to use as an input to the neural network. You can have a look at it by changing the Road Overlay to Learning Input lanesSide = 1; patchesAhead = 10; patchesBehind = 0; The agent is controlled by a function called learn that receives the current state (provided as a flattened array of the defined learning input cutout), a reward for the last step (in this case the average speed in mph) and has to return one of the following actions: Ouptut of neural network is action: Agent is controlled by function called learn Receives current state flattened array reward of last step retunrs &gt; var noAction = 0; var accelerateAction = 1; var decelerateAction = 2; var goLeftAction = 3; var goRightAction = 4; The learn function is as follows learn = function (state, lastReward) { brain.backward(lastReward); var action = brain.forward(state); draw_net(); draw_stats(); return action; } An overview of the most important variables is given below 6.3.3.1 Crowdsourced Hyperparmeter tuning MIT “DeepTraffic: Crowdsourced Hyperparameter Tuning of Deep Reinforcement Learning Systems for Multi-Agent Dense Traffic Navigation” (Fridman, Terwilliger, and Jenik 2018) in which they present the results of Results of study: Number of submissions: 24,013 Total network parameters optimized: 572.2 million Total duration of RL simulations: 96.6 years The results show that over time the results became better until a plateau was reached. Figure from (Fridman, Terwilliger, and Jenik 2018) Looking at average speed vs neural network parameters it can be seen that Average speed vs NN parameters: Few layers sufficient for high average speed Balance needed between neural network width deepth Figure from (Fridman, Terwilliger, and Jenik 2018) Looking at the training iterations it can be seen that with fewer parameters less iterations are necessary Figure from (Fridman, Terwilliger, and Jenik 2018) There is a clear optimum of three lines at either side of the car Figure from (Fridman, Terwilliger, and Jenik 2018) It can be concluded that it is worth looking into the future when looking at the image below depicting average speed vs pachtes ahead. Figure from (Fridman, Terwilliger, and Jenik 2018) Whereas looking into the past only pays of until 5 patches behind. Figure from (Fridman, Terwilliger, and Jenik 2018) Another idea to improve the performance is to look not only at one image but at several images to get a bette understanding of the dynamics of the scenario. However, the graph below shows that is was not helpful to look into the past Figure from (Fridman, Terwilliger, and Jenik 2018) The reduction factor \\(\\gamma\\) considers future rewards, the higher the value the more attention is given to future awards. The image below shows that paying attention to future rewards is beneficial Figure from (Fridman, Terwilliger, and Jenik 2018) To find a rule out of the parameters analysed a t-SNE mapping of the following parameters onto 2 dimensions was conducted: patches ahead patches behind l2 decay layer count gamma learning rate lanes side training iterations Figure from (Fridman, Terwilliger, and Jenik 2018) The figure shows spots with high average speed, those patches can be used as basis for further improvement. References "],
["MlAlgorithm.html", "Chapter 7 ML algorithms", " Chapter 7 ML algorithms A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets. Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers. The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set. https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py "],
["MlAlgoLinReg.html", "7.1 Linear regression TBD", " 7.1 Linear regression TBD A linear regression is a regression analysis, a statistical method, at which a dependent variable is explained through several independent variables. Simple linear regression Only one independent variable Multiple linear regression more than one independent variables Linear regression algorithm is one of the fundamental supervised learning algorithms. 7.1.1 Example for linear regression In this example the procedure of a linear regression is described Data Given is a set of data created by a linear expression plus some noise \\[y = 3*x+2+n\\] where \\(n\\) is noise The data can be depicted as below. It is easy to be seen that we are looking at a linear function with superimposed noise. Model The task is to find the value for \\(w_0\\) and \\(w_1\\) of a model which is as close as possible to the original function \\[\\hat{y} = w_0*x+w_1\\] Loss function The metric to define how good the model fits the data is defined as mean squared error (MSE) \\[L=(\\hat{y}-y)^2\\] Minimise loss function The difference between \\(\\hat{y}\\) and \\(y\\) shall be small stochastic gradient descent (SGD) can be applied. SGD: Iteratively updating values of \\(w_0, w_1\\) using gradient learning rate \\(\\eta\\) In maths terms this can be written as: \\[ w_{new} = w_{current} - \\eta \\frac{\\partial L}{\\partial w_{current}}\\] A graphical representation of SGD is given below. In this example the loss function can be depicted as a 3D plot. In the current case the surface is flat which makes it easy to find the global optimum Figure from https://nbviewer.jupyter.org/gist/joshfp/85d96f07aaa5f4d2c9eb47956ccdcc88/lesson2-sgd-in-action.ipynb "],
["MlAlgoLogReg.html", "7.2 Logistic regression", " 7.2 Logistic regression Logistic regression is similar to linear regression, however, the value range of the dependent variable y is limited to: \\[0\\leq y \\geq 1\\] Logistic regression is a algorithm with the low computational complexity TBD Low computational complexity y limited range of values \\(0\\leq y \\geq 1\\) maps x on y (\\(y \\leftarrow x\\)) using the logisit function Used of classification The logistic function is depicted in the graph below The logistic function is defined as: Logistic function: \\[logistic(\\eta) = \\frac{1}{1+exp^{-\\eta}}\\] \\[P(Y = 1 \\vert X_i = x_i) = \\frac{1}{1+exp^{-(\\beta_0 + \\beta_1X_1+ \\dots \\beta_n X_n)}}\\] where: \\(\\beta_n\\) are the coeffcients we are searching \\(X_n\\) are the features The second equation reads: The probability of \\(Y=1\\) given the value \\(X=x_i\\) which is exactly the result needed for a classification problem. 7.2.1 Python example logistic regression An example of scikit-learn is given at https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py and emphasises on the difference between linear and logistic regression. The synthetic data set has values either 0 or 1. This can be modeled quite well with logisitc regression, but not at all with linear regression. The python code is given below import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model from scipy.special import expit # General a toy dataset:s it&#39;s just a straight line with some Gaussian noise: xmin, xmax = -5, 5 n_samples = 100 np.random.seed(0) X = np.random.normal(size=n_samples) y = (X &gt; 0).astype(np.float) X[X &gt; 0] *= 4 X += .3 * np.random.normal(size=n_samples) X = X[:, np.newaxis] # Fit the classifier clf = linear_model.LogisticRegression(C=1e5) clf.fit(X, y) # and plot the result plt.figure(1, figsize=(4, 3)) plt.clf() plt.scatter(X.ravel(), y, color=&#39;black&#39;, zorder=20) X_test = np.linspace(-5, 10, 300) loss = expit(X_test * clf.coef_ + clf.intercept_).ravel() plt.plot(X_test, loss, color=&#39;red&#39;, linewidth=3) ols = linear_model.LinearRegression() ols.fit(X, y) plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1) plt.axhline(.5, color=&#39;.5&#39;) plt.ylabel(&#39;y&#39;) plt.xlabel(&#39;X&#39;) plt.xticks(range(-5, 10)) plt.yticks([0, 0.5, 1]) plt.ylim(-.25, 1.25) plt.xlim(-4, 10) plt.legend((&#39;Logistic Regression Model&#39;, &#39;Linear Regression Model&#39;), loc=&quot;lower right&quot;, fontsize=&#39;small&#39;) plt.tight_layout() plt.show() "],
["MlAlgoTrees.html", "7.3 Tree based methods TBD", " 7.3 Tree based methods TBD Tree based methods can be used for different predictions: Types of predictions: Regression trees predict quantitative response Classification trees predict qualitative response Depending on the task the metric to decide how to split the data is different: Metric for splits: Regression Residual sum of squares (RSS) Goal is to minimize the value Classification Gini index Cross entropy Both metrics are numerically very similar Goal is to minimize the value 7.3.1 Splitting metrics Deciding how to split the data at a node is done based on metrics which shall be minimal for the split Residual sum of squares (RSS): Regression trees How close are the samples to the mean of all samples in the resulting node \\(RSS = \\sum_{k=1}^{K}\\sum_{bi€R_j}(y_i-\\hat{y}_{Rj})^2\\) Gini index: Classification How pure is are the resulting leafs \\(G = \\sum_{k=1}^{K}p_i(1-p_i)\\) Cross-entropy: How pure is are the resulting leafs \\(D = - \\sum_{k=1}^{K}p_i \\log_{10}(p_i)\\) An example on how the gini value changes # source: https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree # Parameters n_classes = 3 plot_colors = &quot;ryb&quot; plot_step = 0.02 # Load data iris = load_iris() for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]): # We only take the two corresponding features X = iris.data[:, pair] y = iris.target # Train clf = DecisionTreeClassifier().fit(X, y) # Plot the decision boundary plt.subplot(2, 3, pairidx + 1) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu) plt.xlabel(iris.feature_names[pair[0]]) plt.ylabel(iris.feature_names[pair[1]]) # Plot the training points for i, color in zip(range(n_classes), plot_colors): idx = np.where(y == i) plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.RdYlBu, edgecolor=&#39;black&#39;, s=15) plt.suptitle(&quot;Decision surface of a decision tree using paired features&quot;) plt.legend(loc=&#39;lower right&#39;, borderpad=0, handletextpad=0) plt.axis(&quot;tight&quot;) plt.figure(dpi = 300) # Uwe Sterr added dpi argument for better readability of plot clf = DecisionTreeClassifier().fit(iris.data, iris.target) plot_tree(clf, filled=True) plt.show() 7.3.2 Ensembles Prediction ability of a single decision tree is limited, several techniques are employed to enhance the ability. All of them are aimed at buidling a ensemble of trees which combined have a higher prediction ability than a single tree. Ensembling methods: Bootstrap random sample with replacement Bagging short for bootstrap and aggregation used for example with random forests Boosting build several trees trees learn from errors of previous trees 7.3.2.1 Bootstrap Bootstrapping is resembling method that relies on sampling with replacement as shown in the image below Bootstrap is a widely applicable and extremely powerful statistical tool that allow assigning measures of accuracy associated with a given estimator or statistical learning method. It is used by the random forest algorithm as described in chapter 7.3.3 7.3.2.2 Bagging Bagging is short for bootstrap and aggregation and is a general purpose procedure for reducing the variance of a machine learning algorithm. It is particularly useful and frequently used in the context of decision trees. For random forests the method works as follows: Bootstrapping for random forest: Generate training data by bootstrapping from the original training data set Generate a tree Repeat this M times Predict by averaging the predictions of all trees 7.3.2.3 Boosting Boosting can be utilized for regression and classification problems. It produces an ensemble of weak learners, typically decision trees. The models are build sequentially allowing optimization of an arbitrary differentiable loss function. An example on how boosting works for tree is given in chapter 7.3.4 7.3.2.4 Types of decision trees Two dominant decision tree concepts are: Two dominant concepts used for ensemble trees are described at: Random forest in chapter 7.3.3 Gradient boosted trees in chapter 7.3.4 7.3.3 Random forest TBD Random forest has its name from the randomly selected predictors at each split. The Algorithm is described in (Kuhn and Johnson 2013) p. 200: Random forest algorithm: Select number of models to build m for each model generate bootstrap sample of the original data train a tree model for this sample at each split select randomly k of the original predictors select best predictor partition the data until model stop criteria is meet average prediction of all trees for new samples The algorithm can be depicted as below Random forests have weaknesses and strengths Pros and cons of random forest: Pro Handle higher dimensionality data very well Handles missing values well Cons Due to aggregation of all trees no precise values for regression 7.3.3.1 Python example for random forest The sample code for a random forest classifier produces a ROC image as shown below import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import plot_roc_curve from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split X, y = load_wine(return_X_y=True) y = y == 2 X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) rfc = RandomForestClassifier(n_estimators=10, random_state=42) rfc.fit(X_train, y_train) ax = plt.gca() rfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=ax, alpha=0.8) plt.show() 7.3.3.2 Parameters for random forest The parameters are from the scikit-learn webpage https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier n_estimatorsinteger, optional (default=100) The number of trees in the forest. Changed in version 0.22: The default value of n_estimators changed from 10 to 100 in 0.22. criterionstring, optional (default=”gini”) The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific. max_depthinteger or None, optional (default=None) The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_splitint, float, optional (default=2) The minimum number of samples required to split an internal node: If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split. Changed in version 0.18: Added float values for fractions. min_samples_leafint, float, optional (default=1) The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. If int, then consider min_samples_leaf as the minimum number. If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node. Changed in version 0.18: Added float values for fractions. min_weight_fraction_leaffloat, optional (default=0.) The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. max_featuresint, float, string or None, optional (default=”auto”) The number of features to consider when looking for the best split: If int, then consider max_features features at each split. If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split. If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features. max_leaf_nodesint or None, optional (default=None) Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_decreasefloat, optional (default=0.) A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child. N, N_t, N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed. New in version 0.19. min_impurity_splitfloat, (default=1e-7) Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf. Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19. The default value of min_impurity_split will change from 1e-7 to 0 in 0.23 and it will be removed in 0.25. Use min_impurity_decrease instead. bootstrapboolean, optional (default=True) Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree. oob_scorebool (default=False) Whether to use out-of-bag samples to estimate the generalization accuracy. n_jobsint or None, optional (default=None) The number of jobs to run in parallel. fit, predict, decision_path and apply are all parallelized over the trees. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. random_stateint, RandomState instance or None, optional (default=None) Controls both the randomness of the bootstrapping of the samples used when building trees (if bootstrap=True) and the sampling of the features to consider when looking for the best split at each node (if max_features &lt; n_features). See Glossary for details. verboseint, optional (default=0) Controls the verbosity when fitting and predicting. warm_startbool, optional (default=False) When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See the Glossary. class_weightdict, list of dicts, “balanced”, “balanced_subsample” or None, optional (default=None) Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown. For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ccp_alphanon-negative float, optional (default=0.0) Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details. New in version 0.22. max_samplesint or float, default=None If bootstrap is True, the number of samples to draw from X to train each base estimator. If None (default), then draw X.shape[0] samples. If int, then draw max_samples samples. If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0, 1). New in version 0.22. 7.3.4 Boosted trees TBD Boosted trees are an ensemble of weak learners where each learner is build on the knowledge gained by all previous learners. The following image depicts the algorithm which can be summarized: Boosted tree algorithm Generate small tree Calculate residuals for all samples Use residuals to generate next tree Combine all trees to build new model Repeat from step 1. The algorithm is depicted below Figure based on (Zhang et al. 2018), added explanation at the right hand side Boosted trees have weaknesses and strengths Pros and cons of boosted trees: Pro Supports different loss functions Cons Prone to overfitting Carefully tuning of hyperparameters is required The algorithm of boosted trees for regression is described in a rather mathematically way in (James et al. 2013) p. 323: Set \\(\\hat{f} = 0\\) and \\(r_i = y_i\\) for all \\(i\\) in the training set For \\(b=1,2,\\dots,B\\) repeat: Fit a tree \\(\\hat{f}^b\\) with \\(d\\) splits (\\(d+1\\) terminal nodes) to the training data \\((X,r)\\) Update \\(\\hat{f}\\) by adding in a shrunken version of the new tree \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)\\) Update the residuals \\(r_i \\leftarrow r_i + \\lambda \\hat{f}^b(x_i)\\) Output the boosted model \\(\\hat{f}(x) = \\sum_{b=1}^{B}\\lambda \\hat{f}^b(x)\\) Another introduction to boosted trees is given at the XGBoost Documentation with a thorough mathematical explanation of the approach. 7.3.4.1 Python examples for boosted trees A popular library for boosted trees in Python is XGBoost, the documentation is hosted at https://xgboost.readthedocs.io/en/latest/. Plenty of examples are on the GitHub page https://github.com/dmlc/xgboost/tree/master/demo/guide-python. The example script basic_walkthrough.py is shown below #!/usr/bin/python import numpy as np import scipy.sparse import pickle import xgboost as xgb ### simple example # load file from text file, also binary buffer generated by xgboost dtrain = xgb.DMatrix(&#39;../data/agaricus.txt.train&#39;) dtest = xgb.DMatrix(&#39;../data/agaricus.txt.test&#39;) # specify parameters via map, definition are same as c++ version param = {&#39;max_depth&#39;:2, &#39;eta&#39;:1, &#39;silent&#39;:1, &#39;objective&#39;:&#39;binary:logistic&#39;} # specify validations set to watch performance watchlist = [(dtest, &#39;eval&#39;), (dtrain, &#39;train&#39;)] num_round = 2 bst = xgb.train(param, dtrain, num_round, watchlist) # this is prediction preds = bst.predict(dtest) labels = dtest.get_label() print(&#39;error=%f&#39; % (sum(1 for i in range(len(preds)) if int(preds[i] &gt; 0.5) != labels[i]) / float(len(preds)))) bst.save_model(&#39;0001.model&#39;) # dump model bst.dump_model(&#39;dump.raw.txt&#39;) # dump model with feature map bst.dump_model(&#39;dump.nice.txt&#39;, &#39;../data/featmap.txt&#39;) # save dmatrix into binary buffer dtest.save_binary(&#39;dtest.buffer&#39;) # save model bst.save_model(&#39;xgb.model&#39;) # load model and data in bst2 = xgb.Booster(model_file=&#39;xgb.model&#39;) dtest2 = xgb.DMatrix(&#39;dtest.buffer&#39;) preds2 = bst2.predict(dtest2) # assert they are the same assert np.sum(np.abs(preds2 - preds)) == 0 # alternatively, you can pickle the booster pks = pickle.dumps(bst2) # load model and data in bst3 = pickle.loads(pks) preds3 = bst3.predict(dtest2) # assert they are the same assert np.sum(np.abs(preds3 - preds)) == 0 ### # build dmatrix from scipy.sparse print(&#39;start running example of build DMatrix from scipy.sparse CSR Matrix&#39;) labels = [] row = []; col = []; dat = [] i = 0 for l in open(&#39;../data/agaricus.txt.train&#39;): arr = l.split() labels.append(int(arr[0])) for it in arr[1:]: k,v = it.split(&#39;:&#39;) row.append(i); col.append(int(k)); dat.append(float(v)) i += 1 csr = scipy.sparse.csr_matrix((dat, (row, col))) dtrain = xgb.DMatrix(csr, label=labels) watchlist = [(dtest, &#39;eval&#39;), (dtrain, &#39;train&#39;)] bst = xgb.train(param, dtrain, num_round, watchlist) print(&#39;start running example of build DMatrix from scipy.sparse CSC Matrix&#39;) # we can also construct from csc matrix csc = scipy.sparse.csc_matrix((dat, (row, col))) dtrain = xgb.DMatrix(csc, label=labels) watchlist = [(dtest, &#39;eval&#39;), (dtrain, &#39;train&#39;)] bst = xgb.train(param, dtrain, num_round, watchlist) print(&#39;start running example of build DMatrix from numpy array&#39;) # NOTE: npymat is numpy array, we will convert it into scipy.sparse.csr_matrix in internal implementation # then convert to DMatrix npymat = csr.todense() dtrain = xgb.DMatrix(npymat, label=labels) watchlist = [(dtest, &#39;eval&#39;), (dtrain, &#39;train&#39;)] bst = xgb.train(param, dtrain, num_round, watchlist) The parameters below are from their webpage https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters Learning Task Parameters¶ Specify the learning task and the corresponding learning objective. The objective options are below: objective [default=reg:squarederror] reg:squarederror: regression with squared loss. reg:squaredlogerror: regression with squared log loss 12[𝑙𝑜𝑔(𝑝𝑟𝑒𝑑+1)−𝑙𝑜𝑔(𝑙𝑎𝑏𝑒𝑙+1)]212[log(pred+1)−log(label+1)]2 . All input labels are required to be greater than -1. Also, see metric rmsle for possible issue with this objective. reg:logistic: logistic regression binary:logistic: logistic regression for binary classification, output probability binary:logitraw: logistic regression for binary classification, output score before logistic transformation binary:hinge: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. count:poisson –poisson regression for count data, output mean of poisson distribution max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR). multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes) multi:softprob: same as softmax, but output a vector of ndata nclass, which can be further reshaped to ndata nclass matrix. The result contains predicted probability of each data point belonging to each class. rank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. base_score [default=0.5] The initial prediction score of all instances, global bias For sufficient number of iterations, changing this value will not have too much effect. eval_metric [default according to objective] Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking) User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one The choices are listed below: rmse: root mean square error rmsle: root mean square log error: 1𝑁[𝑙𝑜𝑔(𝑝𝑟𝑒𝑑+1)−𝑙𝑜𝑔(𝑙𝑎𝑏𝑒𝑙+1)]2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√1N[log(pred+1)−log(label+1)]2 . Default metric of reg:squaredlogerror objective. This metric reduces errors generated by outliers in dataset. But because log function is employed, rmsle might output nan when prediction value is less than -1. See reg:squaredlogerror for other requirements. mae: mean absolute error logloss: negative log-likelihood error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances. error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’. merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases). mlogloss: Multiclass logloss. auc: Area under the curve aucpr: Area under the PR curve ndcg: Normalized Discounted Cumulative Gain map: Mean Average Precision ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation. ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions. poisson-nloglik: negative log-likelihood for Poisson regression gamma-nloglik: negative log-likelihood for gamma regression cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression gamma-deviance: residual deviance for gamma regression tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter) seed [default=0] Random number seed. This parameter is ignored in R package, use set.seed() instead. References "],
["MlAlgoSvm.html", "7.4 Support Vector Machine (SVM) TBD", " 7.4 Support Vector Machine (SVM) TBD A good explanation of the theory behind SVMs is given in (Tibshirani et al. 2013) The support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels. We will now discuss this extension, the details of which are somewhat complex and are beyond the scope of this report. The main idea is to enlarge the feature space in order to accommodate a non-linear boundary between the classes. The kernel approach that we describe here is simply an efficient computational approach for enacting this idea. The solution to the support vector classifier problem involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two r-vectors a and b is defined as \\(\\langle a, b\\rangle=\\sum_{i=1}^{r} a_{i} b_{i}\\);. Thus the inner product of two observations \\(x_{i}, x_{i^{\\prime}}\\) is given by \\[\\begin{equation} \\left\\langle x_{i}, x_{i^{\\prime}}\\right\\rangle=\\sum_{j=1}^{p} x_{i j} x_{i^{\\prime} j} \\tag{7.1} \\end{equation}\\] It can be shown that - The linear support vector classifier can be represented as: \\[\\begin{equation} f(x)=\\beta_{0}+\\sum_{i=1}^{n} \\alpha_{i}\\left\\langle x, x_{i}\\right\\rangle \\tag{7.2} \\end{equation}\\] where there are n parameters \\(\\alpha_{i}, \\quad i=1, \\dots, n\\), one per training observation. To estimate the parameters \\(\\alpha_{1}, \\ldots, \\alpha_{n} \\text { and } \\beta_{0}\\), all we need are the \\(\\left( \\begin{array}{l}{n} \\\\ {2}\\end{array}\\right)\\) inner products \\(\\left\\langle x_{i}, x_{i^{\\prime}}\\right\\rangle\\) between all pairs of training observations. The notation \\(\\left( \\begin{array}{c}{n} \\\\ {2}\\end{array}\\right)\\) means \\(n(n-1) / 2\\) and gives the number of pairs among a set of \\(n\\) items. Notice that in (7.2), in order to evaluate the function f(x), we need to compute the inner product between the new point x and each of the training points \\(x_i\\). However, it turns out that \\(α_i\\) is nonzero only for the support vectors in the solution—that is, if a training observation is not a support vector, then its \\(α_i\\) equals zero. So if S is the collection of indices of these support points, we can rewrite any solution function of the form (7.2) as \\[\\begin{equation} f(x)=\\beta_{0}+\\sum_{i \\in \\mathcal{S}} \\alpha_{i}\\left\\langle x, x_{i}\\right\\rangle \\tag{7.3} \\end{equation}\\] which typically involves far fewer terms than in (7.2) 7.4.1 Kernels To summarize, in representing the linear classifier \\(f (x)\\), and in computing its coefficients, all we need are inner products. Now suppose that every time the inner product (7.1) appears in the representation (7.2), or in a calculation of the solution for the support vector classifier, we replace it with a generalization of the inner product of the form \\[\\begin{equation} K\\left(x_{i}, x_{i^{\\prime}}\\right) \\tag{7.4} \\end{equation}\\] where K is some function that we will refer to as a kernel. A kernel is a function that quantifies the similarity of two observations. For instance, we could simply take \\[\\begin{equation} K\\left(x_{i}, x_{i^{\\prime}}\\right)=\\sum_{j=1}^{p} x_{i j} x_{i^{\\prime} j} \\tag{7.5} \\end{equation}\\] which would just give us back the support vector classifier. Equation (7.5) is known as a linear kernel because the support vector classifier is linear in the features; the linear kernel essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation. 7.4.1.1 Polynomial Kernel But one could instead choose another form for (7.4). For instance, one could replace every instance of \\(\\sum_{j=1}^{p} x_{i j} x_{i^{\\prime} j}\\) with the quantity \\[\\begin{equation} K\\left(x_{i}, x_{i^{\\prime}}\\right)=\\left(1+\\sum_{j=1}^{p} x_{i j} x_{i^{\\prime} j}\\right)^{d} \\tag{7.6} \\end{equation}\\] This is known as a polynomial kernel of degree d, where d is a positive integer. Using such a kernel with d &gt; 1, instead of the standard linear kernel (7.5), in the support vector classifier algorithm leads to a much more flexible decision boundary. It essentially amounts to fitting a support vector classifier in a higher-dimensional space involving polynomials of degree d, rather than in the original feature space. When the support vector classifier is combined with a non-linear kernel such as (7.6), the resulting classifier is known as a support vector machine. Note that in this case the (non-linear) function has the form \\[\\begin{equation} f(x)=\\beta_{0}+\\sum_{i \\in \\mathcal{S}} \\alpha_{i} K\\left(x, x_{i}\\right) \\tag{7.7} \\end{equation}\\] The polynomial kernel shown in (7.6) is one example of a possible non-linear kernel, but alternatives abound. 7.4.1.2 Radial Kernel Another popular choice is the radial kernel, which takes the form \\[\\begin{equation} K\\left(x_{i}, x_{i^{\\prime}}\\right)=\\exp \\left(-\\gamma \\sum_{j=1}^{p}\\left(x_{i j}-x_{i^{\\prime} j}\\right)^{2}\\right) \\tag{7.8} \\end{equation}\\] In (7.8), γ is a positive constant. How does the radial kernel (7.8) actually work? If a given test observation \\(x^{*}=\\left(x_{1}^{*} \\ldots x_{p}^{*}\\right)^{T}\\) is far from a training observation \\(x_i\\) in terms of Euclidean distance, then \\(\\sum_{j=1}^{p}\\left(x_{j}^{*}-x_{i j}\\right)^{2}\\) will be large, and so \\(K\\left(x_{i}, x_{i^{\\prime}}\\right)={\\exp \\left(-\\gamma \\sum_{j=1}^{p}\\left(x_{j}^{*}-x_{i j}\\right)^{2}\\right)}\\) will be very tiny. This means that in (7.7), \\(x_{i}\\) will play virtually no role in \\(f\\left(x^{*}\\right)\\). Recall that the predicted class label for the test observation \\(x^{*}\\) is based on the sign of \\(f\\left(x^{*}\\right)\\). In other words, training observations that are far from \\(x^{*}\\) will play essentially no role in the predicted class label for \\(x^{*}\\). This means that the radial kernel has very local behaviour, in the sense that only nearby training observations have an effect on the class label of a test observation. What is the advantage of using a kernel rather than simply enlarging the feature space using functions of the original features? One advantage is computational, and it amounts to the fact that using kernels, one need only compute \\(K\\left(x_{i}, x_{i^{\\prime}}\\right)\\) for \\(\\left( \\begin{array}{l}{n} \\\\ {2}\\end{array}\\right)\\) distinct pairs \\(i\\), \\(i^{\\prime}\\).This can be done without explicitly working in the enlarged feature space. This is im- portant because in many applications of SVMs, the enlarged feature space is so large that computations are intractable. For some kernels, such as the radial kernel (7.8), the feature space is implicit and infinite-dimensional, so we could never do the computations there anyway! 7.4.2 Python example for SVM Two examples are given, both take images and classify them. 7.4.2.1 SVM face recognition The following example is given at scikit-learn.org It uses a SVM with rbf kernel grid search for hyper parameter C gamma using scikit-learn GridSearchCV PCA to create input features 150 dimensions See below some examples of the resulting classification of the algorithm Total dataset size: n_samples: 1288 n_features: 1850 n_classes: 7 Extracting the top 150 eigenfaces from 966 faces done in 0.320s Projecting the input data on the eigenfaces orthonormal basis done in 0.013s Fitting the classifier to the training set done in 28.379s Best estimator found by grid search: SVC(C=1000.0, break_ties=False, cache_size=200, class_weight=&#39;balanced&#39;, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=0.005, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) Predicting people&#39;s names on the test set done in 0.045s precision recall f1-score support Ariel Sharon 0.88 0.54 0.67 13 Colin Powell 0.80 0.87 0.83 60 Donald Rumsfeld 0.94 0.63 0.76 27 George W Bush 0.83 0.98 0.90 146 Gerhard Schroeder 0.91 0.80 0.85 25 Hugo Chavez 1.00 0.53 0.70 15 Tony Blair 0.96 0.75 0.84 36 accuracy 0.85 322 macro avg 0.90 0.73 0.79 322 weighted avg 0.86 0.85 0.84 322 Confusion matrix [[ 7 1 0 5 0 0 0] [ 1 52 0 7 0 0 0] [ 0 3 17 7 0 0 0] [ 0 3 0 143 0 0 0] [ 0 1 0 3 20 0 1] [ 0 4 0 2 1 8 0] [ 0 1 1 6 1 0 27]] The python code is given below from time import time import logging import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.datasets import fetch_lfw_people from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from sklearn.decomposition import PCA from sklearn.svm import SVC print(__doc__) # Display progress logs on stdout logging.basicConfig(level=logging.INFO, format=&#39;%(asctime)s %(message)s&#39;) # ############################################################################# # Download the data, if not already on disk and load it as numpy arrays lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4) # introspect the images arrays to find the shapes (for plotting) n_samples, h, w = lfw_people.images.shape # for machine learning we use the 2 data directly (as relative pixel # positions info is ignored by this model) X = lfw_people.data n_features = X.shape[1] # the label to predict is the id of the person y = lfw_people.target target_names = lfw_people.target_names n_classes = target_names.shape[0] print(&quot;Total dataset size:&quot;) print(&quot;n_samples: %d&quot; % n_samples) print(&quot;n_features: %d&quot; % n_features) print(&quot;n_classes: %d&quot; % n_classes) # ############################################################################# # Split into a training set and a test set using a stratified k fold # split into a training and testing set X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=42) # ############################################################################# # Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled # dataset): unsupervised feature extraction / dimensionality reduction n_components = 150 print(&quot;Extracting the top %d eigenfaces from %d faces&quot; % (n_components, X_train.shape[0])) t0 = time() pca = PCA(n_components=n_components, svd_solver=&#39;randomized&#39;, whiten=True).fit(X_train) print(&quot;done in %0.3fs&quot; % (time() - t0)) eigenfaces = pca.components_.reshape((n_components, h, w)) print(&quot;Projecting the input data on the eigenfaces orthonormal basis&quot;) t0 = time() X_train_pca = pca.transform(X_train) X_test_pca = pca.transform(X_test) print(&quot;done in %0.3fs&quot; % (time() - t0)) # ############################################################################# # Train a SVM classification model print(&quot;Fitting the classifier to the training set&quot;) t0 = time() param_grid = {&#39;C&#39;: [1e3, 5e3, 1e4, 5e4, 1e5], &#39;gamma&#39;: [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], } clf = GridSearchCV( SVC(kernel=&#39;rbf&#39;, class_weight=&#39;balanced&#39;), param_grid ) clf = clf.fit(X_train_pca, y_train) print(&quot;done in %0.3fs&quot; % (time() - t0)) print(&quot;Best estimator found by grid search:&quot;) print(clf.best_estimator_) # ############################################################################# # Quantitative evaluation of the model quality on the test set print(&quot;Predicting people&#39;s names on the test set&quot;) t0 = time() y_pred = clf.predict(X_test_pca) print(&quot;done in %0.3fs&quot; % (time() - t0)) print(classification_report(y_test, y_pred, target_names=target_names)) print(confusion_matrix(y_test, y_pred, labels=range(n_classes))) # ############################################################################# # Qualitative evaluation of the predictions using matplotlib def plot_gallery(images, titles, h, w, n_row=3, n_col=4): &quot;&quot;&quot;Helper function to plot a gallery of portraits&quot;&quot;&quot; plt.figure(figsize=(1.8 * n_col, 2.4 * n_row)) plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35) for i in range(n_row * n_col): plt.subplot(n_row, n_col, i + 1) plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray) plt.title(titles[i], size=12) plt.xticks(()) plt.yticks(()) # plot the result of the prediction on a portion of the test set def title(y_pred, y_test, target_names, i): pred_name = target_names[y_pred[i]].rsplit(&#39; &#39;, 1)[-1] true_name = target_names[y_test[i]].rsplit(&#39; &#39;, 1)[-1] return &#39;predicted: %s\\ntrue: %s&#39; % (pred_name, true_name) prediction_titles = [title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])] plot_gallery(X_test, prediction_titles, h, w) # plot the gallery of the most significative eigenfaces eigenface_titles = [&quot;eigenface %d&quot; % i for i in range(eigenfaces.shape[0])] plot_gallery(eigenfaces, eigenface_titles, h, w) plt.show() Figure from Alisneaky, svg version by User:Zirguezi [CC BY-SA (https://creativecommons.org/licenses/by-sa/4.0)] 7.4.2.2 SVM Image recognition From the scikit-learn help page an example showing how the scikit-learn can be used to recognize images of hand-written digits. The input data are hand written numbers Top: training data Bottom: Prediction The confusion matrix is given below and shows for example that a true “3” is often mistaken as a “8” (see red circle) print(__doc__) # Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt; # License: BSD 3 clause # Standard scientific Python imports import matplotlib.pyplot as plt # Import datasets, classifiers and performance metrics from sklearn import datasets, svm, metrics from sklearn.model_selection import train_test_split # The digits dataset digits = datasets.load_digits() # The data that we are interested in is made of 8x8 images of digits, let&#39;s # have a look at the first 4 images, stored in the `images` attribute of the # dataset. If we were working from image files, we could load them using # matplotlib.pyplot.imread. Note that each image must have the same size. For these # images, we know which digit they represent: it is given in the &#39;target&#39; of # the dataset. _, axes = plt.subplots(2, 4) images_and_labels = list(zip(digits.images, digits.target)) for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) ax.set_title(&#39;Training: %i&#39; % label) # To apply a classifier on this data, we need to flatten the image, to # turn the data in a (samples, feature) matrix: n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1)) # Create a classifier: a support vector classifier classifier = svm.SVC(gamma=0.001) # Split data into train and test subsets X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.5, shuffle=False) # We learn the digits on the first half of the digits classifier.fit(X_train, y_train) # Now predict the value of the digit on the second half: predicted = classifier.predict(X_test) images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted)) for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation=&#39;nearest&#39;) ax.set_title(&#39;Prediction: %i&#39; % prediction) print(&quot;Classification report for classifier %s:\\n%s\\n&quot; % (classifier, metrics.classification_report(y_test, predicted))) disp = metrics.plot_confusion_matrix(classifier, X_test, y_test) disp.figure_.suptitle(&quot;Confusion Matrix&quot;) print(&quot;Confusion matrix:\\n%s&quot; % disp.confusion_matrix) plt.show() References "],
["neural-networks.html", "7.5 Neural networks", " 7.5 Neural networks non linear activation softmax types of layers siehe keras fully connected 7.5.1 Convolutional Neural Network (CNN) TBD A Convolutional Neural Network is an neural network are mainly used to analyze image and audio data. The following explanation is based on The learning machine tutorial “Classification Convolutional Neural Network (CNN)” https://www.thelearningmachine.ai/cnn A classical CNN consists of one or more convolutoional layers one or more pooling layers one or more fully connected layers A classical CNN is depicted in the image below Figure from https://www.thelearningmachine.ai/cnn An image is an 3 dimensional array where the third dimension are for the colors red, green and blue, in case of an RGB image. An image can therefore be represented as shown below Figure from https://www.thelearningmachine.ai/cnn To analyze an image the spatial relation between different pixels hold important information. Therefore it is beneficial to use an algorithm which looks not only at one pixel but also at the neighbouring pixels. One way of doing so is to slide an 2 dimensional array over the image array as can be seen below Figure from https://www.thelearningmachine.ai/cnn The 2 dimensional array is called a kernel and is named depending on its dimensions. The kernel in the graph below is a “3 by 3 kernel”. Sliding the array across the image as gives a set of numbers as shown above. Those kernels can detect structures in images such as lines boxes circles and kernels in later layers in the CNN can detect more complex structures such as faces wheels trees Figure from https://www.thelearningmachine.ai/cnn A kernel has the same depth of the input, in a case of an RGB image, the depth of the kernel is 3. The output of the kernels is added, for each of the positions of the kernels there is one value at the output. The movement across the image is based on the stride parameters for x and y direction. In the case below the stride is as follows stride x-direction = 1 stride y-direction = 1 Depending on stride with and image dimension it might be necessary to apply padding, for details on padding see deepAi Figure from https://www.thelearningmachine.ai/cnn 7.5.1.1 Pooling layer The pooling layer reduces the dimension as shown below. There are different types of pooling layers max average below the working mechanism for a max pooling layer is shown. The stride for x and y is one, the dimension of the 5 by 5 input is reduced to 3 by 3 Figure from https://www.thelearningmachine.ai/cnn After one or more combination of convolutional and pooling layers one or more fully connected layers learn how to classify the image based on non-linear combinations of the high-level features learned by the previous layers. https://blogs.nvidia.com/wp-content/uploads/2018/09/autos-672x378.png The fully connected layer with the soft-max activation at the end gives the probability of each category, often as a result the three to five classes with the highest probability are reported. convolutional and pooling layers =&gt; high-level features fully connected layers =&gt; combine non-linear high level features for classification Figure from https://www.thelearningmachine.ai/cnn The operating principle of a CNN is shown below 7.5.2 RNN TBD 7.5.3 GANs GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f credit of the image (Zhu et al. 2017) Generative models learn the intrinsic distribution function of the input data p(x) (or p(x,y) if there are multiple targets/classes in the dataset), allowing them to generate both synthetic inputs x’ and outputs/targets y’, typically given some hidden parameters. GANs they have proven to be really succesfull in modeling and generating high dimensional data, which is why they’ve become so popular. Nevertheless they are not the only types of Generative Models, others include Variational Autoencoders (VAEs) and pixelCNN/pixelRNN and real NVP. Each model has its own tradeoffs. Some of the most relevant GAN pros and cons for the are: They currently generate the sharpest images They are easy to train (since no statistical inference is required), and only back-propogation is needed to obtain gradients GANs are difficult to optimize due to unstable training dynamics. No statistical inference can be done with them (except here): GANs belong to the class of direct implicit density models; they model p(x) without explicitly defining the p.d.f. A neural network G(z, θ₁) Jupyter notebook on github https://github.com/diegoalejogm/gans/blob/master/1.%20Vanilla%20GAN%20PyTorch.ipynb References "],
["a-gentle-introduction-to-cyclegan-for-image-translation.html", "7.6 A Gentle Introduction to CycleGAN for Image Translation", " 7.6 A Gentle Introduction to CycleGAN for Image Translation https://machinelearningmastery.com/what-is-cyclegan/ 7.6.1 Examples for GANs 7.6.1.1 gans-awesome-applications a list of plenty of applications can be found at https://github.com/nashory/gans-awesome-applications "],
["software-that-can-generate-photos-from-paintings-turn-horses-into-zebras-perform-style-transfer-and-more-.html", "7.7 Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.", " 7.7 Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more. with software to do style transfer https://github.com/junyanz/CycleGAN ### Pix2pix framework Jupyter notebook for Colab https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb#scrollTo=aDyGj8DmXCJI "],
["transformers-tbd.html", "7.8 Transformers TBD", " 7.8 Transformers TBD Attention Is All You Need https://arxiv.org/abs/1706.03762 Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) What is a Transformer? The Illustrated Transformer "],
["food-for-the-algorithms-data.html", "Chapter 8 Food for the algorithms: Data ", " Chapter 8 Food for the algorithms: Data "],
["nlp.html", "8.1 NLP", " 8.1 NLP CCMatrix: A billion-scale bitext data set for training translation models Facebook learned a multilingual sentence embedding to help it represent sentences from different languages in the same featurespace, and then using the distance between sentences in featurespace to help the system figure out if they’re parallel sentences from two different languages. https://ai.facebook.com/blog/ccmatrix-a-billion-scale-bitext-data-set-for-training-translation-models/ "],
["discovering-millions-of-datasets-on-the-web.html", "8.2 Discovering millions of datasets on the web", " 8.2 Discovering millions of datasets on the web Published Jan 23, 2020 by Google Across the web, there are millions of datasets about nearly any subject that interests you. If you’re looking to buy a puppy, you could find datasets compiling complaints of puppy buyers or studies on puppy cognition. Or if you like skiing, you could find data on revenue of ski resorts or injury rates and participation numbers. Dataset Search has indexed almost 25 million of these datasets, giving you a single place to search for datasets and find links to where the data is. Over the past year, people have tried it out and provided feedback, and now Dataset Search is officially out of beta. https://blog.google/products/search/discovering-millions-datasets-web/ "],
["ExplainableMl.html", "Chapter 9 Explainable ML tbd", " Chapter 9 Explainable ML tbd adfdasf "],
["lime-tbd.html", "9.1 Lime tbd", " 9.1 Lime tbd First paper on LIME was (Tulio Ribeiro, Singh, and Guestrin 2016) References "],
["alibi-tbd.html", "9.2 alibi tbd", " 9.2 alibi tbd https://github.com/SeldonIO/alibi https://docs.seldon.io/projects/alibi/en/stable/overview/algorithms.html "],
["tf-explain-tbd.html", "9.3 tf-explain tbd", " 9.3 tf-explain tbd https://github.com/sicara/tf-explain "],
["keras-salient-object-visualization.html", "9.4 keras-salient-object-visualization", " 9.4 keras-salient-object-visualization Keras implementation of nvidia paper ‘Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car’. The goal of the visualization is to explain what Donkey Car (https://github.com/wroscoe/donkey) learns and how it makes its decisions. The central idea in discerning the salient objects is finding parts of the image that correspond to locations where the feature maps of CNN layers have the greatest activations. Original paper: https://arxiv.org/pdf/1704.07911.pdf https://arxiv.org/abs/1704.07911 9.4.1 Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car (Bojarski et al. 2017) Enable further system improvement Create trust that the system is paying attention to the essential cues 9.4.2 VisualBackProp: efficient visualization of CNNs (Bojarski et al. 2016) https://arxiv.org/abs/1611.05418 References "],
["MlResources.html", "Chapter 10 ML online resources ", " Chapter 10 ML online resources "],
["in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos.html", "10.1 In-depth introduction to machine learning in 15 hours of expert videos", " 10.1 In-depth introduction to machine learning in 15 hours of expert videos https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/ 10.1.1 An Introduction to Statistical Learning This book provides an introduction to statistical learning methods. http://faculty.marshall.usc.edu/gareth-james/ISL/ "],
["the-learning-machine.html", "10.2 The learning machine", " 10.2 The learning machine https://www.thelearningmachine.ai WELCOME TO TLM TLM is a new open-source project that aims to create an interactive textbook containing A-Z explanations of concepts and methods, algorithms and their code implementations from the fields of data science, machine learning, deep learning, natural language processing, statistics, and more. "],
["deepai-the-front-page-of-a-i-.html", "10.3 DeepAI: The front page of A.I.", " 10.3 DeepAI: The front page of A.I. https://deepai.org The most popular research, guides, news and more in artificial intelligence "],
["tensorflow-tutorials.html", "10.4 TensorFlow tutorials", " 10.4 TensorFlow tutorials https://www.tensorflow.org/tutorials The website has a range of examples which can be TensorFlow tutorials examples: - run in Colab - looked at in GitHub - downloaded as notebook Example include Text Image Generative Transfer learning 10.4.1 MIT 6.S191 Introduction to Deep Learning MIT’s official introductory course on deep learning methods with applications in game play, and more! http://introtodeeplearning.com "],
["fastai.html", "10.5 FastAI", " 10.5 FastAI https://course.fast.ai "],
["embedding-projector.html", "10.6 Embedding Projector", " 10.6 Embedding Projector Visualisation of PCA t-SNE UMAP Available data sets Word2Vec all Word2Vec 10k Minst with images Iris http://projector.tensorflow.org "],
["tensorboard-playground.html", "10.7 Tensorboard playground", " 10.7 Tensorboard playground Tinker With a Neural Network Right Here in Your Browser. Don’t Worry, You Can’t Break It. We Promise. https://playground.tensorflow.org "],
["KaggleExamples.html", "Chapter 11 Examples in Kaggle", " Chapter 11 Examples in Kaggle Kaggle is a platform for data scientists and machine learning practitioners which allows users to: find datasets publish datasets exlplore models on web-based data-science environment in Python R SQLite Julia work with other machine learning practitioners on competitions 379 (Feb 2020) Host competitions engage in discussions find jobs Same facts about Kaggle Founded April 2010 Headquarter in San Francisco More than 1 million member since March 2017 Kinds of competitions - Featured - generally commercially-purposed prediction problems - with up to $1.5 mio price money - 177 (Feb 2020) - 2 active - good opportunity to learn from the best - Research - more experimental than featured competition problems - with up to $50,000 price money - 94 (Feb 2020) - 2 active - Recruitment - corporation-curated challenges - teams of size one - with up to $20,000 price money - 17 (Feb 2020) - 0 active - interested participants can upload their resume for consideration by the host - price: job interview - Getting started - easiest, most approachable competitions - getting the foot in the door - 11 (Feb 2020) - 4 active - Playground - one step above Getting Started in difficulty - with up to $30,000 price money - 60 (Feb 2020) - 2 active - Masters - limited participation - only by invitation - with up to $125,000 price money - 6 (Feb 2020) - 0 active More details on how competitions are conducted can be found at https://www.kaggle.com/docs/competitions "],
["melbourne-university-aesmathworksnih-seizure-prediction.html", "Chapter 12 Melbourne University AES/MathWorks/NIH Seizure Prediction", " Chapter 12 Melbourne University AES/MathWorks/NIH Seizure Prediction The competition was hosted in 2016 by at https://www.kaggle.com/c/melbourne-university-seizure-prediction and was subtitled “Predict seizures in long-term human intracranial EEG recordings” The price money was $20,000, the competition ended 24.11.2016 and 478 teams had submitted a solution. The competition was sponsored by: MathWorks National Institutes of Health (NINDS) American Epilepsy Society University of Melbourne and organized in partnership with: Alliance for Epilepsy Research University of Pennsylvania Mayo Clinic. Challenge: Predict seizures 1h before they occur Data Ten minutes intracranial EEG (iEEG) clips Benefits Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. The metric was area under the ROC curve between the predicted probability and the observed target. The best possible score for perfect predictions is 1. The leader board look as follows: Leaderboard: 0.80701 0.79898 0.79652 The winning solution is described in the next chapter "],
["winning-solution-1st.html", "12.1 Winning solution (1st)", " 12.1 Winning solution (1st) The first placed team was a two man show, they present their solution at Kaggle discussion The team consisted of: Four ML experts Private team The team members build a total of 11 models which were blended by using an average of ranked predictions of each individual model. The weight of all models was 1. Models: 11 models in total Each weighted 1 12.1.1 Alex / Gilberto models The two created 4 models which were selected for the final ensemble 12.1.1.1 Pre-processing For all models of Alex and Gilberto the pre-processing was the same. The code can be found at GitHub Pre-processing: Segmentation of 10min segments into non-overlapping 30 20s segments No filtering 12.1.1.2 Software The team used Python and several libraries Software: Python scikit-learn pyRiemann xgboost mne-python pandas pyyaml 12.1.1.3 Model 1 The feature generation code is given at GitHub Model 1 used XGB algorithm and 96 features 96 features: normalized log power 6 different frequency band (0.1 - 4 ; 4- 8 ; 8 - 15 ; 15 - 30 ; 30 - 90 ; 90 - 170 Hz) for each channel Power spectral density Welch’s method (window of 512 sample, 25% overlap) averaged in each band normalized by the total power taking logarithm. 12.1.1.4 Model 2 Model 2 used XGB algorithm and 336 features 336 features: relative log power as described above -with the addition of various measures -mean - min - max - variance - 90th - 10th percentiles) auto regressive error coefficient (order 5) fractal dimension Petrosian Higuchi Hurst exponent 12.1.1.5 Model 3 Model 3 used XGB algorithm and 576 features Each of the autocorrelation matrices were projected into their respective riemannian tangent space (see (Barachant et al. 2013), this operation can be seen as a kernel operation that unfold the natural structure of symmetric and positive define matrices) and vectorized to produce a single feature vector of 36 item. 576 features: auto-correlation matrix projected into their respective riemannian tangent space kernel operation that unfold the natural structure of symmetric and positive define matrices 12.1.1.6 Model 4 Model 4 used XGB algorithm and 336 features This feature set is composed by cross-frequency coherence (in the same 6 sub-band as in the relative log power features) of each channels, i.e. the estimation of coherence is achieved between pairs of frequency of the same channel instead to be between pairs of channels for each frequency band. This produce set of 6x6 coherence matrices, that are then projected in their tangent space and vectorized. 336 features: cross-frequency coherence projected in their tangent space and vectorized 12.1.2 Feng models Feng created 4 models which were selected for the final ensemble. Total training time (including feature extraction) is estimated to less than 6 hours for these 4 models on my 8 GB RAM MacBook Pro. 12.1.2.1 Pre-processing The pre-processing was the same for all models Pre-processing: Butterworth filter (5th order with 0.1-180 HZ cutoff ) segmentation of 10min -non-overlapping 30s windows 12.1.2.2 Features Two different sets of features were produced and used in different combinations for the models. The script to generate the features can be found at GitHub The parameters of the feature generation is organized in the json file kaggle_SETTINGS.json Feature set 1: bands: (0.1–4 Hz), theta (4–8 Hz), alpha (8–12 Hz), beta (12–30 Hz), low gamma (30–70 Hz) and high gamma (70–180Hz) standard deviation average spectral power Feature set 2: correlation time domain frequency domain upper triangle values of correlation matrices eigenvalues 12.1.2.3 Models The models used different algorithms and either feature set 1 or both feature sets Model 1: XGB with feature 1 Model 2: KNN with feature1 Model 3: KNN with feature1+feature2 Model 4: Logistic Regression with L2 penalty with feature1+feature2 12.1.3 Andriy models Andriy created 3 models which were selected for the final ensemble 12.1.3.1 Pre-processing For all models of Andriy the pre-processing was the same Pre-processing: demeaning the EEG signal filtering of the EEG signal between 0.5 and 128 Hz with a notch filter set at 60Hz downsampling to 256 Hz segmentation of the 10 minutes segment non-overlapping 30 seconds segment. 12.1.3.2 Features Andriy created 1965 features from which he choose by computing the feature importance using an XGB classifier. The univarant features have been previously used in several EEG applications, including seizure detection in newborns and adults (Temko, Thomas, Marnane, Lightbody, and Boylan 2011a) and (Temko, Thomas, Marnane, Lightbody, and Boylan 2011b) per-channel feature (univariate): - 111 feature per channel =&gt; 11*16 = 1776 - peak frequency of spectrum - spectral edge frequency (80%, 90%, 95%) - fine spectral log-filterbank energies in 2Hz width sub-bands (0-2Hz, 1-3Hz, …30-32Hz) - coarse log filterbank energies in delta, theta, alpha, beta, gamma frequency bands - normalised FBE in those sub-bands - wavelet energy - curve length - Number of maxima and minima - RMS amplitude - Hjorth parameters - Zero crossings (raw epoch, Δ, ΔΔ) - Skewness - Kurtosis - Nonlinear energy - Variance (Δ, ΔΔ) - Mean frequency - band-width - Shannon entropy - Singular value decomposition entropy - Fisher information - Spectral entropy - Autoregressive modelling error (model order 1-9) These multivariate were extracted for the five conventional EEG sub-bands (delta, theta, alpha, beta, gamma) for 6 different montages (horizontal, vertical, diagonal, etc cross-channel features (multivariate): 180 features lag of maximum cross correlation correlation brain asymmetry index brain synchrony index coherence frequency of maximum coherence. 12.1.3.3 Models Out of the 1965 features listed above the first model computed the feature importance which was then used to select features for model 2 and 3 Model 1: All features were used in a bagged XGB classifier (XGB). Model 2: Linear SVM was trained with top 300 features (SVM) Model 3: GLM was trained with top 200 features (Glmnet) 12.1.4 Code on GitHub A detailed explanation of solution and code is given at GitHub 12.1.4.1 Alex / Gilberto code The code of Alex / Gilberto is analyzed below 12.1.4.1.1 Pre-processing For all models of Alex and Gilberto the pre-processing was the same. The code can be found at GitHub 12.1.4.1.2 Feature generation The feature generation code is given at GitHub 12.1.4.1.3 Models They use the XGB algorithm, the XGB hyperparameters are set in .yml files as follows Yaml file: output: Alex_Gilberto_autocorrmat_TS_XGB datasets: - autocorrmat n_jobs: 1 safe_old: True imports: models: - CoherenceToTangent xgboost: - XGBClassifier sklearn.ensemble: - BaggingClassifier model: - CoherenceToTangent: tsupdate: False metric: &#39;&quot;identity&quot;&#39; n_jobs: 8 - BaggingClassifier: max_samples: 0.99 max_features: 0.99 random_state: 666 n_estimators: 4 base_estimator: XGBClassifier(n_estimators=500, learning_rate=0.01, max_depth=4, subsample=0.50, colsample_bytree=0.50, colsample_bylevel=1.00, min_child_weight=2, seed=42) The models were than called within a shell script: for entry in &quot;models&quot;/*.yml do echo config file &quot;$entry&quot; python generate_submission.py -c &quot;$entry&quot; -p done 12.1.4.2 Feng code The code of Feng is analyzed below 12.1.4.2.1 Pre-processing The scripts for pre-processing are given at GitHub 12.1.4.2.2 Features The script to generate the features can be found at GitHub The parameters of the feature generation is organized in the json file kaggle_SETTINGS.json { &quot;path&quot;:{ &quot;root&quot; : &quot;../&quot;, &quot;raw_data_path&quot; : &quot;../data&quot;, &quot;processed_data_path&quot; :&quot;./postprocessedfile&quot;, &quot;submission_path&quot; : &quot;../submissions&quot; }, &quot;preprocessor&quot;:{ &quot;highcut&quot; : 180, &quot;lowcut&quot; : 0.1, &quot;nfreq_bands&quot;: 6, &quot;win_length_sec&quot;: 30, &quot;features&quot;: &quot;meanlog_std&quot;, &quot;stride_sec&quot;: 30 } } 12.1.4.2.3 Models The code for the GLM model: def train(subject, data_path, plot=False): d = load_train_data_lasso(data_path, subject) x, y = d[&#39;x&#39;], d[&#39;y&#39;] print &#39;n_preictal&#39;, np.sum(y) print &#39;n_inetrictal&#39;, np.sum(1-y) n_channels = x.shape[1] n_fbins = x.shape[2] x, y = reshape_data(x, y) x[np.isneginf(x)] = 0 data_scaler = StandardScaler() x = data_scaler.fit_transform(x) ## Normalizaiton logreg = linear_model.LogisticRegression(penalty=&#39;l2&#39;,C=0.6) logreg.fit(x, y) return logreg, data_scaler The code for the KNN model: def train(subject,data_path): d=load_train_data_knn(data_path,subject) x,y=reshape_data(d[&#39;x&#39;],d[&#39;y&#39;]) x[np.isneginf(x)] = 0 x[np.isnan(x)]=0 data_scaler = StandardScaler() x = data_scaler.fit_transform(x) clf = KNeighborsClassifier(n_neighbors=40, weights=&#39;distance&#39;,metric=&#39;manhattan&#39;, n_jobs=-1) clf.fit(x, y) return clf The code for the XGB model: params = { &quot;objective&quot;: &quot;binary:logistic&quot;, &quot;booster&quot; : &quot;gbtree&quot;, &quot;eval_metric&quot;: &quot;auc&quot;, &quot;eta&quot;: 0.22,##0.22 &quot;max_depth&quot;: 3, &quot;subsample&quot;: 0.80, &quot;colsample_bytree&quot;: 0.78, &quot;silent&quot;: 1, } def train(subject,data_path,params): d=load_train_data_xgb(data_path,subject) x,y=reshape_data(d[&#39;x&#39;],d[&#39;y&#39;]) dtrain=xgb.DMatrix(x,y) gbm=xgb.train(params,dtrain,num_boost_round=500,verbose_eval=20) return gbm 12.1.4.3 Andriy code The code of Andriy is analyzed below 12.1.4.3.1 Pre-processing Pre-processing is done in Matlab scripts on GitHub 12.1.4.3.2 Feature generation The feature generation is also done in 4 Matlab scripts at GitHub FE_main_AR.m FE_main_CONN.m FE_main_CSP_AR.m FE_main_F.m 12.1.4.3.3 Models The files for the models are: -GLM model - mod_glmnet_5_3.R Creates SVM model and submission -SVM model -mod_svm_5_7.R -XGB model - mod_xgb_7_5.R Code for the XGB model param &lt;- list( objective = &quot;binary:logistic&quot;, booster = &quot;gbtree&quot;, eval_metric = &quot;auc&quot;, eta = 0.3, max_depth = 3, subsample = 0.8, colsample_bytree = 1, num_parallel_tree = 2 ) cat(&#39;model1...&#39;) set.seed(1234) model1 &lt;- xgb.train( params = param, data = dtrain, nrounds = 1000) importance &lt;- xgb.importance(model = model1) References "],
["solution4th-place.html", "12.2 Solution(4th place)", " 12.2 Solution(4th place) The 4th placed team was a one man show, he presented his solution at Kaggle discussion The solution got a AUC of 0.79457 compared to the winning solution of 0.80701 12.2.1 Pre-processing Pre-processing: - Splitting into 75s windows - Resampled to 100Hz 12.2.2 Features Features: Divide frequency spectrum 50 bands 0.67 - 46.67Hz take power of bands correlation matrix between channels eingenvalues of correlation matrix Divide frequency spectrum 5 bands delta (0.1-4Hz), theta (4-8Hz), alpha (8-12Hz), beta (12-30Hz), low-gamma (30-50Hz) take power of bands entropy of bands original signal correlation matrix eigenvalues square all above features as additionals features 12.2.3 Model A single XGB model was used 12.2.4 GitHub code The code is hosted on GitHub "],
["bosch-production-line-performance.html", "Chapter 13 Bosch Production Line Performance", " Chapter 13 Bosch Production Line Performance This competition was hosted in 2016 by Bosch at https://www.kaggle.com/c/bosch-production-line-performance and was subtitled “Reduce manufacturing failures” The price money was $30,000, the competition ended 11.11.2016 and 1373 teams had submitted a solution. Challenge: Predict internal failures Data anonymized measurements tests Benefit reduce manufacturing failures The metric was Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by: \\[MCC = score=\\frac{\\left(TP*TN\\right) - \\left(FP*FN\\right) }{\\sqrt{\\left(TP*FP\\right) \\left(TP*FN\\right) \\left(TN*FP\\right) \\left(TN*FN\\right)}} \\] where: TP: number of true postive FP: number of false positive TN: number of true negative FN: number of false negative The best possible score for perfect predictions is 1. The leader board look as follows: 0.52401 0.51847 0.51621 The winning solution is described in the next chapter "],
["st-place-solution.html", "13.1 1st place solution", " 13.1 1st place solution The first placed team was a two man show, they present their solution at Kaggle discussion The team consisted of Two ML experts Private team 13.1.1 Data exploration Two weeks were invested to explore the data regarding: Statistics Correlation 13.1.2 Hand crafted features The team created their own features Time features are: StartStationTimes StartTime, EndTime, Duration StationTimeDiff Start/End part of week (mod 1680) Number of records in next/last 2.5h, 24h, 168h for each station Number of records in the same time (6 mins) MeanTimeDiff since last 1/5/10 failure(s) MeanTimeDiff till next 1/5/10 failure(s) Numeric features are: Raw numeric features (most of the time we used the raw numeric features or simple subsets based on xgb feature importance) Z-scaled features for each week Count encoding for each value Feature combinations (f1 + - * f2) 13.1.3 Hardware Since there was no usage of NN the hardware cold be rather modest Desktop machine (16GB RAM) "],
["rd-place-solution-tbd.html", "13.2 3rd place solution TBD", " 13.2 3rd place solution TBD discussion not very elaborated On Kaggle discussion "],
["th-place-solution-with-github.html", "13.3 8th place solution with GitHub", " 13.3 8th place solution with GitHub The eighth placed team was a team of eighth, they present their solution at Kaggle discussion The team consisted of Eigth people Private group Organised via the net 13.3.1 Overall architecture A variety of model were combined LightGBM (gbm) xgboost (xgb) Random Forest (rf) Neural Networks (didn’t get picked up on level 2, so they were removed) 13.3.2 Input data sets The team created different data sets and used them with different models Level 1 data set: Data set 1 (0.477 gbm): order, raw numeric, date, categorical Data set 2 (0.482 gbm, 0.477 xgb, 0.473 rf): order, path, raw numeric, date Data set 3 (0.479 gbm, 0.473 xgb): order, path, numeric, date, refined categorical Data set 4 (0.469 xgb, 0.442 rf): has features sorted by numeric values + date features + path, unsupervised nearest neighbors (L1 = Manhattan / L2 = Euclidean distances) per label Data set 5 (0.43 xgb): path, unsupervised nearest neighbors The model was two staged, the second stage was as given below Level 2 data set: Level 1 predictions (we had 12 predictions from level 1) Data set 5 Duplicate feature (count and position) 13.3.3 Ensembling Often a better performance can be achieved when ensembling several model together, good practice is it to use models which a dissimilar because the variance helps to improve the overall performance. 30% weighted xgboost gbtree (~0.488 CV) 70% weighted Random Forest (~0.485 CV) 13.3.4 Features 13.3.4.1 Features used Features were created using several methods Maximum Minimum Kurtosis Lead Lag One-hot encoded 13.3.5 Validation method The validation method used was 5-fold cross validation 13.3.6 Software The team used a variety of programming languages and tools Programming language R Python Tools LightGBM through Laurae package xgboost Random Forest scikit-learn H2O Random Forest Keras Neural Networks Markdown Rmarkdown RStudio for R, Spyder for Python 13.3.7 Code on GitHub A detailed explanation of the code is given on GitHub The scripts for: Pre-processing Feature engineering Modeling scripts Hyperparameter optimization using HyperOpt 13.3.7.1 Level 1 model scripts Lets look into some of the model scripts 13.3.7.1.1 GBM Model temp_model &lt;- lgbm.cv(y_train = label, x_train = train, x_test = test, data_has_label = TRUE, NA_value = &quot;nan&quot;, lgbm_path = my_lgbm_is_at, workingdir = my_script_is_using, files_exist = TRUE, save_binary = FALSE, validation = TRUE, folds = folds, predictions = TRUE, importance = TRUE, full_quiet = FALSE, verbose = FALSE, num_threads = threads, # The number of threads to run for LightGBM. application = &quot;binary&quot;, learning_rate = eta, # The shrinkage rate applied to each iteration num_iterations = 5000, # The number of boosting iterations early_stopping_rounds = 700, # The number of boosting iterations whose validation metric is lower than the best is required for LightGBM to automatically stop num_leaves = leaves, # The number of leaves in one tree min_data_in_leaf = min_sample, # Minimum number of data in one leaf min_sum_hessian_in_leaf = min_hess, # Minimum sum of hessians in one leaf to allow a split max_bin = 255, # The maximum number of bins created per feature feature_fraction = colsample, # Column subsampling percentage. For instance, 0.5 means selecting 50% of features randomly for each iteration bagging_fraction = subsample, # Row subsampling percentage. For instance, 0.5 means selecting 50% of rows randomly for each iteration. bagging_freq = sampling_freq, # The frequency of row subsampling is_unbalance = FALSE, # For binary classification, setting this to TRUE might be useful when the training data is unbalanced metric = &quot;auc&quot;, is_training_metric = TRUE, # Whether to report the training metric in addition to the validation metric is_sparse = FALSE) # Whether sparse optimization is enabled 13.3.7.1.2 XGBoost model temp_model &lt;- xgb.train(data = dtrain, nthread = 12, nrounds = floor(best_iter * 1.1), # max number of boosting iterations. eta = 0.05, # control the learning rate: scale the contribution of each tree by a factor of 0 &lt; eta &lt; 1 when it is added to the current approximation depth = 7, # maximum depth of a tree #gamma = 20, # minimum loss reduction required to make a further partition on a leaf node of the tree. subsample = 0.9, # Setting it to 0.5 means that xgboost randomly collected half of the data instances to grow trees colsample_bytree = 0.7, # subsample ratio of columns when constructing each tree min_child_weight = 50, # minimum sum of instance weight (hessian) needed in a child booster = &quot;gbtree&quot;, # which booster to use, can be gbtree or gblinear #feval = mcc_eval_nofail, eval_metric = &quot;auc&quot;, maximize = TRUE, objective = &quot;binary:logistic&quot;, verbose = TRUE, prediction = TRUE, watchlist = list(test = dtrain)) 13.3.7.2 Level 2 model scripts 13.3.7.2.1 70% weighted Random Forest (~0.485 CV) First read in the results of level 1 models which are now the features for the level 2 model train &lt;- read_feather(&quot;Shubin/retrain_material/train.feather&quot;) test &lt;- read_feather(&quot;Shubin/retrain_material/test.feather&quot;) train[, &quot;xgb_jay_joost_v2&quot;] &lt;- fread(&quot;Laurae/20161110_xgb_jayjoost_fix2/aaa_stacker_preds_train_headerY_scale.csv&quot;)$x test[, &quot;xgb_jay_joost_v2&quot;] &lt;- fread(&quot;Laurae/20161110_xgb_jayjoost_fix2/aaa_stacker_preds_test_headerY_scale.csv&quot;)$x train[, &quot;gbm_jay_joost_v2&quot;] &lt;- fread(&quot;Laurae/20161111_lgbm_jayjoost/aaa_stacker_preds_train_headerY_scale.csv&quot;)$x test[, &quot;gbm_jay_joost_v2&quot;] &lt;- fread(&quot;Laurae/20161111_lgbm_jayjoost/aaa_stacker_preds_test_headerY_scale.csv&quot;)$x train[, &quot;gbm_jay&quot;] &lt;- fread(&quot;Laurae/20161111_lgbm_jay/aaa_stacker_preds_train_headerY_scale.csv&quot;)$x test[, &quot;gbm_jay&quot;] &lt;- fread(&quot;Laurae/20161111_lgbm_jay/aaa_stacker_preds_test_headerY_scale.csv&quot;)$x train[, &quot;gbm_mike&quot;] &lt;- fread(&quot;Laurae/20161110_lgbm_mike/aaa_stacker_preds_train_headerY_scale.csv&quot;)$x test[, &quot;gbm_mike&quot;] &lt;- fread(&quot;Laurae/20161110_lgbm_mike/aaa_stacker_preds_test_headerY_scale.csv&quot;)$x train[, &quot;xgb_mike&quot;] &lt;- fread(&quot;Laurae/20161110_xgb_mike/aaa_stacker_preds_train_headerY_scale.csv&quot;)$x test[, &quot;xgb_mike&quot;] &lt;- fread(&quot;Laurae/20161110_xgb_mike/aaa_stacker_preds_test_headerY_scale.csv&quot;)$x then train the level 2 model temp_model &lt;- h2o.randomForest(x = 1:12, y = &quot;Response&quot;, training_frame = my_train[[i]], ntrees = 200, # Number of trees max_depth = 12, # Maximum tree depth min_rows = 20, # Fewest allowed (weighted) observations in a leaf seed = 11111) 13.3.7.2.2 Hyperparameter optimization using HyperOpt The models have been implemented in R, the hyperparameter optimizsation is implemented in Python. Define parameters to be optimized # Random Forest Params params = {&#39;n_estimators&#39;: 100} params[&#39;random_state&#39;] = 100 params[&#39;max_features&#39;] = hp.choice(&#39;max_features&#39;, range(10, 199)) params[&#39;max_depth&#39;] = hp.choice(&#39;max_depth&#39;, range(7,30)) params[&#39;verbose&#39;] = 10 params[&#39;n_jobs&#39;] = -1 Run optimizer from the library Hyperopt # Hyperopt trials = Trials() counter = 0 best = fmin(score_rf, params, algo=tpe.suggest, # search algorithm max_evals=200, trials=trials) choosing the trials option gives back a dictionary with trials.trials - a list of dictionaries representing everything about the search trials.results - a list of dictionaries returned by ‘objective’ during the search trials.losses() - a list of losses (float for each ‘ok’ trial) trials.statuses() - a list of status strings "],
["corporación-favorita-grocery-sales-forecasting.html", "Chapter 14 Corporación Favorita Grocery Sales Forecasting", " Chapter 14 Corporación Favorita Grocery Sales Forecasting https://www.kaggle.com/c/favorita-grocery-sales-forecasting "],
["st-place-solution-1.html", "14.1 1st place solution", " 14.1 1st place solution The first place solution is described at Kaggle discussion "],
["th-place-solution-overview.html", "14.2 4th-Place Solution Overview", " 14.2 4th-Place Solution Overview On Kaggle discussion A similar code used in a different competition was shared on GitHub "],
["th-place-solution.html", "14.3 5th Place Solution", " 14.3 5th Place Solution On Kaggle discussion The code was shared on GitHub "],
["severstal-steel-defect-detection.html", "Chapter 15 Severstal: Steel Defect Detection", " Chapter 15 Severstal: Steel Defect Detection https://www.kaggle.com/c/severstal-steel-defect-detection "],
["lyft-3d-object-detection-for-autonomous-vehicles.html", "Chapter 16 Lyft 3D Object Detection for Autonomous Vehicles", " Chapter 16 Lyft 3D Object Detection for Autonomous Vehicles https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles "],
["rd-place-solution.html", "16.1 3rd place solution", " 16.1 3rd place solution https://www.kaggle.com/c/3d-object-detection-for-autonomous-vehicles/discussion/117269#latest-679717 "],
["aptos-2019-blindness-detection.html", "Chapter 17 APTOS 2019 Blindness Detection", " Chapter 17 APTOS 2019 Blindness Detection https://www.kaggle.com/c/aptos2019-blindness-detection "],
["st-place-solution-summary.html", "17.1 1st place solution summary", " 17.1 1st place solution summary https://www.kaggle.com/c/aptos2019-blindness-detection/discussion/108065#latest-673088 "],
["predicting-molecular-properties.html", "Chapter 18 Predicting Molecular Properties", " Chapter 18 Predicting Molecular Properties The competition was hosted by a group of UK universities as a featured competition at https://www.kaggle.com/c/champs-scalar-coupling and was subtitled “Can you measure the magnetic interactions between a pair of atoms?” Hosts: CHemistry and Mathematics in Phase Space (CHAMPS) University of Bristol Cardiff University Imperial College University of Leeds The price money was $30,000, the competition ended 21.08.2019 and 2,749 teams had submitted a solution. Challenge Develop algorithm that can predict the magnetic interaction between two atoms in a molecule Data dipole moments magnetic shielding tensor mulliken charge potential energy Benefit designing molecules to carry out specific cellular tasks designing better drug molecules The metric was Log of the Mean Absolute Error (MAE), calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type. \\[Log MAE = score=\\frac{1}{T} \\sum_{t=1}^{T} \\log \\left(\\frac{1}{n_{t}} \\sum_{i=1}^{n_{t}}\\left|y_{i}-\\hat{y}_{i}\\right|\\right)\\] Where: \\(T\\) is the number of scalar coupling types \\(n_t\\): is the number of observations of type \\(t\\) \\(y_i\\): is the actual scalar coupling constant for the observation \\(\\hat{y}_{i}\\) is the predicted scalar coupling constant for the observation The best possible score for perfect predictions is approximately -20.7232. The leader board look as follows: -3.23968 -3.22349 -3.19498 The winning solution is described in the next chapter "],
["solution-hybrid.html", "18.1 #1 Solution - hybrid", " 18.1 #1 Solution - hybrid The winning team was from Bosch Research, they present their solution at Kaggle discussion The team consisted of Two Bosch research groups Bosch Corporate Research Bosch Center for AI (BCAI, Pittsburgh) Domain experts ML experts 18.1.1 Overall architecture The winning team used a neural network Wrote NN model from scratch Model processes an entire molecule at once simultaneously making a prediction for each of the scalar couplings in the molecule 18.1.1 Input features and embeddings Embeddings Plus two scalar constants 18.1.2 Ensembling Often a better performance can be achieved when ensembling several model together, good practice is it to use models which a dissimilar because the variance helps to improve the overall performance. Trained 13 models iterations and versions of same basic structure Best single model: -3.08 Straight median across predictions: ~-3.22 More involved blending: -3.24520 18.1.3 Hardware The variety of models were trained on different machines, each running a Linux OS: 5 machines had 4 GPUs, each a NVIDIA GeForce RTX 2080 Ti 2 machines had 1 GPU NVIDIA Tesla V100 with 32 GB memory 6 machines had 1 GPU NVIDIA Tesla V100 with 16 GB memory 18.1.4 Software The team did not use any of the popular ML frameworks but coded their models from scratch Python 3.5+ PyTorch CUDA 10.1 NVIDIA APEX (Only available through the repo at this phase) 18.1.5 Code on GitHub A detailed explanation of the principle setup of the code for pre-processing and for the models is given at https://github.com/boschresearch/BCAI_kaggle_CHAMPS using the median of all 13 models to determine which 9 models seemed best, then taking the mean of a few different medians of the different model predictions↩︎ "],
["solution-quantum-uncertainty.html", "18.2 #2 solution 🤖 Quantum Uncertainty 🤖", " 18.2 #2 solution 🤖 Quantum Uncertainty 🤖 The second placed team was a two man show, they present their solution at Kaggle discussion The team consisted of No domain experts ML experts Private team 18.2.1 Overall architecture Since the team had no domain knowledge and “obviously we were at a disadvantage if we tried to become quantum experts in 1 month” they needed the model to build the features. Deep learning Dimension 512 to 2048 Layers 6 to 24 Parameters from ~12M to ~100M Letting the model build the features 18.2.2 Input features and embeddings Three input arrays of dimension 29 (maximum number of atoms) x,y,z position of each atom atom type index (C=0, H=1, etc…) j-coupling type index (1JHC=0,’2JHH=1,etc.) No manually engineered features 18.2.3 Data augmentation Data augmentation helps to increase the data basis by producing new samples. Depending on how the augmentation is done it can also be a way of making the model more robust to disturbance, e.g. createing artificially shadow in images makes model less susceptible to lightning conditions Rotations (though not used in final model) J-coupling symmetriy as described here 18.2.4 Ensembling Often a better performance can be achieved when ensembling several model together, good practice is it to use models which a dissimilar because the variance helps to improve the overall performance. Trained 14 models iterations and versions of same basic structure Best single model: -3.16234 18.2.5 Hardware On permise as well as rented hardware was used by the team. 3 x 2080 Ti + 128 Gb RAM + 16c32t processor 2 x 1080 Ti + 64 Gb RAM + 8c16t processor Rented 8+ 2080 Ti + 64 Gb RAM + 16c32t processor (multiple machines rented as needed) 18.2.6 Software The team did not use any of the popular ML frameworks but coded their models from scratch PyTorch FastAi 18.2.7 Code on GitHub The code is shared at https://github.com/antorsae/champs-scalar-coupling. The jupyter notebook using FastAi is at https://github.com/antorsae/champs-scalar-coupling/blob/master/atom-transfomer.ipynb In the “Model” section the transformer is defined as follows: class AtomTransformer(Module): def __init__(self,n_layers,n_heads,d_model,embed_p:float=0,final_p:float=0,d_head=None,deep_decoder=False, dense_out=False, **kwargs): self.d_model = d_model d_head = ifnone(d_head, d_model//n_heads) self.transformer = Transformer(n_layers=n_layers,n_heads=n_heads,d_model=d_model,d_head=d_head, final_p=final_p,dense_out=dense_out,**kwargs) channels_out = d_model*n_layers if dense_out else d_model channels_out_scalar = channels_out + n_types + 1 if deep_decoder: sl = [int(channels_out_scalar/(2**d)) for d in range(int(math.ceil(np.log2(channels_out_scalar/4)-1)))] self.scalar = nn.Sequential(*(list(itertools.chain.from_iterable( [[nn.Conv1d(sl[i],sl[i+1],1),nn.ReLU(),nn.BatchNorm1d(sl[i+1])] for i in range(len(sl)-1)])) + [nn.Conv1d(sl[-1], 4, 1)])) else: self.scalar = nn.Conv1d(channels_out_scalar, 4, 1) self.magnetic = nn.Conv1d(channels_out, 9, 1) self.dipole = nn.Linear(channels_out, 3) self.potential = nn.Linear(channels_out, 1) self.pool = nn.AdaptiveAvgPool1d(1) n_atom_embedding = d_model//2 n_type_embedding = d_model - n_atom_embedding - 3 #- 1 - 1 self.type_embedding = nn.Embedding(len(types)+1,n_type_embedding) self.atom_embedding = nn.Embedding(len(atoms)+1,n_atom_embedding) self.drop_type, self.drop_atom = nn.Dropout(embed_p), nn.Dropout(embed_p) def forward(self,xyz,type,ext,atom,mulliken,coulomb,mask_atoms,n_atoms): bs, _, n_pts = xyz.shape t = self.drop_type(self.type_embedding((type+1).squeeze(1))) a = self.drop_atom(self.atom_embedding((atom+1).squeeze(1))) # x = torch.cat([xyz, mulliken, ext, mask_atoms.type_as(xyz)], dim=1) #x = torch.cat([xyz, mask_atoms.type_as(xyz)], dim=1) x = xyz x = torch.cat([x.transpose(1,2), t, a], dim=-1) * math.sqrt(self.d_model) # B,N(29),d_model mask = (coulomb == 0).unsqueeze(1) x = self.transformer(x, mask).transpose(1,2).contiguous() t_one_hot = torch.zeros(bs,n_types+1,n_pts,device=type.device,dtype=x.dtype).scatter_(1,type+1, 1.) scalar = self.scalar(torch.cat([x, t_one_hot], dim=1)) magnetic = self.magnetic(x) px = self.pool(x).squeeze(-1) dipole = self.dipole(px) potential = self.potential(px) return type,ext,scalar,magnetic,dipole,potential def reset(self): pass The model is instantiated net, learner = None,None gc.collect() torch.cuda.empty_cache() n_layers=6 n_heads=16 d_model=1024 d_inner=2048*2 deep_decoder = False dense_out = False net = AtomTransformer(n_layers=n_layers, n_heads=n_heads,d_model=d_model,d_inner=d_inner, resid_p=0., attn_p=0., ff_p=0., embed_p=0, final_p=0., deep_decoder=deep_decoder, dense_out=dense_out) learner = Learner(data,net, loss_func=LMAEMaskedLoss(),) learner.callbacks.extend([ SaveModelCallback(learner, monitor=&#39;👉🏻LMAE👈🏻&#39;, mode=&#39;min&#39;), LMAEMetric(learner)]) "],
["local-examples.html", "Chapter 19 Local examples ", " Chapter 19 Local examples "],
["university-suttgart-indoor-ortung-mit-mobilfunk.html", "19.1 University Suttgart: Indoor-Ortung mit Mobilfunk", " 19.1 University Suttgart: Indoor-Ortung mit Mobilfunk University Stuttgart Institute of Telecommunications Leveraging 5G Infrastructure for a Robust Positioning System Using neural networks More information on the work can be found in (Widmaier et al. 2019) References "],
["bionic-learning-network.html", "19.2 Bionic Learning Network", " 19.2 Bionic Learning Network Inspiration for factory and process automation IT-Designers Gruppe http://www.it-designers-gruppe.de/unternehmens-gruppe/it-designers-gmbh/ "],
["RealWorld.html", "Chapter 20 Real world example", " Chapter 20 Real world example This ESA funded project was conducted in 2018/19. The question asked was: How can the system be made faster and more reliable? "],
["subject-of-the-project.html", "20.1 Subject of the project", " 20.1 Subject of the project Depending from where you were looking: Looking from the perspective of machine learning expert "],
["project-phases.html", "20.2 Project phases", " 20.2 Project phases The main project phases are: After data gathering iteration is trump Figure from (Kuhn and Johnson 2018) (Image Credit: Owlsmcgee [Public domain] ) EDA =&gt; exploratory data analysis source http://www.feat.engineering/intro-intro.html#the-model-versus-the-modeling-process] Exploratory data analysis Find correlations or mutial depence Quantiative analysis Check distribution Long tail =&gt; log of variable Feature engineering21 Create and select meaningful features Model fit Selecting a few suited models Model tuning Vary model hyperpparameters 20.2.1 Feature engineering Variables that go into model are called: Predictors Features Independent variables Quantity being modeled called: Prediction Outcome Response Dependent variable From input to output \\[outcome = f(features) = f(X_1, X_2, \\dots, Xp) = f(X)\\] \\[\\hat{Y} = \\hat{f}(X)\\] References "],
["algorithm-selection.html", "20.3 Algorithm selection", " 20.3 Algorithm selection The following algorithms were meant to be investigated following the rule to start with the least complex one. This is even more important since the algorithm was to be run on a satellite which where computing power is more limited than on earth Start with simple model 20.3.1 Logistic regression Logistic regression is the algorithm with the lowest computational complexity and therefore it was the algorithm with which the investigation for the suitable model would start Lowest computational complexity Start algorithm to determine suitable algorithm Details of algorithm are given in chapter 7.2 \\[ logistic(\\eta) = \\frac{1}{1+exp^{-\\eta}}\\] \\[P(Y = 1 \\vert X_i = x_i) = \\frac{1}{1+exp^{-(\\beta_0 + \\beta_1X_1+ \\dots \\beta_n X_n)}}\\] where: \\(\\beta_n\\) are the coeffcients we are searching \\(X_n\\) are the features 20.3.2 Tree based TBD Two dominant concepts are used for tree based algorithms: Details on the algorithms are given at Random forest in chapter 7.3.3 Gradient boosted trees in chapter 7.3.4 20.3.3 Support Vector Machine (SVM) TBD Support vector machine in chapter 7.4 \\[maximize \\(M\\) \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\) subject to \\(\\sum_{j=1}^{p} \\beta_{j}^{2}=1\\) \\(y_{i}\\left(\\beta_{0}+\\beta_{1} x_{i 1}+\\ldots+\\beta_{p} x_{i p}\\right) \\geq M\\) for all \\(i=1, \\dots, N\\)\\] Figure from Alisneaky, svg version by User:Zirguezi [CC BY-SA (https://creativecommons.org/licenses/by-sa/4.0)] \\(〖 𝑅〗_𝑡=∑_(𝑖=𝑡)^∞▒〖γ^𝑖 𝑟_𝑖 〗=γ^𝑡 𝑟_𝑡+γ^(𝑡+1) 𝑟_(𝑡+1)…+γ^(𝑡+𝑛) 𝑟_(𝑡+𝑛)+ …\\) "],
["performance-measurement.html", "20.4 Performance measurement", " 20.4 Performance measurement The performance for a classification task is measured with a confusion matrix For more serios scenarios the false predictions can have severe impact: FP: False prediction Healthy person is unesseary troubled FN: False negative Ill person does not get necessary treatment Based on the four elements of the confusion matrix various metrics are defined, for details check https://en.wikipedia.org/wiki/Confusion_matrix 20.4.1 Sensitivity and specificity Two metrics which are derived from the confusion matrix are: Sensitivity is the proportion of cats which have been identified as cats, or the proportion of people with the illness that have been identified as being ill. It is therefore also called probability of detection Sensitivity =&gt; P(cat predicted | cat given) \\[ \\ Sensitivity = \\frac {\\text{Sample with cat and predicted cat}} {\\text{Samples having cat}} = \\frac{TP}{TP+FN} \\] Specificity is the proportion of non-cats which have been identified as non-cats, or the proportion of healthy people which have been identified as healthy. Specificity =&gt; P(non-cat predicted | non-cat observed) \\[ \\ Specificity = \\frac {\\text{Sample with non-cat and predicted as non-cat}} {\\text{Samples with non-cat}} = \\frac{TN}{TN+FP} \\] 20.4.2 Receiver operating characteristic (ROC) The result of a classification with two classes (binary classification) is given as a percentage value of how sure the algorithm is that the sample belongs to a class. Depending on the the overall project target the threshold upon which the class is rated as identified is set. If a false positive is to be avoided than the threshold for classifying a positive is set high Used to set the probability threshold of detection Visual representation of confusion matrix Includes for various probability thresholds Sensitivity Specificity AUC =&gt; area under curve The higher the better 0 &lt; AUC &lt; 1 "],
["confusion-matrix-and-roc-for-pulse.html", "20.5 Confusion matrix and ROC for pulse", " 20.5 Confusion matrix and ROC for pulse 20.5.1 R Plots "],
["create-augmented-labeled-data.html", "20.6 Create augmented labeled data", " 20.6 Create augmented labeled data How to label data as being positive? Create augmented hits Vary parameters On the left hand side are the pulses which have to be detected amid noise as shown in the right hand side image. Note, the y-axis have the same scaling, i.e. the pulse signal strengths is lower than the noise. 20.6.1 Features of time signals "],
["features-generated.html", "20.7 Features generated", " 20.7 Features generated Sample values of window Dynamic time warp (window) Min(window) Max(window) Median(window) Variance(window) 20.7.1 Analysis of generated features 20.7.2 Dynamic time warp (DTW) for signal "],
["algorithm.html", "20.8 Algorithm", " 20.8 Algorithm Determination which algorithm is best suited depends on Start with simplest algorithm Use simple algorithm for feature engineering Use more complex algorithm if result is unsatisfactory "],
["confusion-matrix-results-logistic-regression-for-measured-data.html", "20.9 Confusion matrix results logistic regression for measured data", " 20.9 Confusion matrix results logistic regression for measured data 20.9.1 ROC results for measured data ROC of logistic regression Perfect separation of two classes No need for more complex algorithm "],
["several-algorithms-results-for-snr-18db.html", "20.10 Several algorithms results for SNR = 18dB", " 20.10 Several algorithms results for SNR = 18dB 20.10.1 ROC results for SNR 18dB ROC of logistic regression Not perfect separation of two classes Need more complex algorithm =&gt; Gradient boosted trees Depending on ROC characteristic choosing the threshold value which leads to the highest available profit is not easy "],
["compare-models-for-snr-18db.html", "20.11 Compare models for SNR = 18dB", " 20.11 Compare models for SNR = 18dB ROC, Sensitivity and Specificity for gradient boosted trees (GBM) and logistic regression (LogReg) and support vector machine (SVM) vs cross validation "],
["optimize-ml-hyper-parameter.html", "20.12 Optimize ML hyper parameter", " 20.12 Optimize ML hyper parameter "],
["CloudBasedMl.html", "Chapter 21 Cloud-based machine learning", " Chapter 21 Cloud-based machine learning Build your own Robust Deep Learning Environment in Minutes https://towardsdatascience.com/build-your-own-robust-deep-learning-environment-in-minutes-354cf140a5a6 Google Colaboratory Pricing information =&gt; For free Since Feb 2020 there is a commercial version available (only in US, checked 10.2.2020) https://colab.research.google.com/signup Faster GPU Longer runtimes More memory Paperspace Gradient Pricing information FloydHub Workspace Pricing information Lambda GPU Cloud Pricing information AWS Deep Learning AMIs Pricing information, select EU (Frankfurt) GCP Deep Learning VM Images Pricing information "],
["KaggleSurvey.html", "Chapter 22 Kaggle survey introduction", " Chapter 22 Kaggle survey introduction Kaggle is a platform for data scientists and machine learning practitioners which allows users to: Kaggle allows users to: find datasets publish datasets exlplore models on web-based data-science environment in Python R SQLite Julia work with other machine learning practitioners on competitions Host competitions "],
["kaggle-survey-details.html", "22.1 Kaggle survey details", " 22.1 Kaggle survey details This is an analysis based on Kaggle survey data, details are at https://www.kaggle.com/c/kaggle-survey-2019. Kaggle is a subsidiary of Google LLC online community of data scientists machine learners with more than 1Mio members. It offers data sets, a no-setup, customizable, Jupyter Notebooks environment, machine learning competitions and access free GPUs and a huge repository of community published data &amp; code. Info on survey The survey received 19,717 usable respondents from 171 countries and territories. If a country or territory received less than 50 respondents, they were grouped and named “Other” for anonymity. The survey was live from October 8th to October 28th 2019. The median response time for those who participated in the survey was approximately 10 minutes. An overview of the world wide participation is given in the map below. The first three countries are Countries of most participants: India USA Brazil All numbers of all countries are given in the interactive table below. To find a specific country, type the name in the search field. Surprising facts: Almost as many participants from Saudi Arabia (50) and Norway (51) Peru (74) higher than Belgium (70) Iran (96) higher than Sweden (92) The word frequency word cloud shows that software engineers and data scientist are heavily involved the field of machine learning Easy histogram plots of all questions can be created in R as shown at https://www.kaggle.com/paultimothymooney/how-to-explore-the-2019-kaggle-survey-data "],
["purpose.html", "22.2 Purpose", " 22.2 Purpose The purpose of the survey analysis is to create insight into which Purpose of survey, get insight into usage of: algorithms tools platforms are used in the field of machine learning. Contrary to public opinion machine learning is not mainly focused on neural networks. "],
["navigation-and-handling.html", "22.3 Navigation and handling", " 22.3 Navigation and handling To navigate between results use arrow keys or click on sidebar entry Further information on handling can be obtained by clicking on the “i” at the left hand side on top of the page "],
["results.html", "Chapter 23 Results", " Chapter 23 Results The results are presented by graphs relating parameters either vs time or vs other parameters. "],
["survey-participants-education-level.html", "23.1 Survey participants education level", " 23.1 Survey participants education level The following plot shows survey participants education level. Very few participants have a non-academic background. By no means a academic background is a pre-requisit to use machine learning, however, two skills are very helpful Helpful skills for ML: Coding experience Statistical knowledge Coding experience speeds up the process to implement the machine learning ideas and concepts. Most effort during a machine learning project will go into Main effort during ML process: Data pre-processing Model tuning The actual implementation of the algorithm is often a matter of 10 - 20 lines of code. Below the neural network definition for a self driving RC model car of the donkey car framework. The neural network is defined using the Keras API which sits on top of Tensorflow, the program is written in Python img_in = Input(shape=input_shape, name=&#39;img_in&#39;) x = img_in x = Convolution2D(24, (5,5), strides=(2,2), activation=&#39;relu&#39;, name=&quot;conv2d_1&quot;)(x) x = Dropout(drop)(x) x = Convolution2D(32, (5,5), strides=(2,2), activation=&#39;relu&#39;, name=&quot;conv2d_2&quot;)(x) x = Dropout(drop)(x) if input_shape[0] &gt; 32 : x = Convolution2D(64, (5,5), strides=(2,2), activation=&#39;relu&#39;, name=&quot;conv2d_3&quot;)(x) else: x = Convolution2D(64, (3,3), strides=(1,1), activation=&#39;relu&#39;, name=&quot;conv2d_3&quot;)(x) if input_shape[0] &gt; 64 : x = Convolution2D(64, (3,3), strides=(2,2), activation=&#39;relu&#39;, name=&quot;conv2d_4&quot;)(x) elif input_shape[0] &gt; 32 : x = Convolution2D(64, (3,3), strides=(1,1), activation=&#39;relu&#39;, name=&quot;conv2d_4&quot;)(x) x = Dropout(drop)(x) x = Convolution2D(64, (3,3), strides=(1,1), activation=&#39;relu&#39;, name=&quot;conv2d_5&quot;)(x) x = Flatten(name=&#39;flattened&#39;)(x) x = Dense(100, activation=&#39;relu&#39;, name=&quot;fc_1&quot;)(x) x = Dropout(drop)(x) x = Dense(50, activation=&#39;relu&#39;, name=&quot;fc_2&quot;)(x) x = Dropout(drop)(x) angle_out = Dense(15, activation=&#39;softmax&#39;, name=&#39;angle_out&#39;)(x) throttle_out = Dense(20, activation=&#39;softmax&#39;, name=&#39;throttle_out&#39;)(x) model = Model(inputs=[img_in], outputs=[angle_out, throttle_out]) "],
["who-uses-which-algorithm.html", "23.2 Who uses which algorithm", " 23.2 Who uses which algorithm There are plenty of machine learning algorithms in use, some have been around for quite some time already, others are quite new. Especially in the field of neural networks there is plenty of research ongoing as can be seen by a search with the keywords “neural network” on the moderated but not peer reviewed electronic preprint platform Arxiv. The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” Splitting the graphs up for each category of education and plotting the percentage of usage for the given education level gives an insight into how the usage of algorithms differs over levels of education The graph above shows that regression and tree-based algorithms are very popular Regression and tree-based algorithms are: Less computationally intensive than neural networks Available in the de facto standard machine learning library in Python, scikit-learn. Below historical data to some the algorithms are given, together with links to the Wikipedia article on the algorithm. Linear regression Legendre, 1805 Gauss, 1809 Logistic regression Pierre Francois Verhulst, 1830s Random forest Ho, 1995 Gradient boosting trees L. Breiman, 1997 Convolutional neural networks Kunihiko Fukushima, 1980 Recurrent neural networks David Rumelhart, 1986 Dense neural networks Independently proposed by Alexander Bain, 1873 and William James, 1890 Generative adversarial networks Goodfellow, 2010-2014 "],
["machine-learning-experience-and-algorithms.html", "23.3 Machine learning experience and algorithms", " 23.3 Machine learning experience and algorithms Most of the survey participants have less than 3 years machine learning experience as can be seen in the graph below. Due to fact that the number in each category differs a lot a representation of percentages is beneficial for some analysis. The usage of algorithms for different duration of experience is given in the graph below. Splitting the graphs up for each category of experience and plotting the percentage of usage for the given experience level gives an insight into how the usage of algorithms differs over levels of experience Findings: Regression and trees are popular at all level of experience Neural networks are more popular for less experienced 20% of very experienced use no algorithm "],
["experience-and-new-algorithms.html", "23.4 Experience and new algorithms", " 23.4 Experience and new algorithms Newer algorithms there are listed below Newer algorithms are: Evolutionary Approaches Transformer Networks (BERT, gpt-2, etc) Generative Adversarial Networks (GAN) where evolutionary approaches have been around for quite some time but the usage of them in machine learning is rather recent. Splitting the graphs up for each category of experience and plotting the percentage of usage for the given experience level gives an insight into how the usage of new algorithms differs over levels of experience From the above graph it can be deducted that: Findings: Very experienced use new algorithms less often Newbies embrace them Evolutionary approaches are popular for medium experienced "],
["role-of-participants.html", "23.5 Role of participants", " 23.5 Role of participants The role of the participants is shown in the graph below The numbers for certain categories certainly have to be taken with a grain of salt since it is not clear how well participants will differentiate e.g. “Data Scientist” and “Data Analyst”. However, it is clear that students are quite active on Kaggle. This might influence the later data since students tend to use freeware more than professionals. Also there are: Summary roles: Many Software engineers Very few Statistician "],
["company-size.html", "23.6 Company size", " 23.6 Company size The company size of the participants is shown in the graph below Groups of many participants: Largest group of participants are from small companies Second largest group of participants are &gt;10,000 employees companies "],
["company-incorporation-of-machine-learning.html", "23.7 Company incorporation of machine learning", " 23.7 Company incorporation of machine learning The degree of machine learning utilization in the companies of the participants is shown in the graph below All participants of companies with &gt; 10,000 employees declare that \"We have well established ML methods (i.e., models in production for more than 2 years)\" Splitting the graphs up for each category of company size and plotting the incorporation of machine learning shows this even more clearly Leaving out the “&gt; 10,000 employees” category for better comparison Findings: More companies explore machine learning than having it established Many companies don’t use machine learning However, their employees invest in ML Danger of loosing employees Maybe companies are slow to discover ML potential "],
["favourite-media-sources-on-data-science-topics.html", "23.8 Favourite media sources on data science topics", " 23.8 Favourite media sources on data science topics The Favourite media sources on data science topics are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” Those sources offer information about: Algorithms New publications Projects Releases of new software versions Recommended courses, popular platforms see Favourite online course platform A few links to sources are given below Links to online media sources: Kaggle forums.fast.ai medium blog "],
["favourite-online-course-platform.html", "23.9 Favourite online course platform", " 23.9 Favourite online course platform Platforms on which survey participants have begun or completed data science courses are shown in the graph below All levels of academics are active on online course platforms. Below there are links to some of the platforms: Links to online course platforms: Coursera Kaggle Courses Udemy Udacity Fast.ai "],
["favourite-data-analyzing-tool.html", "23.10 Favourite data analyzing tool", " 23.10 Favourite data analyzing tool Participants primary tool to analyze data are shown in the graph below Most like to use free tools using the programming language “R” and “Python” "],
["experience-in-data-analysis-coding.html", "23.11 Experience in data analysis coding", " 23.11 Experience in data analysis coding The duration of participants writing code to analyze data is shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” Findings: Most have less than 5 years coding experience in data analysis Data analysis can be done without writing code "],
["favourite-integrated-development-environments-ides.html", "23.12 Favourite integrated development environments (IDE’s)", " 23.12 Favourite integrated development environments (IDE’s) Favourite integrated development environments (IDE’s) are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” List of some IDEs, all of them are free except for Matlab. Jupyter Notebook Works with Python, R, Julia, C++, Ruby Visual Studio Code Works with Python, R, Julia, C++, Ruby , SQL, XML, Swift, JSON, Perl, Sass… Debugger Variable viewer Console RStudio Mainly for R Debugger Variable viewer Console PyCharm For Python Debugger Variable viewer Matlab Very well established in industry Originally for control tasks Commercial tool Own syntax "],
["favourite-hosted-notebook-products.html", "23.13 Favourite hosted notebook products", " 23.13 Favourite hosted notebook products Favourite hosted notebook products are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” Hosted notebooks offer a very easy and comfortable start into writing machine learning code. Some of them are free. Some of them provide many examples from which valuable techniques can be learned. List of hosted notebooks Kaggle Notebooks Great place to find machine learning examples Google Colab Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud Binder Open notebooks in executable environment Microsoft Azure Notebooks Develop and run code from anywhere with Jupyter notebooks on Azure Paperspace Powering next-generation applications and cloud ML/AI pipelines. "],
["favourite-programming-languages.html", "23.14 Favourite programming languages", " 23.14 Favourite programming languages Favourite programming languages are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” Hands down the most popular programming language for machine learning is Python. If speed matters C++ is the way to go, but still, Python can be used for prototyping. "],
["recommended-entry-programming-language.html", "23.15 Recommended entry programming language", " 23.15 Recommended entry programming language Recommended programming language for aspiring data scientist to learn first are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” As Python is the most popular machine learning programming language it is not surprising that it is also the most recommended one for beginners. "],
["favourite-data-visualization-libraries-or-tools.html", "23.16 Favourite data visualization libraries or tools", " 23.16 Favourite data visualization libraries or tools Favourite data visualization libraries or tools are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” With Matplotlib there is a clear winner, however, ggplot2 is the clear Favourite in the R world. Links to visualization libraries: Matplotlib Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hard copy formats and interactive environments across platforms Seaborn Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. ggplot2 ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics Available for R and Python Plotly Interactive plots Available for R and Python D3.js Data-Driven Documents Javascript based Can be used from R and Python Bokeh Bokeh is an interactive visualization library for modern web browsers. "],
["favourite-specialized-hardware.html", "23.17 Favourite specialized hardware", " 23.17 Favourite specialized hardware Favourite specialized hardware are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” Specialized Hardware: CPU =&gt; Central Processing Unit Performs basic arithmetic, logic, and input output instructions Heart of every computing device GPU =&gt; Graphics Processing Unit Optimized processor for graphics Very fast matrix multiplication =&gt; speeds up neural network computation TPU =&gt; Tensor Processing Unit A tensor processing unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning. Edge TPU 4 TOPs22 Power = 2W In Wei, Brooks, and others (2019) a comparison of the three processors with respect to machine learning capabilities is given: TPU is highly-optimized for large batches and CNNs, and has the highest training throughput GPU shows better flexibility and programmability for irregular computations, such as small batches and non- MatMul computations. The training of large FC models also benefits from its sophisticated memory system and higher bandwidth. CPU has the best programmability, so it achieves the highest FLOPS utilization for RNNs, and it supports the largest model because of large memory capacity. Wei, Brooks, and others (2019) References "],
["favourite-machine-learning-frameworks.html", "23.18 Favourite machine learning frameworks", " 23.18 Favourite machine learning frameworks Favourite machine learning frameworks are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” Links to ML frameworks Scikit-learn Machine Learning in Python Open source, commercially usable - BSD license TensorFlow An end-to-end open source machine learning platform Keras Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. RandomForest A random forest classifier Xgboost XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. PyTorch An open source machine learning framework that accelerates the path from research prototyping to production deployment. On 30.01.2020 OpenAI announced OpenAI → PyTorch LightGBM LightGBM is a gradient boosting framework that uses tree based learning algorithms. Caret The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. For the programming language R Fast.ai Making neural nets uncool again Blogs MOOC23 Massive Open Online Courses↩︎ "],
["favourite-cloud-computing-platforms.html", "23.19 Favourite cloud computing platforms", " 23.19 Favourite cloud computing platforms Favourite cloud computing platforms are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” Links to ML cloud computing frameworks: Amazon Web Services (AWS) AWS has the services to help you build sophisticated applications with increased flexibility, scalability and reliability Google Cloud Platform (GCP) Build scalable apps Microsoft Azure Turn ideas into solutions with more than 100 services to build, deploy, and manage applications—in the cloud, on-premises, and at the edge—using the tools and frameworks of your choice. IBM Cloud Discover a faster, more secure journey to cloud trusted by thousands of enterprises across 20 industries VMware Cloud Manage your entire app portfolio across hybrid and native public clouds Oracle Cloud Oracle Cloud is a cloud computing service offered by Oracle Corporation providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers. Salesforce Cloud Try the world’s #1 service platform: the time-saving, joy-boosting, relationship-building machine. Alibaba Cloud Experience the Latest in Cloud Computing, Storage, Networking, Security, Big Data and Artificial Intelligence on Alibaba Cloud SAP Cloud Achieve process excellence, deliver engaging digital experiences, and simplify data-driven innovation with a multi-cloud architecture. Red Hat Cloud Red Hat® Cloud Access is the program that allows our customers to run eligible Red Hat product subscriptions on certified public cloud providers. "],
["favourite-big-data-analytics-products.html", "23.20 Favourite big data / analytics products", " 23.20 Favourite big data / analytics products Favourite big data / analytics products are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” "],
["favourite-automated-machine-learning-tools-or-partial-automl-tools.html", "23.21 Favourite automated machine learning tools (or partial AutoML tools)", " 23.21 Favourite automated machine learning tools (or partial AutoML tools) Favourite automated machine learning tools (or partial AutoML tools) are shown in the graph below The last Qualification which is cut off in the legend in the plot above reads “Some college/university study without earning a bachelor’s degree” "],
["references.html", "References", " References "]
]
